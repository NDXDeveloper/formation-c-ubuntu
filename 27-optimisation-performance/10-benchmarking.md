üîù Retour au [Sommaire](/SOMMAIRE.md)

# 27.10 Benchmarking rigoureux

## Introduction

Le **benchmarking** est l'art de mesurer les performances d'un programme de mani√®re **fiable, reproductible et significative**. C'est une comp√©tence essentielle car :

> "You can't optimize what you don't measure" ‚Äî Proverbe de l'optimisation

Sans benchmarking rigoureux, vous risquez de :
- ‚ùå Optimiser la mauvaise partie du code
- ‚ùå Croire qu'une optimisation fonctionne alors qu'elle ralentit le programme
- ‚ùå Obtenir des r√©sultats non reproductibles
- ‚ùå Tirer des conclusions erron√©es de mesures bruit√©es

### Analogie : Le chronom√®tre de course

**Mauvais benchmark (chronom√®tre approximatif) :**

Vous chronom√©trez un coureur avec une montre ordinaire :
- Vous d√©marrez "√† peu pr√®s" au d√©part
- Vous arr√™tez "√† peu pr√®s" √† l'arriv√©e
- R√©sultat : "environ 10 secondes" ¬± 2 secondes

**Bon benchmark (chronom√®tre de pr√©cision) :**

Vous utilisez un chronom√®tre √©lectronique professionnel :
- D√©part au centi√®me de seconde pr√®s
- Arriv√©e au centi√®me de seconde pr√®s
- Plusieurs essais pour √©liminer les variations
- Conditions identiques (m√™me piste, m√™me m√©t√©o, m√™me coureur)
- R√©sultat : "10.23 secondes" ¬± 0.02 secondes

M√™me principe pour benchmarker du code !

---

## Pourquoi le benchmarking est difficile

### Les sources de bruit

Les mesures de performance peuvent √™tre affect√©es par :

1. **L'√©tat du cache CPU** (chaud vs froid)
2. **Les autres processus** en cours d'ex√©cution
3. **Le CPU throttling** (r√©duction de fr√©quence)
4. **Les interruptions syst√®me**
5. **Le garbage collector** (pour les langages manag√©s)
6. **La latence r√©seau** (si applicable)
7. **L'√©tat du disque** (I/O en cours)
8. **La temp√©rature du CPU** (thermal throttling)
9. **Les optimisations du compilateur** (peuvent √©liminer du code)
10. **Les variations de temps syst√®me**

**Exemple concret :**

```bash
# M√™me commande, r√©sultats diff√©rents !
$ time ./mon_programme
real    0m0.123s

$ time ./mon_programme
real    0m0.145s  ‚Üê +18% de diff√©rence !

$ time ./mon_programme
real    0m0.119s
```

**Pourquoi ?** Cache froid/chaud, autres processus, interruptions...

---

## Principes du benchmarking rigoureux

### 1. Isoler le syst√®me

Minimiser les interf√©rences externes :

```bash
# ‚ùå Mauvais : Beaucoup de programmes en arri√®re-plan
# Navigateur ouvert, musique, compilation en cours...
./benchmark

# ‚úÖ Bon : Environnement contr√¥l√©
# Fermer tous les programmes inutiles
# D√©sactiver les services non essentiels
sudo systemctl stop bluetooth
sudo systemctl stop cups
# Etc.
```

**Encore mieux : Isoler un CPU**

```bash
# R√©server le CPU 3 uniquement pour le benchmark
sudo isolcpus=3

# Lancer le programme sur ce CPU
taskset -c 3 ./benchmark
```

### 2. Mesurer plusieurs fois

**Ne jamais faire confiance √† une seule mesure !**

```bash
# ‚ùå Mauvais : Une seule mesure
time ./programme

# ‚úÖ Bon : Plusieurs mesures
for i in {1..10}; do
    time ./programme
done
```

**Analyse statistique :**
- Calculer la **moyenne**
- Calculer l'**√©cart-type** (pour voir la variabilit√©)
- √âliminer les **outliers** (valeurs aberrantes)
- Rapporter le **minimum** (meilleur cas) et le **m√©dian** (typique)

### 3. R√©chauffer le cache (warm-up)

La premi√®re ex√©cution est souvent plus lente (cache froid).

```c
// ‚ùå Mauvais : Mesurer la premi√®re ex√©cution
clock_t start = clock();
fonction_a_benchmarker();
clock_t end = clock();
printf("Temps: %f\n", (double)(end - start) / CLOCKS_PER_SEC);

// ‚úÖ Bon : R√©chauffer d'abord
fonction_a_benchmarker();  // Warm-up (non mesur√©)
fonction_a_benchmarker();  // Warm-up

clock_t start = clock();
fonction_a_benchmarker();  // Mesure avec cache chaud
clock_t end = clock();
```

### 4. Ex√©cuter suffisamment longtemps

Si votre fonction s'ex√©cute en 1 microseconde, la pr√©cision de `clock()` n'est pas suffisante.

```c
// ‚ùå Mauvais : Fonction trop rapide
clock_t start = clock();
fonction_rapide();  // 1 microseconde
clock_t end = clock();
// Pr√©cision insuffisante, mesure = 0 !

// ‚úÖ Bon : Boucler pour obtenir un temps mesurable
clock_t start = clock();
for (int i = 0; i < 1000000; i++) {
    fonction_rapide();
}
clock_t end = clock();
double temps_moyen = (double)(end - start) / CLOCKS_PER_SEC / 1000000;
```

### 5. Emp√™cher les optimisations ind√©sirables du compilateur

Le compilateur peut **√©liminer** du code qu'il juge inutile !

```c
// ‚ùå Mauvais : Le compilateur peut tout √©liminer
int resultat = calcul_complexe();
// resultat n'est jamais utilis√© ‚Üí tout le calcul peut √™tre √©limin√© !

// ‚úÖ Bon : Utiliser le r√©sultat
int resultat = calcul_complexe();
printf("%d\n", resultat);  // Force le calcul

// ‚úÖ Encore mieux : volatile ou __asm__
volatile int resultat = calcul_complexe();

// Ou forcer le compilateur √† ne pas optimiser
__asm__ __volatile__("" : : "r,m"(resultat) : "memory");
```

---

## Outils de mesure

### 1. clock() (Temps CPU)

**Avantages :**
- ‚úÖ Simple
- ‚úÖ Standard C
- ‚úÖ Mesure uniquement le temps CPU du programme

**Inconv√©nients :**
- ‚ùå Pr√©cision limit√©e (~1 ms)
- ‚ùå Ne compte pas le temps d'attente (I/O, sleep)

**Utilisation :**

```c
#include <time.h>

clock_t start = clock();
// Code √† mesurer
clock_t end = clock();

double temps_cpu = (double)(end - start) / CLOCKS_PER_SEC;
printf("Temps CPU: %.6f secondes\n", temps_cpu);
```

### 2. gettimeofday() (Temps r√©el)

**Avantages :**
- ‚úÖ Pr√©cision microseconde
- ‚úÖ Mesure le temps "horloge murale" (wall-clock)

**Inconv√©nients :**
- ‚ùå Affect√© par les changements d'heure syst√®me
- ‚ùå Moins pr√©cis que clock_gettime()

**Utilisation :**

```c
#include <sys/time.h>

struct timeval start, end;

gettimeofday(&start, NULL);
// Code √† mesurer
gettimeofday(&end, NULL);

double elapsed = (end.tv_sec - start.tv_sec) +
                 (end.tv_usec - start.tv_usec) / 1000000.0;
printf("Temps √©coul√©: %.6f secondes\n", elapsed);
```

### 3. clock_gettime() (Recommand√©, haute pr√©cision)

**Avantages :**
- ‚úÖ Haute pr√©cision (nanoseconde)
- ‚úÖ Plusieurs horloges disponibles
- ‚úÖ Non affect√© par les changements d'heure

**Horloges disponibles :**
- `CLOCK_MONOTONIC` : Temps monotone (ne recule jamais)
- `CLOCK_PROCESS_CPUTIME_ID` : Temps CPU du processus
- `CLOCK_THREAD_CPUTIME_ID` : Temps CPU du thread
- `CLOCK_REALTIME` : Temps r√©el (peut √™tre ajust√©)

**Utilisation :**

```c
#include <time.h>

struct timespec start, end;

clock_gettime(CLOCK_MONOTONIC, &start);
// Code √† mesurer
clock_gettime(CLOCK_MONOTONIC, &end);

double elapsed = (end.tv_sec - start.tv_sec) +
                 (end.tv_nsec - start.tv_nsec) / 1000000000.0;
printf("Temps √©coul√©: %.9f secondes\n", elapsed);
```

### 4. Compteurs de cycles CPU (RDTSC)

**Avantages :**
- ‚úÖ Pr√©cision maximale (cycle CPU)
- ‚úÖ Tr√®s rapide (quelques cycles)

**Inconv√©nients :**
- ‚ùå Architecture sp√©cifique (x86)
- ‚ùå Affect√© par la fr√©quence variable du CPU (Turbo Boost)
- ‚ùå Complexe √† utiliser correctement

**Utilisation :**

```c
#include <x86intrin.h>

unsigned long long start = __rdtsc();
// Code √† mesurer (tr√®s court)
unsigned long long end = __rdtsc();

unsigned long long cycles = end - start;
printf("Cycles CPU: %llu\n", cycles);
```

**Note :** Utilisez `__rdtscp()` pour un ordonnancement correct des instructions.

### 5. perf (Compteurs mat√©riels)

**Le plus complet et pr√©cis pour Linux !**

```bash
# Mesurer le temps d'ex√©cution et les m√©triques
perf stat ./programme

# Exemple de sortie :
#  Performance counter stats for './programme':
#
#           1234.56 msec task-clock                #    0.998 CPUs utilized
#                12      context-switches          #    9.718 /sec
#                 0      cpu-migrations            #    0.000 /sec
#               156      page-faults               #  126.348 /sec
#      4,567,890,123      cycles                    #    3.698 GHz
#      6,789,012,345      instructions              #    1.49  insn per cycle
#      1,234,567,890      branches                  # 1000.123 M/sec
#         12,345,678      branch-misses             #    1.00% of all branches
#
#        1.236789000 seconds time elapsed
```

---

## Template de benchmark robuste

### Benchmark simple mais correct

```c
// benchmark_simple.c
#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <string.h>

#define ITERATIONS 1000000
#define WARMUP_RUNS 3
#define BENCHMARK_RUNS 10

// Fonction √† benchmarker
int fonction_a_tester(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum += i;
    }
    return sum;
}

// Emp√™cher l'optimisation
void escape(void *p) {
    __asm__ __volatile__("" : : "g"(p) : "memory");
}

void clobber() {
    __asm__ __volatile__("" : : : "memory");
}

int main() {
    struct timespec start, end;
    double times[BENCHMARK_RUNS];

    // Warm-up
    for (int i = 0; i < WARMUP_RUNS; i++) {
        volatile int result = fonction_a_tester(1000);
        escape(&result);
    }

    // Benchmarks
    for (int run = 0; run < BENCHMARK_RUNS; run++) {
        clock_gettime(CLOCK_MONOTONIC, &start);

        for (int i = 0; i < ITERATIONS; i++) {
            volatile int result = fonction_a_tester(1000);
            escape(&result);
            clobber();
        }

        clock_gettime(CLOCK_MONOTONIC, &end);

        double elapsed = (end.tv_sec - start.tv_sec) +
                         (end.tv_nsec - start.tv_nsec) / 1e9;
        times[run] = elapsed / ITERATIONS;  // Temps par it√©ration
    }

    // Analyse statistique
    double sum = 0, min = times[0], max = times[0];
    for (int i = 0; i < BENCHMARK_RUNS; i++) {
        sum += times[i];
        if (times[i] < min) min = times[i];
        if (times[i] > max) max = times[i];
    }
    double mean = sum / BENCHMARK_RUNS;

    // √âcart-type
    double variance = 0;
    for (int i = 0; i < BENCHMARK_RUNS; i++) {
        variance += (times[i] - mean) * (times[i] - mean);
    }
    double stddev = sqrt(variance / BENCHMARK_RUNS);

    printf("=== R√©sultats du benchmark ===\n");
    printf("It√©rations: %d\n", ITERATIONS);
    printf("Runs: %d\n", BENCHMARK_RUNS);
    printf("\n");
    printf("Temps moyen:  %.3f ns\n", mean * 1e9);
    printf("√âcart-type:   %.3f ns (%.1f%%)\n",
           stddev * 1e9, (stddev/mean)*100);
    printf("Min:          %.3f ns\n", min * 1e9);
    printf("Max:          %.3f ns\n", max * 1e9);

    return 0;
}
```

**Compilation :**

```bash
gcc -O2 -o benchmark benchmark_simple.c -lm
./benchmark
```

**R√©sultat typique :**

```
=== R√©sultats du benchmark ===
It√©rations: 1000000
Runs: 10

Temps moyen:  523.456 ns
√âcart-type:   12.345 ns (2.4%)
Min:          510.123 ns
Max:          545.678 ns
```

**Analyse :** √âcart-type de 2.4% ‚Üí R√©sultats stables ‚úÖ

---

## Comparer deux versions

### M√©thodologie

Pour comparer deux impl√©mentations (baseline vs optimis√©e) :

1. **Mesurer la baseline** (version de r√©f√©rence)
2. **Mesurer la version optimis√©e**
3. **Calculer le speedup** (am√©lioration)
4. **V√©rifier la significativit√© statistique**

### Script de comparaison

```c
// compare_versions.c
#include <stdio.h>
#include <time.h>
#include <math.h>

#define ITERATIONS 1000000
#define RUNS 20

// Version 1 : Baseline
int version_baseline(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum += i;
    }
    return sum;
}

// Version 2 : Optimis√©e (formule math√©matique)
int version_optimisee(int n) {
    return n * (n - 1) / 2;
}

double benchmark_function(int (*func)(int), int n) {
    struct timespec start, end;
    double times[RUNS];

    // Warm-up
    for (int i = 0; i < 3; i++) {
        volatile int r = func(n);
    }

    // Mesures
    for (int run = 0; run < RUNS; run++) {
        clock_gettime(CLOCK_MONOTONIC, &start);

        for (int i = 0; i < ITERATIONS; i++) {
            volatile int result = func(n);
        }

        clock_gettime(CLOCK_MONOTONIC, &end);

        times[run] = (end.tv_sec - start.tv_sec) +
                     (end.tv_nsec - start.tv_nsec) / 1e9;
    }

    // Calculer la m√©diane (plus robuste que la moyenne)
    qsort(times, RUNS, sizeof(double), compare_double);
    return times[RUNS / 2];
}

int compare_double(const void *a, const void *b) {
    double diff = *(double*)a - *(double*)b;
    return (diff > 0) - (diff < 0);
}

int main() {
    int n = 10000;

    printf("=== Comparaison de performances ===\n");
    printf("Taille: %d, It√©rations: %d, Runs: %d\n\n", n, ITERATIONS, RUNS);

    double time_baseline = benchmark_function(version_baseline, n);
    double time_optimisee = benchmark_function(version_optimisee, n);

    printf("Baseline:   %.3f ms\n", time_baseline * 1000);
    printf("Optimis√©e:  %.3f ms\n", time_optimisee * 1000);
    printf("\n");

    double speedup = time_baseline / time_optimisee;
    double improvement = ((time_baseline - time_optimisee) / time_baseline) * 100;

    printf("Speedup:       %.2fx\n", speedup);
    printf("Am√©lioration:  %.1f%%\n", improvement);

    if (speedup > 1.05) {
        printf("‚úÖ Optimisation significative !\n");
    } else if (speedup < 0.95) {
        printf("‚ùå R√©gression de performance !\n");
    } else {
        printf("‚ö†Ô∏è  Pas de diff√©rence significative\n");
    }

    return 0;
}
```

**R√©sultat typique :**

```
=== Comparaison de performances ===
Taille: 10000, It√©rations: 1000000, Runs: 20

Baseline:   245.678 ms
Optimis√©e:  12.345 ms

Speedup:       19.89x
Am√©lioration:  95.0%
‚úÖ Optimisation significative !
```

---

## Pi√®ges courants du benchmarking

### Pi√®ge 1 : Dead Code Elimination

**Probl√®me :** Le compilateur √©limine le code non utilis√©.

```c
// ‚ùå Mauvais
clock_t start = clock();
int result = calcul_lourd();
// result n'est jamais utilis√© ‚Üí tout √©limin√© par -O2 !
clock_t end = clock();

// ‚úÖ Bon
clock_t start = clock();
int result = calcul_lourd();
printf("%d\n", result);  // Ou volatile, ou escape()
clock_t end = clock();
```

### Pi√®ge 2 : Constant Folding

**Probl√®me :** Le compilateur calcule les constantes √† la compilation.

```c
// ‚ùå Mauvais
clock_t start = clock();
int result = fibonacci(10);  // Calcul√© √† la compilation !
clock_t end = clock();
// Temps mesur√© ‚âà 0

// ‚úÖ Bon
int n;
scanf("%d", &n);  // Valeur inconnue √† la compilation
clock_t start = clock();
int result = fibonacci(n);
clock_t end = clock();
```

### Pi√®ge 3 : Loop Unrolling et Vectorization

**Probl√®me :** Le compilateur optimise diff√©remment selon le contexte.

```c
// Code de benchmark
for (int i = 0; i < 1000000; i++) {
    fonction();  // Peut √™tre vectoris√©e/d√©roul√©e
}

// Code r√©el d'usage
for (int i = 0; i < n; i++) {  // n inconnu
    fonction();  // Optimisation diff√©rente
}
```

**Solution :** Benchmarker avec des conditions similaires √† l'usage r√©el.

### Pi√®ge 4 : Cache Effects

**Probl√®me :** Le cache peut fausser les r√©sultats.

```c
// ‚ùå Mauvais : Donn√©es en cache
int data[1000];
for (int i = 0; i < 1000; i++) data[i] = i;

// Benchmark (cache chaud)
clock_t start = clock();
for (int i = 0; i < 1000; i++) {
    process(data[i]);
}
clock_t end = clock();

// ‚úÖ Bon : Invalider le cache entre les mesures
for (int run = 0; run < 10; run++) {
    flush_cache();  // Ou r√©allouer les donn√©es

    clock_t start = clock();
    for (int i = 0; i < 1000; i++) {
        process(data[i]);
    }
    clock_t end = clock();
}
```

### Pi√®ge 5 : CPU Frequency Scaling

**Probl√®me :** Le CPU change de fr√©quence dynamiquement (Turbo Boost, PowerSave).

```bash
# V√©rifier la fr√©quence actuelle
cat /proc/cpuinfo | grep MHz

# Fixer la fr√©quence (n√©cessite root)
sudo cpupower frequency-set -g performance

# Ou d√©sactiver le Turbo Boost
echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
```

### Pi√®ge 6 : Mesurer du code trop court

**Probl√®me :** Pr√©cision insuffisante pour du code qui s'ex√©cute en < 1 ¬µs.

```c
// ‚ùå Mauvais
start = clock();
x = a + b;  // Quelques nanosecondes
end = clock();
// R√©sultat = 0 ou tr√®s impr√©cis

// ‚úÖ Bon
start = clock();
for (int i = 0; i < 1000000; i++) {
    x = a + b;
}
end = clock();
double temps_par_operation = elapsed / 1000000.0;
```

---

## Biblioth√®ques de benchmarking

### Google Benchmark (C++, mais utilisable avec C)

**Avantages :**
- ‚úÖ Tr√®s sophistiqu√©
- ‚úÖ G√®re automatiquement le warm-up, les it√©rations, les statistiques
- ‚úÖ D√©tecte et √©limine les outliers
- ‚úÖ Format de sortie standardis√©

**Exemple :**

```cpp
#include <benchmark/benchmark.h>

static void BM_MonAlgorithme(benchmark::State& state) {
    // Setup
    int n = state.range(0);
    int* data = new int[n];

    // Benchmark
    for (auto _ : state) {
        process_data(data, n);
    }

    // Cleanup
    delete[] data;
}

BENCHMARK(BM_MonAlgorithme)->Range(8, 8<<10);
BENCHMARK_MAIN();
```

### Criterion (Pour C uniquement)

**Avantages :**
- ‚úÖ Sp√©cialement con√ßu pour C
- ‚úÖ Analyse statistique automatique
- ‚úÖ D√©tection de r√©gression

**Exemple :**

```c
#include <criterion/criterion.h>

Test(benchmark, somme_tableau) {
    int data[1000];
    for (int i = 0; i < 1000; i++) data[i] = i;

    cr_benchmark {
        int sum = 0;
        for (int i = 0; i < 1000; i++) {
            sum += data[i];
        }
    }
}
```

### hyperfine (Outil en ligne de commande)

**Id√©al pour comparer des programmes complets !**

```bash
# Installer hyperfine
sudo apt install hyperfine

# Comparer deux versions
hyperfine './programme_v1' './programme_v2'

# Sortie :
# Benchmark 1: ./programme_v1
#   Time (mean ¬± œÉ):      23.4 ms ¬±   0.8 ms    [User: 22.1 ms, System: 1.2 ms]
#   Range (min ‚Ä¶ max):    22.1 ms ‚Ä¶  25.3 ms    100 runs
#
# Benchmark 2: ./programme_v2
#   Time (mean ¬± œÉ):      15.2 ms ¬±   0.5 ms    [User: 14.8 ms, System: 0.4 ms]
#   Range (min ‚Ä¶ max):    14.5 ms ‚Ä¶  16.8 ms    150 runs
#
# Summary
#   './programme_v2' ran
#     1.54 ¬± 0.06 times faster than './programme_v1'
```

**Avantages :**
- ‚úÖ Tr√®s simple √† utiliser
- ‚úÖ Analyse statistique automatique
- ‚úÖ Comparaison claire avec speedup
- ‚úÖ Warm-up automatique

---

## Bonnes pratiques de reporting

### Informations √† inclure

Un bon rapport de benchmark contient :

1. **Environnement**
   - OS et version (Ubuntu 22.04)
   - CPU (Intel i7-12700K @ 3.6 GHz)
   - RAM (32 GB DDR4-3200)
   - Compilateur (GCC 11.3.0)
   - Flags de compilation (`-O3 -march=native`)

2. **M√©thodologie**
   - Nombre d'it√©rations
   - Nombre de runs
   - Warm-up effectu√© ou non
   - Taille des donn√©es d'entr√©e

3. **R√©sultats**
   - Temps moyen ¬± √©cart-type
   - Minimum et maximum
   - M√©diane (si applicable)
   - Speedup relatif (si comparaison)

4. **Reproductibilit√©**
   - Code source du benchmark
   - Commande de compilation
   - Script de lancement

### Exemple de rapport

```
=== Benchmark : Algorithme de tri ===

Environnement :
- OS: Ubuntu 22.04 LTS
- CPU: Intel Core i7-12700K @ 3.6 GHz (12 cores)
- RAM: 32 GB DDR4-3200
- Compilateur: GCC 11.3.0
- Flags: -O3 -march=native -flto

M√©thodologie :
- Donn√©es: 1,000,000 entiers al√©atoires
- It√©rations: 100 (avec warm-up de 10)
- Isolation: CPU 3 d√©di√© (isolcpus=3)

R√©sultats :

QuickSort (baseline):
  Temps moyen: 125.3 ¬± 3.2 ms
  Min: 121.8 ms
  Max: 132.1 ms

MergeSort (version optimis√©e):
  Temps moyen: 98.7 ¬± 2.1 ms
  Min: 96.2 ms
  Max: 102.4 ms

Comparaison:
  Speedup: 1.27x
  Am√©lioration: 21.2%
  ‚úÖ Am√©lioration statistiquement significative (p < 0.01)

Code et reproduction:
  https://github.com/user/projet/benchmarks/sort_bench.c
  $ gcc -O3 -march=native -flto sort_bench.c -o bench
  $ taskset -c 3 ./bench
```

---

## Checklist : Benchmark rigoureux

Avant de publier des r√©sultats :

- ‚úÖ **Environnement contr√¥l√©** (fermer les autres programmes)
- ‚úÖ **CPU fr√©quence fixe** (mode performance, pas de Turbo Boost)
- ‚úÖ **Warm-up** (plusieurs ex√©cutions avant mesure)
- ‚úÖ **Mesures multiples** (minimum 10 runs)
- ‚úÖ **Analyse statistique** (moyenne, √©cart-type, outliers)
- ‚úÖ **V√©rification du code** (pas d'√©limination par le compilateur)
- ‚úÖ **Donn√©es repr√©sentatives** (taille et distribution r√©alistes)
- ‚úÖ **Reproductibilit√©** (documenter l'environnement et la m√©thode)
- ‚úÖ **V√©rification des r√©sultats** (corrects, pas seulement rapides !)
- ‚úÖ **Contexte clair** (ce qui est mesur√© exactement)

---

## Erreurs √† √©viter

### ‚ùå Erreur 1 : Mesurer une seule fois

```bash
# Mauvais
time ./programme
# 0.123s
# "Mon programme prend 0.123 secondes"
```

**Pourquoi c'est mauvais :** Cette mesure unique peut √™tre un outlier.

**Solution :** Toujours mesurer plusieurs fois et analyser statistiquement.

### ‚ùå Erreur 2 : Ignorer la variabilit√©

```
Run 1: 100 ms
Run 2: 150 ms
Run 3: 98 ms
Moyenne: 116 ms
```

**Probl√®me :** √âcart-type de 30 ms (26% !) ‚Üí R√©sultats instables !

**Solution :** Investiguer pourquoi il y a autant de variabilit√© avant de conclure.

### ‚ùå Erreur 3 : Comparer des pommes et des oranges

```bash
# Mauvais : Compilations diff√©rentes
gcc -O0 version1.c -o v1
gcc -O3 version2.c -o v2
# Comparer v1 et v2 ‚Üí Non significatif !
```

**Solution :** Toujours compiler avec les m√™mes flags.

### ‚ùå Erreur 4 : Oublier de v√©rifier la correction

```c
// Version "optimis√©e" ultra-rapide
int somme(int *tableau, int n) {
    return 0;  // "Optimisation" : toujours retourner 0 !
}
```

**Solution :** Toujours v√©rifier que les r√©sultats sont **corrects** !

### ‚ùå Erreur 5 : Benchmarker du code debug

```bash
# Mauvais
gcc -O0 -g programme.c -o programme_debug
time ./programme_debug
```

**Solution :** Benchmarker du code **optimis√© pour la production** (`-O2` ou `-O3`).

---

## Microbenchmarks vs Macrobenchmarks

### Microbenchmarks

**D√©finition :** Mesurer une petite partie isol√©e du code (une fonction, une boucle).

**Avantages :**
- ‚úÖ Rapide √† ex√©cuter
- ‚úÖ Isole pr√©cis√©ment l'impact d'une optimisation
- ‚úÖ Reproductible

**Inconv√©nients :**
- ‚ùå Peut ne pas refl√©ter la performance r√©elle (cache chaud, contexte artificiel)
- ‚ùå Le compilateur peut sur-optimiser du code isol√©

**Quand utiliser :** Pour comparer deux impl√©mentations d'un algorithme sp√©cifique.

### Macrobenchmarks

**D√©finition :** Mesurer le programme complet avec des donn√©es r√©elles.

**Avantages :**
- ‚úÖ Refl√®te la performance r√©elle
- ‚úÖ Prend en compte tous les facteurs (I/O, cache, etc.)

**Inconv√©nients :**
- ‚ùå Lent √† ex√©cuter
- ‚ùå Difficile d'isoler l'effet d'une optimisation pr√©cise

**Quand utiliser :** Pour valider qu'une optimisation am√©liore vraiment le programme en conditions r√©elles.

**Recommandation :** Faire **les deux** ! Microbenchmark pour d√©velopper, macrobenchmark pour valider.

---

## Exemple complet : Benchmark de fonction de hachage

```c
// hash_benchmark.c
#include <stdio.h>
#include <stdint.h>
#include <time.h>
#include <string.h>

#define DATA_SIZE 1000000
#define RUNS 20

// Hash function 1: djb2
uint32_t hash_djb2(const char *str) {
    uint32_t hash = 5381;
    int c;
    while ((c = *str++)) {
        hash = ((hash << 5) + hash) + c;
    }
    return hash;
}

// Hash function 2: FNV-1a
uint32_t hash_fnv1a(const char *str) {
    uint32_t hash = 2166136261u;
    while (*str) {
        hash ^= (uint8_t)(*str++);
        hash *= 16777619;
    }
    return hash;
}

double benchmark_hash(uint32_t (*hash_func)(const char*),
                      char **data, int count) {
    struct timespec start, end;

    // Warm-up
    for (int i = 0; i < count / 10; i++) {
        volatile uint32_t h = hash_func(data[i % count]);
    }

    // Mesure
    clock_gettime(CLOCK_MONOTONIC, &start);
    for (int i = 0; i < count; i++) {
        volatile uint32_t h = hash_func(data[i]);
    }
    clock_gettime(CLOCK_MONOTONIC, &end);

    return (end.tv_sec - start.tv_sec) +
           (end.tv_nsec - start.tv_nsec) / 1e9;
}

int main() {
    // G√©n√©rer des donn√©es de test
    char **data = malloc(DATA_SIZE * sizeof(char*));
    for (int i = 0; i < DATA_SIZE; i++) {
        data[i] = malloc(20);
        snprintf(data[i], 20, "string_%d", i);
    }

    printf("=== Benchmark : Hash Functions ===\n");
    printf("Donn√©es: %d cha√Ænes\n", DATA_SIZE);
    printf("Runs: %d\n\n", RUNS);

    double times_djb2[RUNS], times_fnv1a[RUNS];

    // Benchmarks
    for (int run = 0; run < RUNS; run++) {
        times_djb2[run] = benchmark_hash(hash_djb2, data, DATA_SIZE);
        times_fnv1a[run] = benchmark_hash(hash_fnv1a, data, DATA_SIZE);
    }

    // Analyse
    double avg_djb2 = 0, avg_fnv1a = 0;
    for (int i = 0; i < RUNS; i++) {
        avg_djb2 += times_djb2[i];
        avg_fnv1a += times_fnv1a[i];
    }
    avg_djb2 /= RUNS;
    avg_fnv1a /= RUNS;

    printf("djb2:   %.3f ms (%.2f ns/hash)\n",
           avg_djb2 * 1000, avg_djb2 / DATA_SIZE * 1e9);
    printf("FNV-1a: %.3f ms (%.2f ns/hash)\n",
           avg_fnv1a * 1000, avg_fnv1a / DATA_SIZE * 1e9);
    printf("\n");

    if (avg_djb2 < avg_fnv1a) {
        printf("djb2 est %.2fx plus rapide\n", avg_fnv1a / avg_djb2);
    } else {
        printf("FNV-1a est %.2fx plus rapide\n", avg_djb2 / avg_fnv1a);
    }

    // Cleanup
    for (int i = 0; i < DATA_SIZE; i++) {
        free(data[i]);
    }
    free(data);

    return 0;
}
```

---

## Ressources et outils

### Outils de benchmarking

- **hyperfine** : https://github.com/sharkdp/hyperfine (CLI, simple, excellent)
- **Google Benchmark** : https://github.com/google/benchmark (C++, sophistiqu√©)
- **Criterion** : https://github.com/Snaipe/Criterion (C, complet)
- **perf** : Int√©gr√© √† Linux (le plus pr√©cis)

### Lectures recommand√©es

- **"Systems Performance"** ‚Äî Brendan Gregg (bible du benchmarking)
- **"Benchmarking Crimes"** ‚Äî Gernot Heiser (article sur les erreurs courantes)
- **Google's "How to Benchmark Code"** : https://github.com/google/benchmark/blob/main/docs/user_guide.md

### Commandes utiles

```bash
# Fixer la fr√©quence CPU
sudo cpupower frequency-set -g performance

# D√©sactiver Turbo Boost (Intel)
echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo

# Voir les CPUs isol√©s
cat /sys/devices/system/cpu/isolated

# Lancer sur un CPU sp√©cifique
taskset -c 3 ./programme

# Profiler avec perf
perf stat -r 10 ./programme
```

---

## R√©sum√©

Le benchmarking rigoureux est essentiel pour l'optimisation :

1. ‚úÖ **Mesurer plusieurs fois** : Jamais une seule mesure
2. ‚úÖ **Analyser statistiquement** : Moyenne, √©cart-type, outliers
3. ‚úÖ **Contr√¥ler l'environnement** : Isolation, fr√©quence CPU fixe
4. ‚úÖ **Warm-up** : Cache chaud avant mesure
5. ‚úÖ **√âviter les pi√®ges** : Dead code elimination, constant folding
6. ‚úÖ **Utiliser les bons outils** : `clock_gettime()`, `perf`, `hyperfine`
7. ‚úÖ **Documenter** : Environnement, m√©thodologie, reproductibilit√©
8. ‚úÖ **V√©rifier la correction** : R√©sultats corrects, pas seulement rapides

**Citation finale :**

> "In benchmarking, precision matters more than accuracy. You need reproducible results to detect small improvements" ‚Äî Anonyme

**R√®gle d'or :**

```
Bon benchmark = Mesures multiples + Analyse statistique + Environnement contr√¥l√© + V√©rification
```

Sans benchmark rigoureux, vous optimisez √† l'aveugle. Avec un bon benchmark, vous pouvez prendre des d√©cisions √©clair√©es et mesurer objectivement l'impact de vos optimisations !

---

*Fin du Module 27 : Optimisation et Performance*

‚è≠Ô∏è [Interop√©rabilit√©](/28-interoperabilite/README.md)
