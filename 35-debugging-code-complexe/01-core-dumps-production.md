üîù Retour au [Sommaire](/SOMMAIRE.md)

# 35.1 Analyse de core dumps en production

## Introduction

Un **core dump** (ou fichier core) est une photographie compl√®te de l'√©tat de la m√©moire d'un programme au moment o√π il s'est arr√™t√© brutalement. C'est comme une "bo√Æte noire" qui enregistre tout ce qui se passait dans votre application juste avant le crash.

En production, les core dumps sont souvent votre **seul moyen** de comprendre pourquoi une application critique s'est arr√™t√©e de mani√®re inattendue, surtout quand le probl√®me ne se reproduit pas facilement en d√©veloppement.

### Pourquoi les core dumps sont essentiels en production ?

- **Debugging post-mortem** : Analyser un crash sans pouvoir reproduire le bug
- **Erreurs intermittentes** : Capturer des probl√®mes qui n'arrivent qu'une fois sur mille
- **Contexte complet** : Acc√®s √† toutes les variables, la pile d'appels, et l'√©tat m√©moire
- **Production vs d√©veloppement** : Les bugs en production ont souvent des causes diff√©rentes (charge, donn√©es r√©elles, timing)

---

## Configuration pour g√©n√©rer des core dumps

### V√©rifier la limite de taille des core dumps

Par d√©faut sur beaucoup de syst√®mes, la g√©n√©ration de core dumps est **d√©sactiv√©e** pour √©conomiser de l'espace disque.

```bash
# V√©rifier la limite actuelle (0 = d√©sactiv√©)
ulimit -c
```

### Activer les core dumps

```bash
# Pour la session courante : taille illimit√©e
ulimit -c unlimited

# Pour rendre permanent (ajouter dans /etc/security/limits.conf)
* soft core unlimited
* hard core unlimited
```

### Configurer l'emplacement et le nom des core dumps

Par d√©faut, les core dumps sont cr√©√©s dans le r√©pertoire de travail du programme avec le nom `core` ou `core.PID`. En production, il est recommand√© de centraliser ces fichiers.

```bash
# Voir la configuration actuelle
cat /proc/sys/kernel/core_pattern

# Configuration recommand√©e pour la production (n√©cessite root)
echo "/var/crash/core.%e.%p.%t" | sudo tee /proc/sys/kernel/core_pattern
```

**L√©gende des motifs** :
- `%e` : Nom de l'ex√©cutable
- `%p` : PID du processus
- `%t` : Timestamp (secondes depuis epoch)
- `%u` : UID de l'utilisateur
- `%s` : Num√©ro du signal ayant caus√© le dump

**Exemple de nom g√©n√©r√©** : `/var/crash/core.myapp.12345.1709123456`

### Cr√©er le r√©pertoire de stockage

```bash
sudo mkdir -p /var/crash
sudo chmod 1777 /var/crash  # Accessible √† tous, mais chacun ne voit que ses fichiers
```

---

## Anatomie d'un core dump

Un core dump contient :

1. **Le code du programme** (sections .text)
2. **Toutes les donn√©es** (sections .data, .bss)
3. **La pile d'ex√©cution** (stack) de chaque thread
4. **Le tas** (heap) avec toutes les allocations dynamiques
5. **Les registres CPU** au moment du crash
6. **Les biblioth√®ques partag√©es** mapp√©es en m√©moire

### Taille des core dumps

‚ö†Ô∏è **Attention** : Un core dump peut √™tre **tr√®s volumineux** (plusieurs Go pour une application avec beaucoup de m√©moire allou√©e).

```bash
# Limiter la taille si n√©cessaire (en Ko)
ulimit -c 1048576  # Limite √† 1 Go
```

---

## Analyse avec GDB : Les bases

### Charger un core dump

```bash
# Syntaxe g√©n√©rale
gdb <ex√©cutable> <fichier_core>

# Exemple
gdb ./myapp /var/crash/core.myapp.12345.1709123456
```

**Important** : Vous devez avoir :
- L'ex√©cutable **exact** qui a g√©n√©r√© le core (m√™me version)
- Les symboles de d√©bogage (compil√© avec `-g`)

### Premi√®res commandes essentielles

```gdb
# 1. Voir o√π le crash s'est produit
(gdb) backtrace
# ou
(gdb) bt

# 2. Voir le code source autour du crash
(gdb) list

# 3. Examiner les variables locales
(gdb) info locals

# 4. Examiner les arguments de la fonction
(gdb) info args

# 5. Voir tous les threads
(gdb) info threads

# 6. Changer de frame dans la pile
(gdb) frame 3
(gdb) up      # Monter d'un niveau
(gdb) down    # Descendre d'un niveau
```

### Exemple d'analyse simple

Supposons ce code qui crashe :

```c
#include <stdio.h>
#include <stdlib.h>

void process_data(int *array, int size) {
    for (int i = 0; i <= size; i++) {  // Bug : <= au lieu de <
        array[i] = i * 2;
    }
}

int main() {
    int *data = malloc(10 * sizeof(int));
    process_data(data, 10);
    free(data);
    return 0;
}
```

**Compilation avec symboles** :
```bash
gcc -g -o myapp myapp.c
ulimit -c unlimited
./myapp
# Segmentation fault (core dumped)
```

**Analyse du core dump** :
```gdb
$ gdb ./myapp core
(gdb) bt
#0  0x0000555555555189 in process_data (array=0x555555559260, size=10) at myapp.c:6
#1  0x00005555555551b5 in main () at myapp.c:12

(gdb) frame 0
(gdb) list
1   #include <stdio.h>
2   #include <stdlib.h>
3
4   void process_data(int *array, int size) {
5       for (int i = 0; i <= size; i++) {
6           array[i] = i * 2;      ‚Üê CRASH ICI
7       }
8   }

(gdb) print i
$1 = 10

(gdb) print size
$2 = 10

(gdb) print array
$3 = (int *) 0x555555559260
```

**Diagnostic** : On acc√®de √† `array[10]` alors que le tableau ne va que de 0 √† 9 (buffer overflow).

---

## Techniques d'investigation avanc√©es

### 1. Examiner la m√©moire brute

```gdb
# Examiner 16 mots (4 octets) √† partir d'une adresse
(gdb) x/16xw 0x555555559260

# Examiner en tant que cha√Æne de caract√®res
(gdb) x/s 0x7fffffffe000

# Formats :
# x = hexad√©cimal, d = d√©cimal, s = string
# b = byte, h = halfword (2 bytes), w = word (4 bytes), g = giant (8 bytes)
```

### 2. Analyse des pointeurs NULL ou invalides

```gdb
# Si le crash vient d'un pointeur NULL
(gdb) bt
#0  0x0000555555555180 in process_user (user=0x0) at app.c:45

(gdb) frame 0
(gdb) print user
$1 = (struct User *) 0x0

# Remonter pour trouver d'o√π vient le NULL
(gdb) up
(gdb) print user_list
```

### 3. Identifier les corruptions de m√©moire

```gdb
# V√©rifier les structures critiques
(gdb) print *mystruct
$1 = {
  magic_number = 0,         ‚Üê Devrait √™tre 0xDEADBEEF
  data = 0x4141414141414141  ‚Üê Suspect (AAAAAAAA)
}

# Examiner le heap autour de l'objet
(gdb) x/100xw mystruct
```

### 4. Analyse multi-thread

```gdb
# Lister tous les threads
(gdb) info threads
  Id   Target Id         Frame
* 1    Thread 0x7f... (LWP 12345) in __pthread_cond_wait
  2    Thread 0x7f... (LWP 12346) in read
  3    Thread 0x7f... (LWP 12347) in malloc

# Basculer sur un thread sp√©cifique
(gdb) thread 3
(gdb) bt

# Afficher la backtrace de TOUS les threads
(gdb) thread apply all bt
```

### 5. Examiner les variables globales

```gdb
# Variables globales et statiques
(gdb) print global_counter
(gdb) print 'fichier.c'::static_var
```

---

## Cas d'usage typiques en production

### Sc√©nario 1 : Segmentation Fault al√©atoire

**Sympt√¥mes** : Application crashe une fois par jour, impossible √† reproduire.

**D√©marche** :
1. Activer les core dumps avec timestamp
2. Attendre le prochain crash
3. Analyser le core avec GDB :
   ```gdb
   (gdb) bt full     # Backtrace compl√®te avec toutes les variables
   (gdb) info registers
   ```
4. Chercher des patterns : pointeurs NULL, indices hors limites, corruption

### Sc√©nario 2 : Deadlock

**Sympt√¥mes** : Application fig√©e, mais ne crashe pas (pas de core dump naturel).

**Solution** : Forcer la g√©n√©ration d'un core dump sur un processus vivant :

```bash
# Trouver le PID
ps aux | grep myapp

# G√©n√©rer un core dump sans tuer le processus (Linux >= 3.18)
sudo gcore -o /var/crash/myapp 12345

# Analyser
gdb ./myapp /var/crash/myapp.12345
(gdb) info threads
(gdb) thread apply all bt
```

Chercher des threads bloqu√©s sur des mutex :
```gdb
(gdb) thread 2
(gdb) bt
#0  __pthread_mutex_lock
#1  acquire_lock at myapp.c:123
```

### Sc√©nario 3 : Corruption de heap

**Sympt√¥mes** : Crash dans `malloc()` ou `free()` avec messages cryptiques.

**D√©marche** :
```gdb
(gdb) bt
#0  malloc.c:2374
#1  free (ptr=0x123456) at malloc.c:2957
#2  cleanup at myapp.c:456

# Le crash est dans malloc, mais la cause est ailleurs
# Chercher des double-free, buffer overflows, ou use-after-free

(gdb) frame 2
(gdb) print ptr
(gdb) x/16xw ptr   # Examiner la m√©moire
```

**Astuce** : Recompiler avec AddressSanitizer pour le prochain crash :
```bash
gcc -g -fsanitize=address -o myapp myapp.c
```

---

## Bonnes pratiques pour la production

### 1. Compilation avec symboles s√©par√©s

En production, vous ne voulez pas inclure les symboles de debug dans le binaire (taille).

**Solution** : Fichiers de symboles s√©par√©s (.debug)

```bash
# Compiler avec debug
gcc -g -o myapp myapp.c

# S√©parer les symboles
objcopy --only-keep-debug myapp myapp.debug
strip --strip-debug --strip-unneeded myapp

# Cr√©er un lien entre le binaire et les symboles
objcopy --add-gnu-debuglink=myapp.debug myapp
```

**Chargement dans GDB** :
```gdb
(gdb) symbol-file myapp.debug
```

### 2. Rotation des core dumps

Les core dumps prennent de la place. Automatisez leur nettoyage :

```bash
# Crontab pour supprimer les cores de plus de 7 jours
0 2 * * * find /var/crash -name "core.*" -mtime +7 -delete
```

### 3. Logging avant crash

Ajoutez des signaux handlers pour logger avant de crasher :

```c
#include <signal.h>
#include <execinfo.h>

void signal_handler(int sig) {
    void *array[10];
    size_t size = backtrace(array, 10);

    fprintf(stderr, "Error: signal %d:\n", sig);
    backtrace_symbols_fd(array, size, STDERR_FILENO);

    // R√©tablir le handler par d√©faut et re-raise
    signal(sig, SIG_DFL);
    raise(sig);
}

int main() {
    signal(SIGSEGV, signal_handler);
    signal(SIGABRT, signal_handler);
    // ... votre code
}
```

### 4. Automatisation de l'analyse

Script pour analyser automatiquement les nouveaux core dumps :

```bash
#!/bin/bash
# auto_analyze_core.sh

CORE_FILE=$1
EXECUTABLE=$2

if [ ! -f "$CORE_FILE" ]; then
    echo "Core file not found: $CORE_FILE"
    exit 1
fi

echo "=== Analyzing $CORE_FILE ==="
gdb -batch -ex "bt full" -ex "info registers" -ex "thread apply all bt" \
    "$EXECUTABLE" "$CORE_FILE" > "${CORE_FILE}.analysis.txt"

echo "Analysis saved to ${CORE_FILE}.analysis.txt"
```

### 5. Int√©gration avec systemd

Pour les services systemd, configurer la gestion des core dumps :

```ini
# /etc/systemd/system/myapp.service
[Service]
LimitCORE=infinity
```

V√©rifier :
```bash
systemctl show myapp.service | grep LimitCORE
```

---

## Outils compl√©mentaires

### `coredumpctl` (systemd)

Sur les syst√®mes avec systemd, les core dumps sont g√©r√©s par `systemd-coredump` :

```bash
# Lister les core dumps
coredumpctl list

# Analyser le dernier core dump
coredumpctl debug

# Extraire un core dump sp√©cifique
coredumpctl dump <PID> > core.dump
```

### Analyse avec LLDB (alternative √† GDB)

```bash
# Charger un core dump
lldb ./myapp -c core.12345

# Commandes similaires
(lldb) bt
(lldb) frame variable
(lldb) thread list
```

---

## Limitations et pi√®ges √† √©viter

### 1. Core dump incomplet

Si le programme a beaucoup de m√©moire mapp√©e, le core peut √™tre tronqu√©.

**Solution** : Configurer `/proc/sys/kernel/core_pipe_limit`

### 2. Binaire et core dump d√©synchronis√©s

Si vous avez red√©ploy√© l'application entre le crash et l'analyse, le binaire ne correspondra pas.

**Solution** : Versionner et archiver chaque build avec ses symboles

### 3. ASLR (Address Space Layout Randomization)

Les adresses changent √† chaque ex√©cution, compliquant l'analyse.

**Pour le debugging** (pas en production !) :
```bash
echo 0 | sudo tee /proc/sys/kernel/randomize_va_space
```

### 4. Informations sensibles

Les core dumps peuvent contenir des mots de passe, tokens, cl√©s API en m√©moire.

**Solutions** :
- Effacer les donn√©es sensibles de la m√©moire apr√®s usage
- Restreindre l'acc√®s aux core dumps : `chmod 600 /var/crash/core.*`
- Utiliser `core_pattern` avec filtrage : `|/usr/local/bin/filter_core`

---

## Checklist d'analyse d'un core dump en production

1. ‚úÖ **Collecter les informations de contexte**
   - Date et heure du crash
   - Version exacte de l'application
   - Charge syst√®me au moment du crash
   - Logs applicatifs avant le crash

2. ‚úÖ **V√©rifier les pr√©requis**
   - Binaire correspondant au core dump
   - Symboles de d√©bogage disponibles
   - Biblioth√®ques partag√©es identiques

3. ‚úÖ **Analyse initiale avec GDB**
   ```
   bt full
   info threads
   info registers
   ```

4. ‚úÖ **Identifier le point de crash**
   - Frame 0 de la backtrace
   - Code source correspondant
   - Valeurs des variables locales

5. ‚úÖ **Remonter la cha√Æne d'appels**
   - Examiner chaque frame
   - Chercher l'origine du probl√®me

6. ‚úÖ **Hypoth√®ses et validation**
   - Formuler des hypoth√®ses (NULL pointer, overflow, ...)
   - V√©rifier avec `print`, `x/`, `whatis`

7. ‚úÖ **Documentation et correction**
   - Documenter la cause racine
   - Cr√©er un ticket avec la backtrace
   - Ajouter un test de non-r√©gression

---

## Ressources pour aller plus loin

### Documentation officielle
- `man core` : Format des core dumps
- `man gdb` : Manuel complet de GDB
- [GDB Documentation](https://sourceware.org/gdb/documentation/)

### Outils avanc√©s
- **Crash Utility** : Analyse de core dumps kernel
- **RetroScope** : Time-travel debugging
- **rr (Mozilla)** : Enregistrement et replay de l'ex√©cution

### Lectures recommand√©es
- *"Advanced Linux Programming"* par Mark Mitchell et al.
- *"The Art of Debugging with GDB"* par Norman Matloff

---

## R√©sum√©

L'analyse de core dumps en production est une comp√©tence essentielle pour tout ing√©nieur syst√®me. Les points cl√©s √† retenir :

- **Pr√©parez-vous** : Configurez la g√©n√©ration de core dumps **avant** le crash
- **Symboles de d√©bogage** : Toujours compiler avec `-g` et archiver les binaires
- **GDB est votre ami** : Ma√Ætrisez `bt`, `frame`, `print`, `x/`
- **M√©thodologie** : Analyse syst√©matique du point de crash vers la cause racine
- **S√©curit√©** : Prot√©gez les core dumps, ils contiennent des donn√©es sensibles
- **Automatisation** : Scripts d'analyse et rotation automatique

Un core dump bien analys√© peut transformer un bug myst√©rieux en production en une correction triviale. C'est votre **bo√Æte noire** pour comprendre ce qui s'est r√©ellement pass√© quand tout a mal tourn√©.

---


‚è≠Ô∏è [R√©solution de memory leaks](/35-debugging-code-complexe/02-resolution-memory-leaks.md)
