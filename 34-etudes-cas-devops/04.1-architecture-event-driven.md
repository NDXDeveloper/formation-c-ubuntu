üîù Retour au [Sommaire](/SOMMAIRE.md)

# 34.4.1 Architecture Event-Driven

## Introduction

Dans cette section, nous allons explorer l'**architecture event-driven** (pilot√©e par les √©v√©nements), une approche fondamentale pour cr√©er des serveurs web performants et scalables en C.

Un serveur web doit g√©rer **plusieurs clients simultan√©ment**. Comment faire pour qu'un seul processus puisse servir des dizaines, voire des milliers de connexions en m√™me temps ? C'est le d√©fi que r√©sout l'architecture event-driven.

---

## Le Probl√®me : G√©rer Plusieurs Clients

### Approche na√Øve : Un client √† la fois

Voici le serveur le plus simple possible :

```c
int server_fd = socket(...);  
bind(server_fd, ...);  
listen(server_fd, ...);  

while (1) {
    int client_fd = accept(server_fd, ...);  // Attendre un client

    // Lire la requ√™te
    read(client_fd, buffer, ...);

    // Traiter la requ√™te
    handle_request(buffer);

    // Envoyer la r√©ponse
    write(client_fd, response, ...);

    close(client_fd);
}
```

**Probl√®me majeur :** Ce serveur ne peut g√©rer **qu'un seul client √† la fois**.

```
Client A se connecte
    ‚îî‚îÄ> Serveur traite A (peut prendre 1 seconde)
        ‚îî‚îÄ> Pendant ce temps, Client B attend...
            ‚îî‚îÄ> Client C attend aussi...
                ‚îî‚îÄ> Client D timeout...
```

Si le traitement d'une requ√™te prend 1 seconde et que 100 clients arrivent en m√™me temps, le dernier attendra **100 secondes** !

### Solution 1 : Un thread par client

```c
void *handle_client(void *arg) {
    int client_fd = *(int *)arg;
    // Traiter la requ√™te
    close(client_fd);
    return NULL;
}

while (1) {
    int client_fd = accept(server_fd, ...);

    pthread_t thread;
    pthread_create(&thread, NULL, handle_client, &client_fd);
    pthread_detach(thread);
}
```

**Avantages :**
- ‚úÖ Plusieurs clients simultan√©s
- ‚úÖ Simple √† comprendre

**Inconv√©nients :**
- ‚ùå Co√ªt √©lev√© de cr√©ation de threads (m√©moire, CPU)
- ‚ùå Limite du nombre de threads (~1000-10000)
- ‚ùå Complexit√© de synchronisation
- ‚ùå Context switching co√ªteux

**Probl√®me :** Si vous avez 10 000 connexions, cr√©er 10 000 threads n'est pas viable.

### Solution 2 : Pool de threads

```c
// File d'attente de connexions
queue_t connection_queue;

// Threads workers
void *worker(void *arg) {
    while (1) {
        int client_fd = queue_pop(&connection_queue);
        handle_request(client_fd);
        close(client_fd);
    }
}

// Cr√©er N threads au d√©marrage
for (int i = 0; i < NUM_WORKERS; i++) {
    pthread_create(&workers[i], NULL, worker, NULL);
}

// Thread principal
while (1) {
    int client_fd = accept(server_fd, ...);
    queue_push(&connection_queue, client_fd);
}
```

**Avantages :**
- ‚úÖ Nombre fixe de threads
- ‚úÖ Meilleure utilisation des ressources

**Inconv√©nients :**
- ‚ùå Toujours limit√© par le nombre de workers
- ‚ùå Probl√®me si un worker bloque (I/O lent)
- ‚ùå Complexit√© de gestion de la queue

### Solution 3 : Architecture Event-Driven ‚≠ê

```c
// Boucle d'√©v√©nements (event loop)
while (1) {
    // Attendre qu'un descripteur soit pr√™t
    int ready_fds = epoll_wait(...);

    for (int i = 0; i < ready_fds; i++) {
        if (fd is server_fd) {
            // Nouvelle connexion
            int client_fd = accept(server_fd, ...);
            // Ajouter client_fd √† la surveillance
        } else {
            // Donn√©es disponibles sur un client
            handle_client_data(fd);
        }
    }
}
```

**Avantages :**
- ‚úÖ **Un seul thread** peut g√©rer des milliers de connexions
- ‚úÖ Pas de context switching
- ‚úÖ Scalabilit√© exceptionnelle
- ‚úÖ Faible consommation m√©moire

**Principe cl√© :** Au lieu d'**attendre** qu'une op√©ration se termine (blocking), on **surveille** plusieurs descripteurs et on r√©agit quand quelque chose se passe.

---

## Comprendre les I/O Blocking vs Non-Blocking

### I/O Blocking (par d√©faut)

Par d√©faut, les op√©rations I/O **bloquent** le processus :

```c
int n = read(fd, buffer, 1024);
// Le processus est BLOQU√â ici jusqu'√† ce que des donn√©es arrivent
printf("Donn√©es re√ßues: %d bytes\n", n);
```

**Ce qui se passe :**
1. L'appel `read()` suspend le processus
2. Le kernel met le processus en sommeil
3. Quand des donn√©es arrivent, le kernel r√©veille le processus
4. `read()` retourne avec les donn√©es

**Probl√®me :** Pendant que vous lisez sur `fd1`, vous ne pouvez pas lire sur `fd2`.

### I/O Non-Blocking

Avec des sockets en mode non-blocking :

```c
// Activer le mode non-blocking
int flags = fcntl(fd, F_GETFL, 0);  
fcntl(fd, F_SETFL, flags | O_NONBLOCK);  

// Maintenant read() ne bloque plus
int n = read(fd, buffer, 1024);

if (n < 0 && errno == EAGAIN) {
    // Pas de donn√©es disponibles, mais pas d'erreur
    printf("Rien √† lire pour le moment\n");
} else if (n > 0) {
    printf("Donn√©es re√ßues: %d bytes\n", n);
}
```

**Ce qui se passe :**
1. `read()` tente de lire imm√©diatement
2. Si pas de donn√©es : retourne -1 avec `errno = EAGAIN`
3. Si des donn√©es : retourne le nombre d'octets lus

**Avantage :** Le processus n'est jamais bloqu√©, il peut continuer.

**Mais :** Comment savoir **quand** des donn√©es sont disponibles ?

‚Üí **R√©ponse :** Les m√©canismes de multiplexage I/O.

---

## M√©canismes de Multiplexage I/O

Linux offre trois APIs principales pour surveiller plusieurs descripteurs :

| M√©canisme | Ann√©e | Performance | Limite | Complexit√© |
|-----------|-------|-------------|--------|------------|
| `select()` | 1983 | Faible | 1024 fds | Simple |
| `poll()` | 1986 | Moyenne | Pas de limite | Moyenne |
| `epoll()` | 2002 | Excellente | ~100k fds | Moyenne |

### select() - Le plus ancien

```c
#include <sys/select.h>

fd_set read_fds;  
FD_ZERO(&read_fds);  
FD_SET(server_fd, &read_fds);  
FD_SET(client_fd, &read_fds);  

int max_fd = (server_fd > client_fd) ? server_fd : client_fd;

// Attendre qu'un descripteur soit pr√™t
int ready = select(max_fd + 1, &read_fds, NULL, NULL, NULL);

if (FD_ISSET(server_fd, &read_fds)) {
    // Nouvelle connexion disponible
}

if (FD_ISSET(client_fd, &read_fds)) {
    // Donn√©es disponibles sur client_fd
}
```

**Limites de select() :**
- Maximum **1024 descripteurs** (FD_SETSIZE)
- Doit copier le `fd_set` √† chaque appel
- Complexit√© O(n) : parcourt tous les descripteurs

**Quand l'utiliser :** Pour des serveurs avec <100 connexions simultan√©es.

### poll() - Am√©lioration de select()

```c
#include <poll.h>

struct pollfd fds[2];

// Surveiller le serveur
fds[0].fd = server_fd;  
fds[0].events = POLLIN;  // Int√©ress√© par la lecture  

// Surveiller un client
fds[1].fd = client_fd;  
fds[1].events = POLLIN;  

// Attendre des √©v√©nements
int ready = poll(fds, 2, -1);  // -1 = attente infinie

if (fds[0].revents & POLLIN) {
    // Nouvelle connexion
}

if (fds[1].revents & POLLIN) {
    // Donn√©es sur client
}
```

**Avantages sur select() :**
- Pas de limite de 1024 descripteurs
- Interface plus claire

**Limites :**
- Toujours O(n) en performance
- Doit passer tous les descripteurs √† chaque appel

**Quand l'utiliser :** Serveurs avec quelques centaines de connexions.

### epoll() - Le plus performant (Linux) ‚≠ê

```c
#include <sys/epoll.h>

// Cr√©er une instance epoll
int epoll_fd = epoll_create1(0);

// Ajouter le serveur √† la surveillance
struct epoll_event ev;  
ev.events = EPOLLIN;  // Lecture  
ev.data.fd = server_fd;  
epoll_ctl(epoll_fd, EPOLL_CTL_ADD, server_fd, &ev);  

// Boucle d'√©v√©nements
struct epoll_event events[MAX_EVENTS];

while (1) {
    int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);

    for (int i = 0; i < n; i++) {
        if (events[i].data.fd == server_fd) {
            // Nouvelle connexion
            int client_fd = accept(server_fd, NULL, NULL);

            // Ajouter le client √† la surveillance
            ev.events = EPOLLIN;
            ev.data.fd = client_fd;
            epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &ev);
        } else {
            // Donn√©es sur un client
            handle_client(events[i].data.fd);
        }
    }
}
```

**Avantages majeurs :**
- **Complexit√© O(1)** : Ne retourne QUE les descripteurs pr√™ts
- Scalable jusqu'√† 100 000+ connexions
- Le kernel maintient l'√©tat (pas de copie √† chaque appel)
- Edge-triggered et level-triggered au choix

**Quand l'utiliser :** Production, serveurs haute performance.

---

## Architecture Event-Driven avec epoll

### Principe de fonctionnement

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KERNEL LINUX                             ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ           EPOLL INSTANCE                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Liste des descripteurs surveill√©s:                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - server_fd (EPOLLIN)                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - client_fd_1 (EPOLLIN)                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - client_fd_2 (EPOLLIN)                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - client_fd_3 (EPOLLIN)                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - ...                                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  File d'√©v√©nements pr√™ts: [client_fd_1, server_fd]  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                          ‚ñ≤                                  ‚îÇ
‚îÇ                          ‚îÇ Notification                     ‚îÇ
‚îÇ                          ‚îÇ                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ Socket 1 ‚îÇ  ‚îÇ Socket 2 ‚îÇ  ‚îÇ Socket 3 ‚îÇ  ...              ‚îÇ
‚îÇ  ‚îÇ  (pr√™t)  ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                         ‚ñ≤
        ‚îÇ Donn√©es arrivent                        ‚îÇ
        ‚ñº                                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     R√âSEAU                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PROCESSUS SERVEUR                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  while (1) {                                                ‚îÇ
‚îÇ      // Dormir jusqu'√† ce qu'un √©v√©nement arrive            ‚îÇ
‚îÇ      int n = epoll_wait(epoll_fd, events, MAX, -1);         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ      // Traiter chaque √©v√©nement                            ‚îÇ
‚îÇ      for (int i = 0; i < n; i++) {                          ‚îÇ
‚îÇ          if (event is nouvelle connexion)                   ‚îÇ
‚îÇ              accept + ajouter √† epoll                       ‚îÇ
‚îÇ          else                                               ‚îÇ
‚îÇ              lire/traiter la requ√™te                        ‚îÇ
‚îÇ      }                                                      ‚îÇ
‚îÇ  }                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Impl√©mentation compl√®te d'un serveur event-driven

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <errno.h>
#include <fcntl.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <netinet/in.h>
#include <arpa/inet.h>

#define PORT 8080
#define MAX_EVENTS 64
#define BUFFER_SIZE 4096

// Mettre un socket en mode non-blocking
int set_nonblocking(int fd) {
    int flags = fcntl(fd, F_GETFL, 0);
    if (flags == -1) {
        perror("fcntl F_GETFL");
        return -1;
    }

    if (fcntl(fd, F_SETFL, flags | O_NONBLOCK) == -1) {
        perror("fcntl F_SETFL");
        return -1;
    }

    return 0;
}

// Cr√©er le socket serveur
int create_server_socket(int port) {
    int server_fd;
    struct sockaddr_in address;
    int opt = 1;

    // Cr√©er le socket
    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd == -1) {
        perror("socket");
        return -1;
    }

    // R√©utiliser l'adresse
    if (setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt)) < 0) {
        perror("setsockopt");
        close(server_fd);
        return -1;
    }

    // Configuration de l'adresse
    memset(&address, 0, sizeof(address));
    address.sin_family = AF_INET;
    address.sin_addr.s_addr = INADDR_ANY;
    address.sin_port = htons(port);

    // Bind
    if (bind(server_fd, (struct sockaddr *)&address, sizeof(address)) < 0) {
        perror("bind");
        close(server_fd);
        return -1;
    }

    // Listen
    if (listen(server_fd, SOMAXCONN) < 0) {
        perror("listen");
        close(server_fd);
        return -1;
    }

    // Mettre en mode non-blocking
    if (set_nonblocking(server_fd) < 0) {
        close(server_fd);
        return -1;
    }

    printf("Serveur d√©marr√© sur le port %d\n", port);
    return server_fd;
}

// G√©rer une nouvelle connexion
void handle_new_connection(int epoll_fd, int server_fd) {
    struct sockaddr_in client_addr;
    socklen_t client_len = sizeof(client_addr);

    // Accepter toutes les connexions en attente
    while (1) {
        int client_fd = accept(server_fd,
                               (struct sockaddr *)&client_addr,
                               &client_len);

        if (client_fd < 0) {
            if (errno == EAGAIN || errno == EWOULDBLOCK) {
                // Plus de connexions en attente
                break;
            } else {
                perror("accept");
                break;
            }
        }

        // Afficher l'adresse du client
        char client_ip[INET_ADDRSTRLEN];
        inet_ntop(AF_INET, &client_addr.sin_addr, client_ip, sizeof(client_ip));
        printf("Nouvelle connexion de %s:%d (fd=%d)\n",
               client_ip, ntohs(client_addr.sin_port), client_fd);

        // Mettre le client en mode non-blocking
        if (set_nonblocking(client_fd) < 0) {
            close(client_fd);
            continue;
        }

        // Ajouter le client √† epoll
        struct epoll_event ev;
        ev.events = EPOLLIN | EPOLLET;  // Edge-triggered
        ev.data.fd = client_fd;

        if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &ev) < 0) {
            perror("epoll_ctl: client_fd");
            close(client_fd);
            continue;
        }
    }
}

// G√©rer les donn√©es d'un client
void handle_client_data(int epoll_fd, int client_fd) {
    char buffer[BUFFER_SIZE];

    while (1) {
        ssize_t bytes_read = read(client_fd, buffer, sizeof(buffer) - 1);

        if (bytes_read < 0) {
            if (errno == EAGAIN || errno == EWOULDBLOCK) {
                // Toutes les donn√©es ont √©t√© lues
                break;
            } else {
                perror("read");
                close(client_fd);
                epoll_ctl(epoll_fd, EPOLL_CTL_DEL, client_fd, NULL);
                return;
            }
        } else if (bytes_read == 0) {
            // Client a ferm√© la connexion
            printf("Client d√©connect√© (fd=%d)\n", client_fd);
            close(client_fd);
            epoll_ctl(epoll_fd, EPOLL_CTL_DEL, client_fd, NULL);
            return;
        }

        buffer[bytes_read] = '\0';
        printf("Re√ßu de fd=%d: %zd bytes\n", client_fd, bytes_read);

        // R√©ponse HTTP simple
        const char *response =
            "HTTP/1.1 200 OK\r\n"
            "Content-Type: text/plain\r\n"
            "Content-Length: 13\r\n"
            "\r\n"
            "Hello, World!";

        // Envoyer la r√©ponse
        ssize_t bytes_written = write(client_fd, response, strlen(response));
        if (bytes_written < 0) {
            perror("write");
        }

        // Fermer la connexion (HTTP/1.0 style)
        close(client_fd);
        epoll_ctl(epoll_fd, EPOLL_CTL_DEL, client_fd, NULL);
        return;
    }
}

// Boucle d'√©v√©nements principale
void event_loop(int server_fd) {
    struct epoll_event ev, events[MAX_EVENTS];
    int epoll_fd;

    // Cr√©er l'instance epoll
    epoll_fd = epoll_create1(0);
    if (epoll_fd < 0) {
        perror("epoll_create1");
        return;
    }

    // Ajouter le serveur √† epoll
    ev.events = EPOLLIN;
    ev.data.fd = server_fd;
    if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, server_fd, &ev) < 0) {
        perror("epoll_ctl: server_fd");
        close(epoll_fd);
        return;
    }

    printf("Boucle d'√©v√©nements d√©marr√©e\n");

    // Boucle infinie
    while (1) {
        // Attendre des √©v√©nements
        int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);

        if (n < 0) {
            perror("epoll_wait");
            break;
        }

        // Traiter chaque √©v√©nement
        for (int i = 0; i < n; i++) {
            if (events[i].data.fd == server_fd) {
                // Nouvelle connexion
                handle_new_connection(epoll_fd, server_fd);
            } else {
                // Donn√©es d'un client
                handle_client_data(epoll_fd, events[i].data.fd);
            }
        }
    }

    close(epoll_fd);
}

int main(void) {
    int server_fd = create_server_socket(PORT);
    if (server_fd < 0) {
        return EXIT_FAILURE;
    }

    event_loop(server_fd);

    close(server_fd);
    return EXIT_SUCCESS;
}
```

### Explication du code

#### 1. Mode non-blocking

```c
int set_nonblocking(int fd) {
    int flags = fcntl(fd, F_GETFL, 0);  // R√©cup√©rer flags actuels
    fcntl(fd, F_SETFL, flags | O_NONBLOCK);  // Ajouter O_NONBLOCK
    return 0;
}
```

**Pourquoi ?** En mode non-blocking, `accept()`, `read()`, `write()` ne bloquent jamais.

#### 2. Cr√©ation de l'instance epoll

```c
int epoll_fd = epoll_create1(0);
```

`epoll_fd` est un **descripteur** qui repr√©sente l'instance epoll.

#### 3. Ajout du serveur √† epoll

```c
struct epoll_event ev;  
ev.events = EPOLLIN;  // Surveiller la lecture  
ev.data.fd = server_fd;  
epoll_ctl(epoll_fd, EPOLL_CTL_ADD, server_fd, &ev);  
```

On dit √† epoll : "Surveille `server_fd`, notifie-moi quand des donn√©es sont pr√™tes √† √™tre lues."

#### 4. Boucle d'√©v√©nements

```c
while (1) {
    int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);

    for (int i = 0; i < n; i++) {
        // Traiter events[i]
    }
}
```

- `epoll_wait()` **bloque** jusqu'√† ce qu'un √©v√©nement se produise
- Retourne le nombre d'√©v√©nements pr√™ts
- Remplit le tableau `events[]` avec les descripteurs pr√™ts

#### 5. Gestion des nouvelles connexions

```c
if (events[i].data.fd == server_fd) {
    int client_fd = accept(server_fd, ...);
    set_nonblocking(client_fd);
    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &ev);
}
```

Quand `server_fd` est pr√™t, cela signifie qu'une nouvelle connexion attend. On l'accepte et on l'ajoute √† epoll.

#### 6. Gestion des donn√©es clients

```c
else {
    handle_client_data(epoll_fd, events[i].data.fd);
}
```

Quand un client est pr√™t, on lit ses donn√©es.

---

## Edge-Triggered vs Level-Triggered

epoll supporte deux modes de notification :

### Level-Triggered (par d√©faut)

**Comportement :** epoll notifie **tant que** des donn√©es sont disponibles.

```c
ev.events = EPOLLIN;  // Level-triggered par d√©faut
```

**Exemple :**
1. Client envoie 100 bytes
2. `epoll_wait()` notifie ‚Üí vous lisez 50 bytes
3. **`epoll_wait()` notifie √† nouveau** (il reste 50 bytes)
4. Vous lisez les 50 bytes restants
5. `epoll_wait()` ne notifie plus

**Avantages :**
- Plus simple √† utiliser
- Moins sujet aux bugs

**Inconv√©nients :**
- Peut notifier trop souvent si vous ne lisez pas tout

### Edge-Triggered (recommand√© pour la performance)

**Comportement :** epoll notifie **une seule fois** quand l'√©tat change.

```c
ev.events = EPOLLIN | EPOLLET;  // Edge-triggered
```

**Exemple :**
1. Client envoie 100 bytes
2. `epoll_wait()` notifie **une fois**
3. Vous devez tout lire (boucle jusqu'√† EAGAIN)
4. Si vous ne lisez que 50 bytes, vous ne serez plus notifi√© !

**Obligation :** Lire **toutes** les donn√©es disponibles :

```c
while (1) {
    ssize_t n = read(fd, buffer, sizeof(buffer));

    if (n < 0) {
        if (errno == EAGAIN) {
            break;  // Tout lu
        }
        // Erreur
    }

    // Traiter les donn√©es
}
```

**Avantages :**
- Plus efficace (moins de notifications)
- Meilleure performance avec beaucoup de connexions

**Inconv√©nients :**
- Plus complexe √† impl√©menter correctement
- Facile de perdre des donn√©es si mal g√©r√©

**Recommandation :** Utilisez **EPOLLET** pour un serveur de production.

---

## Patterns Courants

### Pattern 1 : Accepter toutes les connexions en attente

```c
void handle_new_connection(int epoll_fd, int server_fd) {
    while (1) {
        int client_fd = accept(server_fd, NULL, NULL);

        if (client_fd < 0) {
            if (errno == EAGAIN || errno == EWOULDBLOCK) {
                break;  // Plus de connexions
            }
            perror("accept");
            break;
        }

        // Configurer et ajouter √† epoll
        set_nonblocking(client_fd);
        epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &ev);
    }
}
```

**Pourquoi une boucle ?** En mode edge-triggered, on est notifi√© une seule fois. Il faut accepter toutes les connexions en attente.

### Pattern 2 : Lire toutes les donn√©es disponibles

```c
void handle_client_data(int client_fd) {
    char buffer[4096];

    while (1) {
        ssize_t n = read(client_fd, buffer, sizeof(buffer));

        if (n < 0) {
            if (errno == EAGAIN) {
                break;  // Tout lu
            }
            // Erreur vraie
            close(client_fd);
            return;
        }

        if (n == 0) {
            // Connexion ferm√©e
            close(client_fd);
            return;
        }

        // Traiter buffer[0..n-1]
        process_data(buffer, n);
    }
}
```

### Pattern 3 : √âcriture avec gestion du buffer

Si vous avez beaucoup de donn√©es √† envoyer :

```c
typedef struct {
    int fd;
    char *write_buffer;
    size_t write_offset;
    size_t write_length;
} client_t;

void handle_write_ready(int epoll_fd, client_t *client) {
    while (client->write_offset < client->write_length) {
        ssize_t n = write(client->fd,
                         client->write_buffer + client->write_offset,
                         client->write_length - client->write_offset);

        if (n < 0) {
            if (errno == EAGAIN) {
                // Socket plein, r√©essayer plus tard
                // Passer en mode EPOLLOUT
                struct epoll_event ev;
                ev.events = EPOLLIN | EPOLLOUT | EPOLLET;
                ev.data.ptr = client;
                epoll_ctl(epoll_fd, EPOLL_CTL_MOD, client->fd, &ev);
                return;
            }
            // Erreur
            return;
        }

        client->write_offset += n;
    }

    // Tout √©crit, repasser en mode EPOLLIN uniquement
    struct epoll_event ev;
    ev.events = EPOLLIN | EPOLLET;
    ev.data.ptr = client;
    epoll_ctl(epoll_fd, EPOLL_CTL_MOD, client->fd, &ev);
}
```

### Pattern 4 : Gestion d'√©tat par connexion

Pour des protocoles plus complexes :

```c
typedef enum {
    STATE_READING_HEADERS,
    STATE_READING_BODY,
    STATE_PROCESSING,
    STATE_WRITING_RESPONSE,
    STATE_DONE
} client_state_t;

typedef struct {
    int fd;
    client_state_t state;
    char *read_buffer;
    size_t read_offset;
    // ... autres champs
} client_t;

void handle_client(client_t *client) {
    switch (client->state) {
        case STATE_READING_HEADERS:
            read_headers(client);
            if (headers_complete(client)) {
                client->state = STATE_READING_BODY;
            }
            break;

        case STATE_READING_BODY:
            read_body(client);
            if (body_complete(client)) {
                client->state = STATE_PROCESSING;
            }
            break;

        case STATE_PROCESSING:
            process_request(client);
            client->state = STATE_WRITING_RESPONSE;
            break;

        case STATE_WRITING_RESPONSE:
            write_response(client);
            if (response_complete(client)) {
                client->state = STATE_DONE;
                close(client->fd);
            }
            break;
    }
}
```

---

## Avantages de l'Architecture Event-Driven

### 1. Scalabilit√©

```
Mod√®le thread-per-connection:
    100 connexions = 100 threads √ó 8 MB stack = 800 MB

Mod√®le event-driven:
    100 connexions = 1 thread + 100 √ó petite structure = ~50 MB
```

**R√©sultat :** Peut g√©rer 10x-100x plus de connexions.

### 2. Performance

- **Pas de context switching** : Un seul thread
- **Cache-friendly** : Donn√©es localis√©es
- **Moins de locks** : Pas de synchronisation n√©cessaire

### 3. Pr√©visibilit√©

- Pas de race conditions (single-threaded)
- Plus facile √† debugger
- Comportement d√©terministe

---

## Limitations et Solutions

### Limitation 1 : Op√©rations bloquantes

**Probl√®me :** Si vous faites une op√©ration lourde (calcul, I/O disque), tout le serveur se bloque.

```c
void handle_request(client_t *client) {
    // ‚ùå MAUVAIS : Bloque toute la boucle d'√©v√©nements
    sleep(5);  // Simule un calcul long

    send_response(client);
}
```

**Solutions :**

**Option A :** Thread pool pour les t√¢ches lourdes
```c
void handle_request(client_t *client) {
    // D√©l√©guer √† un thread worker
    thread_pool_submit(process_heavy_task, client);
}
```

**Option B :** I/O asynchrone (AIO, io_uring)
```c
// Utiliser io_uring pour I/O disque non-bloquant
```

### Limitation 2 : Utilisation CPU

**Probl√®me :** Un seul thread ne peut pas exploiter tous les c≈ìurs CPU.

**Solution :** Mod√®le hybride

```c
// Cr√©er N processus/threads, chacun avec sa boucle epoll
for (int i = 0; i < num_cores; i++) {
    if (fork() == 0) {
        // Processus enfant
        event_loop(server_fd);
        exit(0);
    }
}

// Processus parent
wait_all_children();
```

**Exemple r√©el :** Nginx utilise un worker par c≈ìur.

### Limitation 3 : Complexit√© du code

**Probl√®me :** Le code devient une machine √† √©tats complexe.

**Solution :** Utiliser des frameworks ou g√©n√©rateurs

- **libevent** : Abstraction portable
- **libuv** : Utilis√© par Node.js
- **libev** : L√©ger et rapide

---

## Comparaison avec d'Autres Architectures

| Approche | Scalabilit√© | Complexit√© | Utilisation CPU | Use Case |
|----------|-------------|------------|-----------------|----------|
| **Thread per connection** | Faible (1000s) | Simple | Multi-core | Petits serveurs |
| **Thread pool** | Moyenne (10000s) | Moyenne | Multi-core | Applications g√©n√©rales |
| **Event-driven** | Excellente (100k+) | Moyenne/√âlev√©e | Single-core | Serveurs haute perf |
| **Event-driven + workers** | Excellente | √âlev√©e | Multi-core | Production (Nginx) |
| **Coroutines** | Excellente | Moyenne | Single-core | Go, Erlang |

---

## Exemples de Serveurs Utilisant epoll

### Nginx

```
- Architecture multi-processus
- 1 processus ma√Ætre + N workers (1 par CPU core)
- Chaque worker : boucle epoll
- Peut g√©rer 100 000+ connexions simultan√©es
- Consommation m√©moire : ~10 Mo par worker
```

### Redis

```
- Single-threaded avec epoll
- Boucle d'√©v√©nements pour commandes clients
- Thread pool pour t√¢ches lourdes (BGSAVE, BGREWRITEAOF)
- Performance : 100 000+ requ√™tes/seconde
```

### HAProxy

```
- Event-driven avec epoll
- Load balancing haute performance
- Millions de connexions concurrentes possibles
```

---

## Bonnes Pratiques

### 1. Toujours utiliser le mode non-blocking

```c
// ‚úÖ BON
set_nonblocking(client_fd);

// ‚ùå MAUVAIS : Garder en mode blocking
// accept() pourrait bloquer toute la boucle
```

### 2. G√©rer les erreurs EAGAIN correctement

```c
ssize_t n = read(fd, buffer, size);

if (n < 0) {
    if (errno == EAGAIN || errno == EWOULDBLOCK) {
        // Normal en non-blocking, pas d'erreur
        return;
    }
    // Vraie erreur
    handle_error();
}
```

### 3. Lire/√©crire en boucle en edge-triggered

```c
// ‚úÖ BON : Boucle jusqu'√† EAGAIN
while (1) {
    n = read(fd, buffer, size);
    if (n < 0 && errno == EAGAIN) break;
    process(buffer, n);
}

// ‚ùå MAUVAIS : Lire une seule fois
n = read(fd, buffer, size);  // Peut perdre des donn√©es
```

### 4. Limiter le nombre de connexions

```c
#define MAX_CONNECTIONS 10000

if (num_connections >= MAX_CONNECTIONS) {
    // Refuser la nouvelle connexion
    int fd = accept(server_fd, NULL, NULL);
    close(fd);
}
```

### 5. Timeout pour les connexions inactives

```c
typedef struct {
    int fd;
    time_t last_activity;
} client_t;

// Dans la boucle
time_t now = time(NULL);  
for (each client) {  
    if (now - client->last_activity > TIMEOUT) {
        close(client->fd);
        epoll_ctl(epoll_fd, EPOLL_CTL_DEL, client->fd, NULL);
    }
}
```

---

## D√©bogage

### V√©rifier les descripteurs

```bash
# Voir les connexions actives
lsof -p $(pidof serveur) | grep TCP

# Compter les connexions
lsof -p $(pidof serveur) | grep TCP | wc -l
```

### Simuler beaucoup de connexions

```bash
# Utiliser Apache Bench
ab -n 10000 -c 1000 http://localhost:8080/

# Ou wrk
wrk -t 4 -c 1000 -d 30s http://localhost:8080/
```

### Profiling

```bash
# Voir o√π le temps est pass√©
perf record -g ./serveur  
perf report  
```

---

## R√©sum√©

L'architecture event-driven avec epoll permet :

- ‚úÖ **Gestion de milliers de connexions** avec un seul thread
- ‚úÖ **Performance exceptionnelle** : pas de context switching
- ‚úÖ **Consommation m√©moire minimale**
- ‚úÖ **Scalabilit√© horizontale** : facile de dupliquer

**Composants cl√©s :**
1. **epoll** : Multiplexage I/O haute performance
2. **Mode non-blocking** : Aucune op√©ration ne bloque
3. **Boucle d'√©v√©nements** : R√©agir aux √©v√©nements r√©seau
4. **Machine √† √©tats** : G√©rer le cycle de vie des connexions

**Prochaine √©tape :** Nous allons utiliser cette architecture pour impl√©menter le parsing HTTP et le serving de fichiers statiques dans les sections suivantes.

---

## Pour Aller Plus Loin

### Ressources

- **man epoll** : Documentation compl√®te de epoll
- **The C10K Problem** : Article fondateur sur la scalabilit√© des serveurs
- **nginx source code** : Impl√©mentation de production

### Technologies Avanc√©es

- **io_uring** : Nouvelle API Linux (encore plus performante qu'epoll)
- **DPDK** : Bypass du kernel pour performance ultime
- **XDP/eBPF** : Traitement r√©seau dans le kernel

### Alternatives Multi-Plateformes

- **libevent** : Portable (Linux, BSD, Windows)
- **libuv** : Utilis√© par Node.js (cross-platform)
- **kqueue** : √âquivalent d'epoll sur BSD/macOS

L'architecture event-driven est la fondation des serveurs web modernes haute performance. Vous √™tes maintenant pr√™t √† l'appliquer √† la construction d'un serveur HTTP complet !

‚è≠Ô∏è [HTTP parsing](/34-etudes-cas-devops/04.2-http-parsing.md)
