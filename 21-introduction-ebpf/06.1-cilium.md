üîù Retour au [Sommaire](/SOMMAIRE.md)

# 21.6.1 Cilium

## Introduction

**Cilium** est l'une des applications les plus embl√©matiques et les plus avanc√©es d'eBPF dans le monde de la production. C'est une solution de **networking, s√©curit√© et observabilit√©** pour les environnements cloud-native, en particulier Kubernetes, qui tire pleinement parti de la puissance d'eBPF.

Si vous vous demandez "√Ä quoi peut bien servir eBPF en production ?", Cilium est la r√©ponse parfaite. C'est un projet mature, utilis√© par des milliers d'entreprises √† travers le monde, et qui d√©montre comment eBPF peut r√©volutionner l'infrastructure moderne.

Dans cette section, nous allons d√©couvrir :
- Ce qu'est Cilium et pourquoi il existe
- Comment il utilise eBPF pour r√©soudre des probl√®mes complexes
- Ses principales fonctionnalit√©s
- Pourquoi c'est important pour les DevOps et SREs

**Note** : Vous n'avez pas besoin de conna√Ætre Kubernetes en profondeur pour comprendre cette section. Nous expliquerons les concepts au fur et √† mesure.

## Qu'est-ce que Cilium ?

### D√©finition simple

**Cilium** est une plateforme qui fournit trois fonctionnalit√©s essentielles pour les applications cloud-native :

1. **Networking** : Connectivit√© r√©seau entre les conteneurs/pods
2. **S√©curit√©** : Contr√¥le d'acc√®s r√©seau et protection
3. **Observabilit√©** : Visibilit√© sur le trafic et les m√©triques r√©seau

La particularit√© ? Tout cela est impl√©ment√© avec **eBPF** dans le kernel Linux, ce qui offre des performances exceptionnelles et une flexibilit√© in√©gal√©e.

### Origine et histoire

- **2016** : Cr√©ation du projet par Thomas Graf chez Cisco
- **2017** : Open-source et adoption croissante
- **2021** : Devient un projet CNCF (Cloud Native Computing Foundation)
- **2023** : Atteint le statut "Graduated" CNCF (niveau le plus √©lev√©, comme Kubernetes, Prometheus)
- **2024-2025** : Adoption massive dans l'industrie (Google GKE, AWS EKS, Azure AKS)

**Fun fact** : Le nom "Cilium" vient du latin et d√©signe les cils des cellules. C'est une r√©f√©rence √† la fa√ßon dont eBPF "capte" et traite le trafic r√©seau au niveau le plus bas.

## Le probl√®me : Networking Kubernetes traditionnel

Pour comprendre pourquoi Cilium est r√©volutionnaire, il faut d'abord comprendre les limitations des solutions traditionnelles.

### Kubernetes sans Cilium

Dans un cluster Kubernetes traditionnel (avec par exemple kube-proxy + Calico/Flannel), le networking fonctionne via :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Pod Application                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           iptables (kube-proxy)                     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚Ä¢ Des milliers de r√®gles iptables                  ‚îÇ
‚îÇ  ‚Ä¢ Traitement s√©quentiel (O(n))                     ‚îÇ
‚îÇ  ‚Ä¢ Overhead sur CPU √©lev√©                           ‚îÇ
‚îÇ  ‚Ä¢ Pas de visibilit√© granulaire                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Stack r√©seau kernel (netfilter)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
                R√©seau
```

**Probl√®mes de cette approche :**

- ‚ùå **Performance** : iptables devient tr√®s lent avec des milliers de services
- ‚ùå **Scalabilit√©** : Overhead CPU proportionnel au nombre de r√®gles
- ‚ùå **Latence** : Chaque paquet traverse de nombreuses couches
- ‚ùå **Observabilit√© limit√©e** : Difficile de savoir ce qui se passe r√©ellement
- ‚ùå **D√©bogage complexe** : Logs √©parpill√©s, pas de visibilit√© bout-en-bout
- ‚ùå **S√©curit√© limit√©e** : Policies basiques au niveau IP/port uniquement

### Exemple concret

Imaginez un cluster Kubernetes avec :
- 1000 pods
- 200 services
- Des network policies complexes

Avec iptables, cela g√©n√®re **des dizaines de milliers de r√®gles** que le kernel doit parcourir **pour chaque paquet**. C'est comme chercher un nom dans un annuaire t√©l√©phonique en lisant page par page !

## La solution Cilium : eBPF √† la rescousse

Cilium remplace compl√®tement la stack r√©seau traditionnelle par des programmes eBPF ultra-performants.

### Architecture Cilium

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Pod Application                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Programmes eBPF Cilium                    ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚Ä¢ XDP (eXpress Data Path) - Niveau driver          ‚îÇ
‚îÇ  ‚Ä¢ TC (Traffic Control) - Niveau kernel             ‚îÇ
‚îÇ  ‚Ä¢ Socket operations - Niveau socket                ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚úì Hash maps O(1) au lieu de O(n)                   ‚îÇ
‚îÇ  ‚úì D√©cisions en quelques nanosecondes               ‚îÇ
‚îÇ  ‚úì Zero-copy des paquets                            ‚îÇ
‚îÇ  ‚úì Bypass de netfilter quand possible               ‚îÇ
‚îÇ  ‚úì Visibilit√© totale sur tout le trafic             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              BPF Maps (Donn√©es)                     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚Ä¢ Endpoint IDs                                     ‚îÇ
‚îÇ  ‚Ä¢ Policies                                         ‚îÇ
‚îÇ  ‚Ä¢ Connection tracking                              ‚îÇ
‚îÇ  ‚Ä¢ M√©triques                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì
                R√©seau
```

**Avantages de l'approche eBPF :**

- ‚úÖ **Performance exceptionnelle** : O(1) au lieu de O(n)
- ‚úÖ **Latence ultra-faible** : Pas de context switches inutiles
- ‚úÖ **CPU efficace** : 10x moins d'utilisation CPU
- ‚úÖ **Scalabilit√©** : Performance constante quel que soit le nombre de r√®gles
- ‚úÖ **Observabilit√© native** : Visibilit√© sur chaque paquet
- ‚úÖ **S√©curit√© avanc√©e** : Policies au niveau protocole (HTTP, gRPC, etc.)
- ‚úÖ **Flexibilit√©** : Mise √† jour des policies √† chaud

## Comment Cilium utilise eBPF

Cilium n'utilise pas qu'un seul type de programme eBPF, mais une **combinaison strat√©gique** de plusieurs hooks pour optimiser chaque partie de la stack.

### 1. XDP : Le point d'entr√©e le plus rapide

```c
// Programme XDP Cilium (simplifi√© conceptuellement)
SEC("xdp")  
int cilium_xdp(struct xdp_md *ctx)  
{
    void *data = (void *)(long)ctx->data;
    void *data_end = (void *)(long)ctx->data_end;

    // Parser les en-t√™tes r√©seau
    struct ethhdr *eth = data;
    if ((void *)(eth + 1) > data_end)
        return XDP_PASS;

    // Consulter les BPF maps pour les policies
    // D√©cision en quelques nanosecondes :
    // - XDP_PASS : Laisser passer
    // - XDP_DROP : Dropper (attaque DDoS par exemple)
    // - XDP_TX : Renvoyer (load balancing)
    // - XDP_REDIRECT : Rediriger vers un autre pod

    return XDP_PASS;
}
```

**Cas d'usage XDP dans Cilium :**
- Protection DDoS : Dropper les paquets malveillants imm√©diatement
- Load balancing : Rediriger vers le bon backend
- Network Address Translation (NAT)

### 2. TC (Traffic Control) : D√©cisions r√©seau avanc√©es

Les programmes TC de Cilium g√®rent :
- **Network policies** : "Le pod A peut-il parler au pod B ?"
- **Service mesh** : Routage applicatif intelligent
- **Encryption** : Chiffrement transparent avec WireGuard/IPsec

```c
// Programme TC Cilium (conceptuel)
SEC("tc")  
int cilium_ingress(struct __sk_buff *skb)  
{
    // Identifier le pod source et destination
    struct endpoint_key key = {};
    extract_endpoint_from_packet(skb, &key);

    // Consulter les policies dans une BPF map
    struct policy *pol = bpf_map_lookup_elem(&policies, &key);
    if (!pol)
        return TC_ACT_SHOT;  // Dropper : pas de policy

    if (pol->action == DENY)
        return TC_ACT_SHOT;  // Dropper : policy interdit

    // Appliquer l'encryption si n√©cessaire
    if (pol->encrypt)
        apply_wireguard_encryption(skb);

    // Enregistrer les m√©triques
    update_metrics(&key, skb->len);

    return TC_ACT_OK;  // Autoriser
}
```

### 3. Socket operations : Optimisation des connexions TCP

Cilium utilise des programmes eBPF attach√©s aux **sockets** pour :
- Acc√©l√©rer les connexions entre pods sur le m√™me n≈ìud (bypass du r√©seau)
- Impl√©menter le service mesh sans sidecar proxy

```c
// Socket connect hook (conceptuel)
SEC("sockops")  
int cilium_sockops(struct bpf_sock_ops *skops)  
{
    // Si les deux pods sont sur le m√™me n≈ìud,
    // √©tablir une connexion directe (sockmap)
    // au lieu de passer par le r√©seau

    if (same_node(skops)) {
        // Redirect direct via sockmap
        bpf_sock_map_update(skops, &sockmap, &key, BPF_ANY);
    }

    return 0;
}
```

**R√©sultat** : Latence divis√©e par 10 pour les communications intra-n≈ìud !

### 4. Tracepoints et kprobes : Observabilit√©

Cilium utilise des tracepoints et kprobes pour capturer :
- M√©triques r√©seau (bande passante, latence, erreurs)
- Traces de connexions
- √âv√©nements de s√©curit√©

Ces donn√©es alimentent **Hubble**, l'outil d'observabilit√© de Cilium.

## Fonctionnalit√©s principales de Cilium

### 1. Networking haute performance

**Kubernetes Services** (Load balancing) :
- Remplacement de kube-proxy
- Load balancing en eBPF (XDP/TC)
- Performances 10x meilleures
- Support de IPVS, Maglev, etc.

**Exemple** :
```yaml
# Service Kubernetes standard
apiVersion: v1  
kind: Service  
metadata:  
  name: my-service
spec:
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 8080
```

Avec Cilium, ce service est impl√©ment√© par des programmes eBPF qui font du load balancing en O(1) au lieu de O(n) avec iptables.

### 2. Network Policies avanc√©es

Cilium √©tend les Network Policies Kubernetes avec des capacit√©s **Layer 7** (couche application).

**Exemple traditionnel (Layer 3/4)** :
```yaml
# Autoriser uniquement le port 80
apiVersion: networking.k8s.io/v1  
kind: NetworkPolicy  
metadata:  
  name: allow-http
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 80
```

**Exemple Cilium (Layer 7 - HTTP)** :
```yaml
# Autoriser uniquement GET /api/*
apiVersion: cilium.io/v2  
kind: CiliumNetworkPolicy  
metadata:  
  name: allow-api-only
spec:
  endpointSelector:
    matchLabels:
      app: backend
  ingress:
  - fromEndpoints:
    - matchLabels:
        app: frontend
    toPorts:
    - ports:
      - port: "80"
        protocol: TCP
      rules:
        http:
        - method: "GET"
          path: "/api/.*"
```

**Ce que √ßa signifie** :
- Cilium parse les requ√™tes HTTP en eBPF
- Autorise uniquement les GET sur /api/*
- Bloque tout le reste (POST, PUT, autres paths)
- Le tout en **quelques microsecondes** dans le kernel

### 3. Chiffrement transparent

Cilium peut chiffrer automatiquement **tout** le trafic entre les pods, sans modifier votre application.

```yaml
# Activer l'encryption avec WireGuard
apiVersion: v1  
kind: ConfigMap  
metadata:  
  name: cilium-config
data:
  enable-wireguard: "true"
```

**R√©sultat** : Toutes les communications inter-pods sont chiffr√©es avec WireGuard, directement en eBPF. Performance : overhead de seulement 5-10% !

### 4. Service Mesh sans sidecar

Les solutions de service mesh traditionnelles (Istio, Linkerd) ajoutent un **proxy sidecar** √† chaque pod :

```
Pod A ‚Üí Envoy Proxy ‚Üí Network ‚Üí Envoy Proxy ‚Üí Pod B
        (overhead)                (overhead)
```

Avec Cilium Service Mesh, tout est fait en **eBPF dans le kernel** :

```
Pod A ‚Üí eBPF ‚Üí Network ‚Üí eBPF ‚Üí Pod B
       (natif)          (natif)
```

**Avantages** :
- ‚úÖ Pas de sidecar = moins de ressources
- ‚úÖ Latence r√©duite de 50%
- ‚úÖ Moins de complexit√© op√©rationnelle

### 5. Observabilit√© avec Hubble

**Hubble** est l'outil d'observabilit√© de Cilium, bas√© sur les donn√©es collect√©es par eBPF.

**Ce que vous pouvez voir** :
- Map des connexions r√©seau en temps r√©el
- Latence de chaque connexion
- Trafic HTTP/gRPC avec m√©thodes et codes de retour
- √âv√©nements de s√©curit√© (connexions bloqu√©es)
- Anomalies r√©seau

**Interface CLI** :
```bash
# Voir toutes les connexions en temps r√©el
hubble observe

# Filtrer par pod
hubble observe --pod my-app

# Voir les connexions HTTP
hubble observe --protocol http

# Identifier les erreurs 5xx
hubble observe --verdict DROPPED
```

**Interface graphique (Hubble UI)** :
```
   Frontend
      ‚Üì
   [HTTP GET /api]
      ‚Üì
   Backend-1 ‚Üê Load Balanced ‚Üí Backend-2
      ‚Üì                           ‚Üì
   [SQL Query]              [Cache Check]
      ‚Üì                           ‚Üì
   Database                     Redis
```

Toutes ces informations sont collect√©es **sans instrumentation** de votre code, gr√¢ce √† eBPF !

## Architecture d√©taill√©e de Cilium

### Composants

Cilium se compose de plusieurs √©l√©ments :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Cluster Kubernetes                       ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Master     ‚îÇ  ‚îÇ   Worker 1   ‚îÇ  ‚îÇ  Worker 2   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Cilium      ‚îÇ  ‚îÇ  Cilium      ‚îÇ  ‚îÇ  Cilium     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Operator    ‚îÇ  ‚îÇ  Agent       ‚îÇ  ‚îÇ  Agent      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  (DaemonSet) ‚îÇ  ‚îÇ  (DaemonSet)‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  eBPF progs  ‚îÇ  ‚îÇ  eBPF progs ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  loaded      ‚îÇ  ‚îÇ  loaded     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 1. Cilium Agent

- **R√¥le** : DaemonSet qui tourne sur chaque n≈ìud
- **Responsabilit√©s** :
  - Charger les programmes eBPF dans le kernel
  - G√©rer les BPF maps (endpoints, policies, etc.)
  - Appliquer les network policies
  - Monitorer le trafic

#### 2. Cilium Operator

- **R√¥le** : Contr√¥leur centralis√© (Deployment)
- **Responsabilit√©s** :
  - Synchroniser l'√©tat du cluster
  - G√©rer les ressources Cilium (CiliumNetworkPolicy, etc.)
  - Coordination entre les agents

#### 3. Hubble Relay

- **R√¥le** : Agr√©gateur de m√©triques
- **Responsabilit√©s** :
  - Collecter les observations de tous les agents
  - Fournir une API pour Hubble CLI/UI

### Flux de donn√©es

Voici ce qui se passe quand un pod A veut communiquer avec un pod B :

```
1. Pod A √©met un paquet
   ‚Üì
2. Programme eBPF TC (egress) de Cilium intercepte
   ‚îú‚îÄ Consulte identity map : Qui est Pod A ?
   ‚îú‚îÄ Consulte policy map : A peut-il parler √† B ?
   ‚îú‚îÄ Consulte endpoint map : O√π est Pod B ?
   ‚îî‚îÄ Applique encryption si n√©cessaire
   ‚Üì
3. Paquet envoy√© sur le r√©seau
   ‚Üì
4. Arrive sur le n≈ìud de Pod B
   ‚Üì
5. Programme eBPF XDP/TC (ingress) de Cilium
   ‚îú‚îÄ V√©rifie l'identity de A
   ‚îú‚îÄ V√©rifie les policies de B
   ‚îú‚îÄ D√©crypte si n√©cessaire
   ‚îî‚îÄ Enregistre les m√©triques (Hubble)
   ‚Üì
6. Paquet d√©livr√© √† Pod B
```

Tout cela se passe en **quelques microsecondes** et **sans quitter le kernel** !

## Cas d'usage concrets

### 1. Protection contre les attaques DDoS

Cilium peut dropper des millions de paquets par seconde avec XDP :

```yaml
# N'autoriser que les requ√™tes HTTP GET sur le port 80
# depuis des sources connues (tout le reste est dropp√© par XDP)
apiVersion: cilium.io/v2  
kind: CiliumClusterwideNetworkPolicy  
metadata:  
  name: ddos-protection
spec:
  endpointSelector: {}
  ingress:
  - fromCIDR:
    - "10.0.0.0/8"
    toPorts:
    - ports:
      - port: "80"
        protocol: TCP
      rules:
        http:
        - method: "GET"
```

> **Note :** Pour du rate limiting HTTP, Cilium s'appuie sur Envoy via
> `CiliumEnvoyConfig`. Les network policies seules ne g√®rent pas le rate
> limiting ; elles permettent de filtrer par IP source, protocole et m√©thode HTTP.

### 2. Zero-Trust Security

Impl√©menter une architecture zero-trust o√π chaque pod doit explicitement autoriser les connexions :

```yaml
# Default deny all : s√©lectionner tous les endpoints
# avec des listes de r√®gles vides = aucun trafic autoris√©
apiVersion: cilium.io/v2  
kind: CiliumNetworkPolicy  
metadata:  
  name: default-deny
spec:
  endpointSelector: {}
  ingress: []
  egress: []
---
# Autoriser uniquement les chemins n√©cessaires
apiVersion: cilium.io/v2  
kind: CiliumNetworkPolicy  
metadata:  
  name: allow-backend-to-db
spec:
  endpointSelector:
    matchLabels:
      app: backend
  egress:
  - toEndpoints:
    - matchLabels:
        app: postgres
    toPorts:
    - ports:
      - port: "5432"
        protocol: TCP
```

### 3. Multi-cluster avec Cluster Mesh

Cilium peut connecter plusieurs clusters Kubernetes de fa√ßon transparente :

```
Cluster A (AWS)          Cluster B (GCP)
    ‚îÇ                        ‚îÇ
    ‚îú‚îÄ Pod Frontend ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ> Service Backend
    ‚îÇ                        ‚îÇ   (load balanced)
    ‚îÇ                        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ Cilium Cluster Mesh
           (encrypted tunnel)
```

Les pods d'un cluster peuvent parler aux services d'un autre cluster comme s'ils √©taient locaux !

### 4. Debugging r√©seau avanc√©

Avec Hubble, vous pouvez diagnostiquer rapidement les probl√®mes :

```bash
# Pourquoi mon frontend ne peut pas joindre le backend ?
hubble observe --pod frontend --verdict DROPPED

# Exemple de sortie :
# DROPPED: frontend:1234 -> backend:80
# Reason: Policy denied (no matching rule)
```

Vous savez imm√©diatement qu'il manque une network policy !

## Pourquoi Cilium est important pour DevOps/SRE

### 1. Performance = √âconomies

R√©duire l'utilisation CPU de 50% sur votre infrastructure Kubernetes = r√©duction des co√ªts cloud significative.

**Exemple r√©el** :
- Cluster avec 100 n≈ìuds
- √âconomie : 30% de CPU
- Co√ªt mensuel : ~10 000 $ ‚Üí 7 000 $
- **√âconomie annuelle : 36 000 $**

### 2. Observabilit√© sans instrumentation

Pas besoin de modifier vos applications pour avoir de la visibilit√©. Cilium capture tout au niveau r√©seau.

**Avantages** :
- Fonctionne avec n'importe quel langage/framework
- Pas de biblioth√®ques √† int√©grer
- Pas de surco√ªt applicatif

### 3. S√©curit√© renforc√©e

Network policies Layer 7 = s√©curit√© bien plus fine qu'avec iptables.

**Exemple** :
- Bloquer les requ√™tes SQL injection au niveau r√©seau
- Autoriser uniquement certaines m√©thodes HTTP
- Rate limiting par endpoint

### 4. Simplification op√©rationnelle

Moins de composants √† g√©rer :
- ‚ùå kube-proxy ‚Üí Cilium
- ‚ùå Calico/Flannel ‚Üí Cilium
- ‚ùå Service mesh sidecar ‚Üí Cilium
- ‚ùå Network monitoring tools ‚Üí Hubble

**R√©sultat** : Une seule solution pour tout le networking.

## Adoption et √©cosyst√®me

### Qui utilise Cilium ?

Cilium est utilis√© par de nombreuses grandes entreprises :

- **Google GKE** : Cilium est l'option de dataplane recommand√©e
- **AWS EKS** : Support natif de Cilium
- **Microsoft Azure AKS** : Support via Azure CNI powered by Cilium
- **DigitalOcean** : DOKS utilise Cilium
- **Adobe**, **Capital One**, **GitLab**, **Sky**, **Datadog**, etc.

### Int√©grations

Cilium s'int√®gre avec l'√©cosyst√®me cloud-native :

- **Kubernetes** : CNI plugin
- **Prometheus** : M√©triques natives
- **Grafana** : Dashboards pr√©-configur√©s
- **Jaeger** : Tracing distribu√©
- **OpenTelemetry** : Instrumentation
- **Falco** : S√©curit√© runtime

### Certifications

- **CNCF Graduated** : Plus haut niveau de maturit√©
- **Certified Kubernetes Conformance** : Compatible avec toutes les distributions K8s

## Cilium vs autres solutions

### Comparaison avec les CNI traditionnels

| Aspect | Calico/Flannel | Cilium |
|--------|---------------|--------|
| **Technologie** | iptables/VXLAN | eBPF |
| **Performance** | O(n) | O(1) |
| **CPU overhead** | √âlev√© (1-2 cores) | Faible (0.1-0.3 cores) |
| **Latence** | Moyenne | Tr√®s faible |
| **Network policies** | Layer 3/4 | Layer 3-7 |
| **Observabilit√©** | Basique | Avanc√©e (Hubble) |
| **Service mesh** | Non | Oui (sans sidecar) |
| **Encryption** | Oui (overhead √©lev√©) | Oui (WireGuard, faible overhead) |

### Comparaison avec les Service Mesh

| Aspect | Istio (Envoy) | Cilium Service Mesh |
|--------|--------------|---------------------|
| **Architecture** | Sidecar proxy | eBPF natif |
| **Overhead m√©moire** | ~50-100 MB/pod | 0 (pas de sidecar) |
| **Overhead CPU** | ~200-500 millicores/pod | N√©gligeable |
| **Latence** | +5-10 ms | +0.1-0.5 ms |
| **Complexit√©** | √âlev√©e | Faible |
| **Layer 7** | Oui | Oui |
| **mTLS** | Oui | Oui |

## Concepts avanc√©s

### 1. Identity-based Security

Cilium utilise un syst√®me d'**identit√©s** plut√¥t que des IP :

```
Pod Frontend ‚Üí Identity 1234  
Pod Backend  ‚Üí Identity 5678  
```

Les policies se basent sur ces identit√©s, pas sur les IPs (qui changent constamment dans K8s).

**Avantage** : Les policies restent valides m√™me quand les pods sont recr√©√©s avec de nouvelles IPs.

### 2. Endpoint Management

Chaque pod est un **endpoint** Cilium avec :
- Une identit√© unique
- Des labels Kubernetes
- Des policies associ√©es
- Des statistiques de trafic

Ces informations sont stock√©es dans des BPF maps pour un acc√®s ultra-rapide.

### 3. Connection Tracking

Cilium maintient un **√©tat des connexions** en eBPF :

```c
struct connection_entry {
    __u32 src_identity;
    __u32 dst_identity;
    __u16 src_port;
    __u16 dst_port;
    __u8 protocol;
    __u64 bytes_sent;
    __u64 packets_sent;
    __u64 timestamp;
};
```

Cela permet :
- De faire du NAT stateful
- De tracker les connexions pour Hubble
- D'appliquer des rate limits

### 4. BGP et routage avanc√©

Cilium peut annoncer les services Kubernetes via BGP pour une int√©gration avec le r√©seau physique.

```yaml
apiVersion: cilium.io/v2alpha1  
kind: CiliumBGPPeeringPolicy  
metadata:  
  name: bgp-policy
spec:
  nodeSelector:
    matchLabels:
      bgp: "true"
  virtualRouters:
  - localASN: 65000
    neighbors:
    - peerAddress: "192.168.1.1"
      peerASN: 65001
```

Utile pour exposer des services sans LoadBalancer externe.

## Limitations et consid√©rations

### Pr√©requis kernel

Cilium n√©cessite un kernel relativement r√©cent :
- **Minimum** : Linux 5.10 (ou 4.18 sur RHEL 8.10)
- **Recommand√©** : Linux 5.15+
- **Optimal** : Linux 6.1+

### Courbe d'apprentissage

Bien que Cilium simplifie beaucoup de choses, la courbe d'apprentissage initiale peut √™tre raide :
- Comprendre eBPF (cette formation aide !)
- Concepts Kubernetes avanc√©s
- Debugging r√©seau

### Migration depuis une solution existante

Migrer de kube-proxy/Calico vers Cilium n√©cessite :
- Planification (downtime potentiel)
- Tests approfondis
- Formation des √©quipes

### Overhead m√©moire kernel

Les BPF maps consomment de la m√©moire kernel. Pour de tr√®s gros clusters (10 000+ pods), il faut surveiller :
- `RLIMIT_MEMLOCK`
- Sizing des maps
- Fragmentation m√©moire

## Ressources pour d√©marrer avec Cilium

### Documentation officielle

- **Site officiel** : https://cilium.io
- **Documentation** : https://docs.cilium.io
- **GitHub** : https://github.com/cilium/cilium
- **Blog** : https://cilium.io/blog

### Tutoriels interactifs

- **Cilium Getting Started** : https://docs.cilium.io/en/stable/gettingstarted/
- **Cilium Labs** : Environnements interactifs pour apprendre
- **KubeCon talks** : Nombreuses pr√©sentations sur YouTube

### Installation locale (pour tester)

```bash
# Cr√©er un cluster local avec kind
kind create cluster --config=kind-config.yaml

# Installer Cilium
cilium install

# V√©rifier le statut
cilium status

# Activer Hubble
cilium hubble enable

# Tester
kubectl run test-pod --image=nginx  
hubble observe  
```

### Certification

Cilium propose une certification officielle :
- **Cilium Certified Associate (CCA)** : D√©montre la ma√Ætrise des bases

### Communaut√©

- **Cilium Slack** : https://cilium.io/slack
- **Forum** : https://github.com/cilium/cilium/discussions
- **Twitter** : @ciliumproject

## Conclusion

Cilium repr√©sente l'**√©tat de l'art** de l'utilisation d'eBPF en production. C'est un exemple parfait de comment eBPF peut transformer compl√®tement une infrastructure :

**Ce que Cilium d√©montre :**

- ‚úÖ **eBPF est production-ready** : Des milliers d'entreprises l'utilisent
- ‚úÖ **Performance exceptionnelle** : 10x mieux que les solutions traditionnelles
- ‚úÖ **S√©curit√© avanc√©e** : Network policies Layer 7 sans compromis
- ‚úÖ **Observabilit√© native** : Visibilit√© totale sans instrumentation
- ‚úÖ **Simplicit√© op√©rationnelle** : Une solution pour tout le networking

**Pourquoi c'est important pour vous :**

Si vous travaillez dans l'infrastructure moderne (Kubernetes, cloud-native, DevOps), Cilium est une technologie √† **absolument conna√Ætre**. C'est rapidement devenu un standard de facto pour le networking Kubernetes haute performance.

**Pour aller plus loin :**

- Section 21.6.2 : Pixie (un autre outil eBPF incroyable pour l'observabilit√©)
- Section 21.6.3 : Ressources pour approfondir eBPF et son √©cosyst√®me

---

**Fun facts** :
- Le logo de Cilium repr√©sente une abeille (bee) car eBPF = extended Berkeley Packet Filter, mais aussi "bee" = abeille üêù
- Cilium a √©t√© cr√©√© √† l'origine pour remplacer iptables dans les data centers de Cisco
- Le code source de Cilium contient plus de 500 000 lignes de code (dont ~50% de Go et ~30% de C/eBPF)

---

*Note : Cette section donne une vue d'ensemble de Cilium. Pour une utilisation en production, consultez la documentation officielle et suivez les best practices de votre organisation.*

‚è≠Ô∏è [Pixie](/21-introduction-ebpf/06.2-pixie.md)
