ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 27.7 Vectorisation et SIMD

## Introduction

**SIMD** (Single Instruction Multiple Data) est une technique qui permet au processeur d'effectuer **la mÃªme opÃ©ration sur plusieurs donnÃ©es en parallÃ¨le** avec une seule instruction. C'est l'un des moyens les plus efficaces d'accÃ©lÃ©rer du code de calcul intensif.

La **vectorisation** est le processus de transformation du code pour utiliser ces instructions SIMD.

### Pourquoi SIMD est-il important ?

Sur un processeur moderne, les instructions SIMD peuvent traiter :
- **4 entiers de 32 bits** simultanÃ©ment (128 bits avec SSE)
- **8 entiers de 32 bits** simultanÃ©ment (256 bits avec AVX)
- **16 entiers de 32 bits** simultanÃ©ment (512 bits avec AVX-512)

**RÃ©sultat :** Un gain de performance potentiel de **4x, 8x ou mÃªme 16x** sur du code bien vectorisÃ© !

### Analogie : La caisse du supermarchÃ©

**Sans SIMD (scalaire) :**
```
Caissier unique qui scanne les articles un par un
â”œâ”€â”€ Article 1 : bip
â”œâ”€â”€ Article 2 : bip
â”œâ”€â”€ Article 3 : bip
â””â”€â”€ Article 4 : bip
Temps total : 4 secondes
```

**Avec SIMD (vectoriel) :**
```
4 caissiers en parallÃ¨le qui scannent simultanÃ©ment
â”œâ”€â”€ Caissier 1 : Article 1 bip â”
â”œâ”€â”€ Caissier 2 : Article 2 bip â”œâ”€ En mÃªme temps !
â”œâ”€â”€ Caissier 3 : Article 3 bip â”‚
â””â”€â”€ Caissier 4 : Article 4 bip â”˜
Temps total : 1 seconde (4x plus rapide)
```

MÃªme principe : une instruction SIMD traite 4 (ou 8, 16) donnÃ©es en mÃªme temps qu'une instruction normale en traite une.

---

## Architectures SIMD

### x86-64 (Intel/AMD)

| Extension | AnnÃ©e | Taille registres | CapacitÃ© |
|-----------|-------|------------------|----------|
| **SSE** | 1999 | 128 bits | 4 floats ou 2 doubles |
| **SSE2** | 2001 | 128 bits | 4 int32, 8 int16, 16 int8 |
| **SSE3** | 2004 | 128 bits | AmÃ©liorations |
| **SSSE3** | 2006 | 128 bits | Plus d'instructions |
| **SSE4** | 2006-2008 | 128 bits | Encore plus |
| **AVX** | 2011 | 256 bits | 8 floats, 4 doubles |
| **AVX2** | 2013 | 256 bits | OpÃ©rations entiÃ¨res |
| **AVX-512** | 2016+ | 512 bits | 16 floats, 8 doubles |

### ARM (smartphones, Raspberry Pi, serveurs ARM)

| Extension | Taille registres | CapacitÃ© |
|-----------|------------------|----------|
| **NEON** | 128 bits | 4 floats, 4 int32 |
| **SVE** | Variable (128-2048 bits) | Vectorisation adaptative |

### VÃ©rifier les capacitÃ©s de votre CPU

```bash
# Sur Linux x86-64
lscpu | grep Flags

# Ou plus dÃ©taillÃ©
cat /proc/cpuinfo | grep flags | head -1

# RÃ©sultat typique :
# flags : ... sse sse2 sse3 ssse3 sse4_1 sse4_2 avx avx2 ...
```

**InterprÃ©tation :**
- `sse` Ã  `sse4_2` : Votre CPU supporte SSE (quasi-universel aujourd'hui)
- `avx` : Support AVX (processeurs depuis 2011)
- `avx2` : Support AVX2 (processeurs depuis 2013)
- `avx512f` : Support AVX-512 (processeurs haut de gamme rÃ©cents)

---

## Comment fonctionne SIMD ?

### Code scalaire (sans SIMD)

```c
// Additionner deux tableaux
float a[4] = {1.0, 2.0, 3.0, 4.0};
float b[4] = {5.0, 6.0, 7.0, 8.0};
float c[4];

// Version scalaire : 4 opÃ©rations distinctes
c[0] = a[0] + b[0];  // Instruction 1
c[1] = a[1] + b[1];  // Instruction 2
c[2] = a[2] + b[2];  // Instruction 3
c[3] = a[3] + b[3];  // Instruction 4
```

**Nombre d'instructions CPU :** 4

### Code vectoriel (avec SIMD)

```c
// Version SIMD conceptuelle
__m128 va = _mm_load_ps(a);      // Charger 4 floats dans un registre SIMD
__m128 vb = _mm_load_ps(b);      // Charger 4 floats dans un registre SIMD
__m128 vc = _mm_add_ps(va, vb);  // UNE SEULE instruction : additionner 4 floats !
_mm_store_ps(c, vc);             // Stocker le rÃ©sultat
```

**Nombre d'instructions CPU :** 1 (plus load/store)

**ReprÃ©sentation visuelle :**

```
Registre SIMD (128 bits) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1.0   â”‚  2.0   â”‚  3.0   â”‚  4.0   â”‚  a
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5.0   â”‚  6.0   â”‚  7.0   â”‚  8.0   â”‚  b
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         ADD (une seule instruction)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6.0   â”‚  8.0   â”‚ 10.0   â”‚ 12.0   â”‚  c
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Vectorisation automatique par le compilateur

### Comment activer la vectorisation automatique

GCC peut automatiquement transformer certaines boucles en code SIMD avec les bons flags :

```bash
# Activer la vectorisation (incluse dans -O3)
gcc -O3 programme.c -o programme

# Ou explicitement
gcc -O2 -ftree-vectorize programme.c -o programme

# Optimiser pour le CPU local (active les instructions disponibles)
gcc -O3 -march=native programme.c -o programme

# Voir les optimisations de vectorisation appliquÃ©es
gcc -O3 -ftree-vectorize -fopt-info-vec-optimized programme.c
```

### Exemple : Addition de tableaux

```c
// add_arrays.c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#define SIZE 10000000  // 10 millions

void add_arrays(float *a, float *b, float *c, int n) {
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    float *a = malloc(SIZE * sizeof(float));
    float *b = malloc(SIZE * sizeof(float));
    float *c = malloc(SIZE * sizeof(float));

    // Initialiser
    for (int i = 0; i < SIZE; i++) {
        a[i] = (float)i;
        b[i] = (float)i * 2.0f;
    }

    clock_t start = clock();
    add_arrays(a, b, c, SIZE);
    clock_t end = clock();

    printf("Temps: %.3f ms\n",
           (double)(end - start) * 1000.0 / CLOCKS_PER_SEC);

    free(a); free(b); free(c);
    return 0;
}
```

**Compilation et test :**

```bash
# Sans vectorisation
gcc -O2 add_arrays.c -o add_O2
./add_O2
# RÃ©sultat : ~45 ms

# Avec vectorisation automatique
gcc -O3 -march=native add_arrays.c -o add_O3
./add_O3
# RÃ©sultat : ~12 ms  â† 3.7x plus rapide !

# VÃ©rifier la vectorisation
gcc -O3 -march=native -fopt-info-vec-optimized add_arrays.c
# Sortie : add_arrays.c:8:5: optimized: loop vectorized using 256 bit vectors
```

**Explication :** Le compilateur a dÃ©tectÃ© que la boucle est vectorisable et a gÃ©nÃ©rÃ© du code AVX (256 bits = 8 floats en parallÃ¨le).

### Voir le code assembleur gÃ©nÃ©rÃ©

```bash
gcc -O3 -march=native -S add_arrays.c -o add_arrays.s
cat add_arrays.s | grep -A 10 "add_arrays:"
```

Vous devriez voir des instructions comme `vaddps` (AVX) ou `addps` (SSE) au lieu de `addss` (scalaire).

---

## Conditions pour la vectorisation automatique

Le compilateur peut vectoriser une boucle si :

### âœ… Conditions favorables

1. **Boucle simple** : Un seul compteur, pas de `break` ou `continue` complexes
2. **Pas de dÃ©pendances de donnÃ©es** : `a[i]` ne dÃ©pend pas de `a[i-1]`
3. **AccÃ¨s mÃ©moire contigus** : `a[i]`, `a[i+1]`, `a[i+2]`... (pas `a[random]`)
4. **Nombre d'itÃ©rations connu** ou facilement calculable
5. **Alignement mÃ©moire** : DonnÃ©es alignÃ©es sur 16/32 octets (optimal)
6. **OpÃ©rations simples** : Addition, multiplication, comparaison, etc.

### âŒ Obstacles Ã  la vectorisation

```c
// âŒ DÃ©pendance de donnÃ©es
for (int i = 1; i < n; i++) {
    a[i] = a[i] + a[i-1];  // a[i] dÃ©pend de a[i-1], pas vectorisable
}

// âŒ AccÃ¨s non contigus
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i * 2];  // Stride = 2 pour b, moins efficace
}

// âŒ Branchements complexes
for (int i = 0; i < n; i++) {
    if (condition_complexe(a[i])) {
        c[i] = calcul1(a[i]);
    } else {
        c[i] = calcul2(a[i]);
    }
}

// âŒ Appels de fonction
for (int i = 0; i < n; i++) {
    c[i] = fonction_externe(a[i]);  // Difficile Ã  vectoriser
}
```

### Comment aider le compilateur

#### a) Utiliser `restrict`

```c
// Indiquer au compilateur que les pointeurs ne se chevauchent pas
void add_arrays(float *restrict a, float *restrict b,
                float *restrict c, int n) {
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}
```

#### b) Aligner les donnÃ©es

```c
// Aligner sur 32 bytes (pour AVX)
float *a = aligned_alloc(32, SIZE * sizeof(float));

// Ou avec GCC
float a[SIZE] __attribute__((aligned(32)));
```

#### c) Annotations pour le compilateur

```c
// Indiquer explicitement qu'une boucle est vectorisable (GCC/Clang)
#pragma GCC ivdep  // Ignore Vector Dependencies
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

// OpenMP SIMD
#pragma omp simd
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}
```

---

## Vectorisation manuelle avec intrinsics

Pour un contrÃ´le total, on peut Ã©crire du code SIMD directement avec les **intrinsics** (fonctions built-in du compilateur).

### Introduction aux intrinsics SSE/AVX

Les intrinsics sont des fonctions C qui correspondent **directement** Ã  des instructions assembleur SIMD.

**Convention de nommage :**
```
_mm_<operation>_<type>

Exemples :
_mm_add_ps   â†’ Addition (add) de floats (ps = packed single precision)
_mm_mul_pd   â†’ Multiplication (mul) de doubles (pd = packed double precision)
_mm_load_si128 â†’ Chargement (load) d'entiers (si128 = signed integers 128 bits)
```

**Types de donnÃ©es :**
```c
__m128   : 4 floats (128 bits)
__m128d  : 2 doubles (128 bits)
__m128i  : Entiers (128 bits)

__m256   : 8 floats (256 bits, AVX)
__m256d  : 4 doubles (256 bits, AVX)
__m256i  : Entiers (256 bits, AVX)
```

### Exemple 1 : Addition de floats (SSE)

```c
// add_sse.c
#include <stdio.h>
#include <xmmintrin.h>  // SSE
#include <time.h>

#define SIZE 10000000

// Version scalaire
void add_scalar(float *a, float *b, float *c, int n) {
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}

// Version SSE (4 floats Ã  la fois)
void add_sse(float *a, float *b, float *c, int n) {
    int i;

    // Traiter 4 Ã©lÃ©ments Ã  la fois
    for (i = 0; i <= n - 4; i += 4) {
        __m128 va = _mm_load_ps(&a[i]);   // Charger 4 floats de a
        __m128 vb = _mm_load_ps(&b[i]);   // Charger 4 floats de b
        __m128 vc = _mm_add_ps(va, vb);   // Additionner
        _mm_store_ps(&c[i], vc);          // Stocker le rÃ©sultat
    }

    // Traiter les Ã©lÃ©ments restants (si n n'est pas multiple de 4)
    for (; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    // Allouer avec alignement pour SSE (16 bytes)
    float *a = aligned_alloc(16, SIZE * sizeof(float));
    float *b = aligned_alloc(16, SIZE * sizeof(float));
    float *c = aligned_alloc(16, SIZE * sizeof(float));

    for (int i = 0; i < SIZE; i++) {
        a[i] = (float)i;
        b[i] = (float)i * 2.0f;
    }

    clock_t start, end;

    // Test scalaire
    start = clock();
    add_scalar(a, b, c, SIZE);
    end = clock();
    printf("Scalaire: %.3f ms\n",
           (double)(end - start) * 1000.0 / CLOCKS_PER_SEC);

    // Test SSE
    start = clock();
    add_sse(a, b, c, SIZE);
    end = clock();
    printf("SSE:      %.3f ms\n",
           (double)(end - start) * 1000.0 / CLOCKS_PER_SEC);

    free(a); free(b); free(c);
    return 0;
}
```

**Compilation :**

```bash
gcc -O2 -msse add_sse.c -o add_sse
./add_sse
```

**RÃ©sultats typiques :**
```
Scalaire: 45.2 ms
SSE:      12.8 ms  â† 3.5x plus rapide
```

### Exemple 2 : Addition de floats (AVX)

```c
// add_avx.c
#include <stdio.h>
#include <immintrin.h>  // AVX
#include <time.h>

#define SIZE 10000000

void add_avx(float *a, float *b, float *c, int n) {
    int i;

    // Traiter 8 Ã©lÃ©ments Ã  la fois avec AVX
    for (i = 0; i <= n - 8; i += 8) {
        __m256 va = _mm256_load_ps(&a[i]);   // Charger 8 floats
        __m256 vb = _mm256_load_ps(&b[i]);
        __m256 vc = _mm256_add_ps(va, vb);   // Additionner 8 floats en une fois
        _mm256_store_ps(&c[i], vc);
    }

    // Reste
    for (; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    float *a = aligned_alloc(32, SIZE * sizeof(float));  // Alignement 32 bytes pour AVX
    float *b = aligned_alloc(32, SIZE * sizeof(float));
    float *c = aligned_alloc(32, SIZE * sizeof(float));

    for (int i = 0; i < SIZE; i++) {
        a[i] = (float)i;
        b[i] = (float)i * 2.0f;
    }

    clock_t start = clock();
    add_avx(a, b, c, SIZE);
    clock_t end = clock();

    printf("AVX: %.3f ms\n",
           (double)(end - start) * 1000.0 / CLOCKS_PER_SEC);

    free(a); free(b); free(c);
    return 0;
}
```

**Compilation :**

```bash
gcc -O2 -mavx add_avx.c -o add_avx
./add_avx
```

**RÃ©sultats :**
```
AVX: 7.2 ms  â† 6.3x plus rapide que le code scalaire !
```

---

## Intrinsics courants

### OpÃ©rations arithmÃ©tiques (SSE)

```c
// Addition
__m128 _mm_add_ps(__m128 a, __m128 b);        // 4 floats
__m128d _mm_add_pd(__m128d a, __m128d b);     // 2 doubles

// Soustraction
__m128 _mm_sub_ps(__m128 a, __m128 b);

// Multiplication
__m128 _mm_mul_ps(__m128 a, __m128 b);

// Division
__m128 _mm_div_ps(__m128 a, __m128 b);

// Racine carrÃ©e
__m128 _mm_sqrt_ps(__m128 a);

// Minimum/Maximum
__m128 _mm_min_ps(__m128 a, __m128 b);
__m128 _mm_max_ps(__m128 a, __m128 b);
```

### OpÃ©rations de chargement/stockage

```c
// Chargement alignÃ© (IMPORTANT : adresse doit Ãªtre alignÃ©e sur 16 bytes)
__m128 _mm_load_ps(const float *p);

// Chargement non alignÃ© (plus lent)
__m128 _mm_loadu_ps(const float *p);

// Stockage alignÃ©
void _mm_store_ps(float *p, __m128 a);

// Stockage non alignÃ©
void _mm_storeu_ps(float *p, __m128 a);

// DÃ©finir des valeurs
__m128 _mm_set_ps(float e3, float e2, float e1, float e0);  // Ordre inversÃ© !
__m128 _mm_set1_ps(float a);  // RÃ©pÃ©ter 'a' dans les 4 positions
__m128 _mm_setzero_ps();      // Mettre Ã  zÃ©ro
```

### OpÃ©rations logiques et comparaisons

```c
// ET logique
__m128 _mm_and_ps(__m128 a, __m128 b);

// OU logique
__m128 _mm_or_ps(__m128 a, __m128 b);

// Comparaisons (retourne un masque)
__m128 _mm_cmpeq_ps(__m128 a, __m128 b);   // a == b
__m128 _mm_cmplt_ps(__m128 a, __m128 b);   // a < b
__m128 _mm_cmpgt_ps(__m128 a, __m128 b);   // a > b
```

---

## Cas d'Ã©tude : Produit scalaire

Le **produit scalaire** (dot product) de deux vecteurs est une opÃ©ration frÃ©quente en calcul scientifique et graphisme.

```
dot(a, b) = a[0]*b[0] + a[1]*b[1] + ... + a[n-1]*b[n-1]
```

### Version scalaire

```c
float dot_product_scalar(float *a, float *b, int n) {
    float sum = 0.0f;
    for (int i = 0; i < n; i++) {
        sum += a[i] * b[i];
    }
    return sum;
}
```

**ComplexitÃ© :** O(n), mais sÃ©quentiel.

### Version SSE

```c
#include <xmmintrin.h>

float dot_product_sse(float *a, float *b, int n) {
    __m128 vsum = _mm_setzero_ps();  // Accumulateur vectoriel (4 floats)
    int i;

    // Traiter 4 Ã©lÃ©ments Ã  la fois
    for (i = 0; i <= n - 4; i += 4) {
        __m128 va = _mm_load_ps(&a[i]);
        __m128 vb = _mm_load_ps(&b[i]);
        __m128 vmul = _mm_mul_ps(va, vb);     // Multiplier 4 paires
        vsum = _mm_add_ps(vsum, vmul);        // Accumuler
    }

    // RÃ©duire le vecteur Ã  un scalaire (horizontal add)
    // vsum contient [s0, s1, s2, s3]
    // On veut sum = s0 + s1 + s2 + s3

    // MÃ©thode 1 : Extraire et additionner
    float result[4];
    _mm_store_ps(result, vsum);
    float sum = result[0] + result[1] + result[2] + result[3];

    // Traiter les Ã©lÃ©ments restants
    for (; i < n; i++) {
        sum += a[i] * b[i];
    }

    return sum;
}
```

**AmÃ©lioration avec SSE3 (horizontal add) :**

```c
#include <pmmintrin.h>  // SSE3

float dot_product_sse3(float *a, float *b, int n) {
    __m128 vsum = _mm_setzero_ps();
    int i;

    for (i = 0; i <= n - 4; i += 4) {
        __m128 va = _mm_load_ps(&a[i]);
        __m128 vb = _mm_load_ps(&b[i]);
        __m128 vmul = _mm_mul_ps(va, vb);
        vsum = _mm_add_ps(vsum, vmul);
    }

    // Horizontal add (SSE3)
    vsum = _mm_hadd_ps(vsum, vsum);  // [s0+s1, s2+s3, s0+s1, s2+s3]
    vsum = _mm_hadd_ps(vsum, vsum);  // [s0+s1+s2+s3, ...]

    float sum;
    _mm_store_ss(&sum, vsum);  // Extraire le premier Ã©lÃ©ment

    // Reste
    for (; i < n; i++) {
        sum += a[i] * b[i];
    }

    return sum;
}
```

### Benchmark

```c
#define SIZE 10000000

int main() {
    float *a = aligned_alloc(16, SIZE * sizeof(float));
    float *b = aligned_alloc(16, SIZE * sizeof(float));

    for (int i = 0; i < SIZE; i++) {
        a[i] = (float)i;
        b[i] = 1.0f;
    }

    clock_t start, end;
    float result;

    // Scalaire
    start = clock();
    result = dot_product_scalar(a, b, SIZE);
    end = clock();
    printf("Scalaire: %.3f ms (rÃ©sultat: %.2f)\n",
           (double)(end - start) * 1000.0 / CLOCKS_PER_SEC, result);

    // SSE
    start = clock();
    result = dot_product_sse(a, b, SIZE);
    end = clock();
    printf("SSE:      %.3f ms (rÃ©sultat: %.2f)\n",
           (double)(end - start) * 1000.0 / CLOCKS_PER_SEC, result);

    free(a); free(b);
    return 0;
}
```

**RÃ©sultats typiques :**
```
Scalaire: 52.3 ms (rÃ©sultat: 49999995000000.00)
SSE:      14.2 ms (rÃ©sultat: 49999995000000.00)  â† 3.7x plus rapide
```

---

## Exemple avancÃ© : Filtre d'image (convolution)

Les filtres d'image (flou, dÃ©tection de contours) utilisent massivement SIMD.

### Flou gaussien simple (moyenne des pixels voisins)

**Version scalaire :**

```c
void blur_scalar(unsigned char *src, unsigned char *dst,
                 int width, int height) {
    for (int y = 1; y < height - 1; y++) {
        for (int x = 1; x < width - 1; x++) {
            int sum = 0;
            // Moyenne 3x3
            for (int dy = -1; dy <= 1; dy++) {
                for (int dx = -1; dx <= 1; dx++) {
                    sum += src[(y + dy) * width + (x + dx)];
                }
            }
            dst[y * width + x] = sum / 9;
        }
    }
}
```

**Version SSE (traiter 16 pixels Ã  la fois) :**

```c
#include <emmintrin.h>  // SSE2

void blur_sse(unsigned char *src, unsigned char *dst,
              int width, int height) {
    for (int y = 1; y < height - 1; y++) {
        int x;
        for (x = 1; x <= width - 17; x += 16) {  // 16 pixels Ã  la fois
            __m128i sum = _mm_setzero_si128();

            // Accumuler les 9 positions
            for (int dy = -1; dy <= 1; dy++) {
                for (int dx = -1; dx <= 1; dx++) {
                    // Charger 16 bytes (16 pixels)
                    __m128i pixels = _mm_loadu_si128(
                        (__m128i*)&src[(y + dy) * width + (x + dx)]
                    );

                    // Convertir en 16-bit pour Ã©viter l'overflow
                    __m128i pixels_lo = _mm_unpacklo_epi8(pixels, _mm_setzero_si128());
                    __m128i pixels_hi = _mm_unpackhi_epi8(pixels, _mm_setzero_si128());

                    // Accumuler
                    sum = _mm_add_epi16(sum, pixels_lo);
                    sum = _mm_add_epi16(sum, pixels_hi);
                }
            }

            // Diviser par 9 (approximation : shift de 3 bits = division par 8)
            sum = _mm_srli_epi16(sum, 3);  // >> 3

            // Reconvertir en 8-bit et stocker
            sum = _mm_packus_epi16(sum, _mm_setzero_si128());
            _mm_storel_epi64((__m128i*)&dst[y * width + x], sum);
        }

        // Traiter les pixels restants
        for (; x < width - 1; x++) {
            // Version scalaire
        }
    }
}
```

**Gain typique :** 8-12x plus rapide pour une image 1920x1080.

---

## PiÃ¨ges et limitations

### 1. Alignement mÃ©moire

**ProblÃ¨me :** Les instructions `_mm_load_ps` nÃ©cessitent un alignement sur 16 bytes (SSE) ou 32 bytes (AVX).

```c
float a[4] = {1.0, 2.0, 3.0, 4.0};  // Peut ne PAS Ãªtre alignÃ© !
__m128 va = _mm_load_ps(a);         // PEUT PLANTER (segfault) si pas alignÃ©
```

**Solutions :**

```c
// 1. Utiliser aligned_alloc
float *a = aligned_alloc(16, 4 * sizeof(float));

// 2. Attribut de compilation
float a[4] __attribute__((aligned(16))) = {1.0, 2.0, 3.0, 4.0};

// 3. Utiliser loadu (non alignÃ©, mais plus lent)
__m128 va = _mm_loadu_ps(a);  // Fonctionne toujours, lÃ©gÃ¨rement plus lent
```

### 2. Boucles non divisibles

Si le nombre d'Ã©lÃ©ments n'est pas un multiple de la taille du vecteur, il faut traiter les Ã©lÃ©ments restants :

```c
// Mauvais : oublie les derniers Ã©lÃ©ments
for (int i = 0; i < n; i += 4) {  // Si n=10, les Ã©lÃ©ments 8 et 9 sont ignorÃ©s !
    // ...
}

// Correct
int i;
for (i = 0; i <= n - 4; i += 4) {
    // Traitement vectoriel
}
for (; i < n; i++) {
    // Traitement scalaire des Ã©lÃ©ments restants
}
```

### 3. PortabilitÃ©

Le code SIMD est **spÃ©cifique Ã  l'architecture** :
- Code SSE/AVX â†’ Seulement x86-64
- Code NEON â†’ Seulement ARM

**Solutions :**

```c
// Compilation conditionnelle
#ifdef __SSE__
    // Code SSE
#else
    // Code scalaire de fallback
#endif

// Ou dÃ©tection Ã  l'exÃ©cution
if (cpu_supports_avx2()) {
    add_avx2(a, b, c, n);
} else if (cpu_supports_sse()) {
    add_sse(a, b, c, n);
} else {
    add_scalar(a, b, c, n);
}
```

### 4. ComplexitÃ© du code

Le code SIMD est **beaucoup plus verbeux et difficile Ã  maintenir** que le code scalaire.

**Recommandation :** Utilisez la vectorisation automatique (avec `-O3 -march=native`) en prioritÃ©. N'Ã©crivez du code SIMD manuel que si :
- Le gain est significatif (>2x)
- C'est un goulot d'Ã©tranglement prouvÃ© par profiling
- La vectorisation automatique ne fonctionne pas

---

## VÃ©rifier la vectorisation

### Avec les flags de compilation

```bash
# Voir les boucles vectorisÃ©es
gcc -O3 -ftree-vectorize -fopt-info-vec-optimized test.c

# Voir pourquoi certaines boucles ne sont PAS vectorisÃ©es
gcc -O3 -ftree-vectorize -fopt-info-vec-missed test.c

# Tout voir
gcc -O3 -ftree-vectorize -fopt-info-vec-all test.c
```

**Exemple de sortie :**

```
test.c:10:5: optimized: loop vectorized using 256 bit vectors
test.c:25:5: missed: couldn't vectorize loop
test.c:25:5: missed: not vectorized: data ref analysis failed
```

### Avec objdump (code assembleur)

```bash
gcc -O3 -march=native test.c -o test
objdump -d test | grep -E "addps|vaddps|addpd|vaddpd"
```

**Instructions Ã  chercher :**
- `addps`, `mulps`, `subps` â†’ SSE
- `vaddps`, `vmulps` â†’ AVX
- `vaddpd`, `vmulpd` â†’ AVX (doubles)

### Avec perf

```bash
# Compter les instructions SIMD exÃ©cutÃ©es
perf stat -e fp_arith_inst_retired.128b_packed_double \
          -e fp_arith_inst_retired.256b_packed_double ./test

# Intel seulement, Ã©vÃ©nements spÃ©cifiques au CPU
```

---

## BibliothÃ¨ques utilisant SIMD

PlutÃ´t que d'Ã©crire du code SIMD manuel, vous pouvez utiliser des bibliothÃ¨ques optimisÃ©es :

### BibliothÃ¨ques populaires

| BibliothÃ¨que | Domaine | Notes |
|--------------|---------|-------|
| **Intel MKL** | AlgÃ¨bre linÃ©aire | PropriÃ©taire, trÃ¨s optimisÃ© |
| **OpenBLAS** | AlgÃ¨bre linÃ©aire | Open-source, excellent |
| **FFTW** | TransformÃ©es de Fourier | Optimisations SIMD |
| **Eigen** | AlgÃ¨bre linÃ©aire (C++) | Header-only, excellent |
| **libsimdpp** | Abstraction SIMD | Multi-plateforme |
| **Highway** | Abstraction SIMD | Moderne, de Google |
| **libjpeg-turbo** | JPEG | Utilise SIMD en interne |
| **libvpx** | VidÃ©o VP8/VP9 | OptimisÃ© SIMD |

**Exemple avec OpenBLAS :**

```c
#include <cblas.h>

// Produit matrice-vecteur (ultra-optimisÃ© avec SIMD)
cblas_sgemv(CblasRowMajor, CblasNoTrans,
            m, n, 1.0, A, n, x, 1, 0.0, y, 1);
```

OpenBLAS utilisera automatiquement AVX2 ou AVX-512 si disponible.

---

## Bonnes pratiques

### âœ… Ã€ faire

1. **Commencer par la vectorisation automatique** (`-O3 -march=native`)
2. **Profiler avant d'optimiser** (identifier les vrais goulots)
3. **Aligner les donnÃ©es** (16 ou 32 bytes)
4. **Utiliser `restrict`** pour Ã©viter l'aliasing
5. **Tester sur plusieurs CPUs** (SSE, AVX, AVX2)
6. **Fournir un fallback scalaire** pour la portabilitÃ©
7. **Utiliser des bibliothÃ¨ques existantes** quand possible
8. **Documenter le code SIMD** (c'est complexe !)

### âŒ Ã€ Ã©viter

1. **Vectoriser du code non critique** (perte de temps)
2. **Oublier les Ã©lÃ©ments restants** (bugs subtils)
3. **Ignorer l'alignement** (segfault ou performance dÃ©gradÃ©e)
4. **Vectoriser des boucles avec dÃ©pendances** (rÃ©sultats incorrects)
5. **Ã‰crire du SIMD sans benchmark** (gain non vÃ©rifiÃ©)
6. **Sacrifier la lisibilitÃ©** sans gain significatif

---

## Checklist : Ai-je besoin de SIMD ?

Avant de vectoriser manuellement :

- âœ… Ai-je **profileÃ©** et identifiÃ© un goulot CPU ?
- âœ… Ce goulot est-il dans **une boucle de calcul** ?
- âœ… Ai-je essayÃ© `-O3 -march=native` ? Quel gain ?
- âœ… La vectorisation automatique Ã©choue-t-elle ?
- âœ… Le gain potentiel justifie-t-il la complexitÃ© ?
- âœ… Puis-je maintenir le code SIMD dans le temps ?

Si vous rÃ©pondez **OUI** Ã  toutes ces questions, alors SIMD manuel peut valoir le coup.

Sinon, restez sur la vectorisation automatique !

---

## Ressources pour aller plus loin

### Documentation officielle

- **Intel Intrinsics Guide** : https://www.intel.com/content/www/us/en/docs/intrinsics-guide/
  (RÃ©fÃ©rence complÃ¨te de toutes les intrinsics, avec exemples)
- **GCC Vector Extensions** : https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html
- **ARM NEON** : https://developer.arm.com/architectures/instruction-sets/simd-isas/neon

### Livres et cours

- **"Computer Organization and Design"** â€” Patterson & Hennessy (chapitre sur SIMD)
- **"Optimizing software in C++"** â€” Agner Fog (gratuit, couvre SIMD en dÃ©tail)
- **Intel Optimization Manual** â€” Guide officiel d'optimisation

### Outils

```bash
# Compiler Explorer (voir le code assembleur en ligne)
https://godbolt.org/

# Intel SDE (simuler AVX-512 sans le hardware)
https://www.intel.com/content/www/us/en/developer/articles/tool/software-development-emulator.html
```

---

## RÃ©sumÃ©

SIMD est un outil puissant pour accÃ©lÃ©rer le code de calcul :

1. âœ… **Gain potentiel Ã©norme** : 4x Ã  16x sur du code bien vectorisÃ©
2. âœ… **Vectorisation automatique** : GCC fait un excellent travail avec `-O3 -march=native`
3. âœ… **Intrinsics manuels** : Pour contrÃ´le total, mais complexe
4. âœ… **Cas d'usage idÃ©aux** : Calculs sur tableaux, traitement d'images, algÃ¨bre linÃ©aire
5. âœ… **PiÃ¨ges** : Alignement, portabilitÃ©, complexitÃ©

**HiÃ©rarchie des optimisations (rappel) :**

```
1. Bon algorithme (O(n) vs O(nÂ²))          â†’ 100-1000x
2. Structure de donnÃ©es adaptÃ©e            â†’ 10-100x
3. SIMD vectorisation                      â†’ 4-16x        â† Nous sommes ici
4. Cache awareness                         â†’ 2-10x
5. Flags de compilation                    â†’ 1.5-3x
```

**Citation :**

> "The best SIMD code is the code you didn't have to write yourself" â€” Anonyme

**RÃ¨gle d'or :** Laissez le compilateur vectoriser automatiquement (`-O3 -march=native`). N'Ã©crivez du SIMD manuel qu'en dernier recours, aprÃ¨s avoir profileÃ© et confirmÃ© le gain !

---

*Prochaine section : 27.8 Link-Time Optimization (LTO)*

â­ï¸ [Link-Time Optimization (LTO)](/27-optimisation-performance/08-lto.md)
