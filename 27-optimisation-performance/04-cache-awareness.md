ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 27.4 Cache awareness

## Introduction

Le **cache awareness** (conscience du cache) est l'art d'Ã©crire du code qui exploite efficacement la **hiÃ©rarchie mÃ©moire** du processeur. C'est l'un des facteurs les plus importants pour les performances modernes, et pourtant souvent nÃ©gligÃ© par les dÃ©butants.

### Pourquoi le cache est-il si important ?

**Fait surprenant :** Sur un processeur moderne, accÃ©der Ã  la RAM est **200 Ã  300 fois plus lent** qu'accÃ©der au cache L1 !

```
Latences typiques (ordres de grandeur) :
- Registre CPU      : 0 cycles    (instantanÃ©)
- Cache L1          : 4 cycles    (~1 nanoseconde)
- Cache L2          : 12 cycles   (~3 nanosecondes)
- Cache L3          : 40 cycles   (~10 nanosecondes)
- RAM               : 200 cycles  (~60 nanosecondes)
- SSD               : 50,000 ns   (50 microsecondes)
- Disque dur        : 5,000,000 ns (5 millisecondes)
```

**ConsÃ©quence :** Un programme qui fait beaucoup de cache misses (dÃ©fauts de cache) peut Ãªtre **10 Ã  100 fois plus lent** qu'un programme cache-friendly !

### Analogie : La bibliothÃ¨que

Imaginez que vous travaillez sur un projet :

- **Registres CPU** = Votre bureau (livres directement sous vos yeux)
- **Cache L1** = Ã‰tagÃ¨re Ã  portÃ©e de main (2 secondes pour prendre un livre)
- **Cache L2** = BibliothÃ¨que de votre bureau (10 secondes)
- **Cache L3** = BibliothÃ¨que de l'Ã©tage (30 secondes)
- **RAM** = BibliothÃ¨que municipale (1 heure aller-retour)
- **Disque** = BibliothÃ¨que nationale (1 journÃ©e)

Si vous devez aller Ã  la bibliothÃ¨que nationale pour chaque page que vous lisez, votre travail sera **trÃ¨s lent**. Si vous ramenez tous les livres nÃ©cessaires sur votre bureau une seule fois, vous serez **beaucoup plus efficace**.

C'est exactement le principe du cache !

---

## La hiÃ©rarchie mÃ©moire

### Architecture typique (processeur moderne)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CPU Core                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Registres (64 bits)        â”‚   â”‚  â† Plus rapide, plus petit
â”‚  â”‚        ~100 bytes                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â†“â†‘                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Cache L1 (donnÃ©es + instr)    â”‚   â”‚
â”‚  â”‚     32 KB / 32 KB                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â†“â†‘                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Cache L2 (unifiÃ©)         â”‚   â”‚
â”‚  â”‚           256 KB - 512 KB         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Cache L3 (partagÃ© entre cores)        â”‚
â”‚         8 MB - 64 MB                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             RAM (DRAM)                   â”‚
â”‚         8 GB - 128 GB                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Stockage (SSD/HDD)             â”‚
â”‚          500 GB - 4 TB                   â”‚  â† Plus lent, plus grand
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Principe fondamental :** Plus c'est rapide, plus c'est petit et cher.

### VÃ©rifier la configuration de votre CPU

```bash
# Voir les informations de cache
lscpu | grep cache

# Exemple de sortie :
# L1d cache:           32K
# L1i cache:           32K
# L2 cache:            256K
# L3 cache:            8192K

# Ou avec getconf
getconf -a | grep CACHE
```

---

## Comment fonctionne le cache ?

### Lignes de cache (Cache Lines)

Le cache ne stocke pas les donnÃ©es **octet par octet**, mais par **blocs** appelÃ©s **lignes de cache** (cache lines).

**Taille typique d'une ligne de cache :** 64 bytes (octets)

**Principe :** Quand vous lisez un seul byte en mÃ©moire, le CPU charge toute la ligne de cache (64 bytes) contenant ce byte.

```c
// Exemple
int tableau[16];  // 16 * 4 = 64 bytes = 1 ligne de cache

// Quand vous accÃ©dez Ã  tableau[0]
int x = tableau[0];  // Le CPU charge tableau[0] Ã  tableau[15] en une fois !
```

**ConsÃ©quence :** AccÃ©der Ã  `tableau[1]` juste aprÃ¨s `tableau[0]` est **gratuit** (dÃ©jÃ  dans le cache) !

### Cache hit vs Cache miss

- **Cache hit** : La donnÃ©e demandÃ©e est dÃ©jÃ  dans le cache â†’ AccÃ¨s rapide âœ…
- **Cache miss** : La donnÃ©e n'est pas dans le cache â†’ Il faut aller la chercher en RAM â†’ AccÃ¨s lent âŒ

### Types de cache miss

1. **Compulsory miss (Cold miss)** : Premier accÃ¨s Ã  une donnÃ©e (inÃ©vitable)
2. **Capacity miss** : Le cache est plein, des donnÃ©es anciennes sont Ã©vincÃ©es
3. **Conflict miss** : Collision dans le cache (architecture associative)

---

## LocalitÃ© spatiale et temporelle

### LocalitÃ© temporelle (Temporal Locality)

**Principe :** Si vous accÃ©dez Ã  une donnÃ©e maintenant, vous Ãªtes susceptible de la rÃ©utiliser **bientÃ´t**.

**Exemple :**
```c
int somme = 0;
for (int i = 0; i < 1000; i++) {
    somme += i;  // 'somme' est rÃ©utilisÃ©e Ã  chaque itÃ©ration
}
```

La variable `somme` reste dans un registre CPU ou le cache L1, ce qui est trÃ¨s rapide.

### LocalitÃ© spatiale (Spatial Locality)

**Principe :** Si vous accÃ©dez Ã  une donnÃ©e, vous Ãªtes susceptible d'accÃ©der aux donnÃ©es **voisines** bientÃ´t.

**Exemple cache-friendly :**
```c
int tableau[1000];

// BON : AccÃ¨s sÃ©quentiel (localitÃ© spatiale)
for (int i = 0; i < 1000; i++) {
    tableau[i] = i;
}
```

Les Ã©lÃ©ments contigus en mÃ©moire sont chargÃ©s ensemble dans les lignes de cache.

**Exemple cache-hostile :**
```c
// MAUVAIS : AccÃ¨s alÃ©atoire
for (int i = 0; i < 1000; i++) {
    int index = rand() % 1000;
    tableau[index] = i;
}
```

Chaque accÃ¨s peut provoquer un cache miss.

---

## Exemples de code : Impact du cache

### Exemple 1 : Parcours de tableau (row-major vs column-major)

**Contexte :** Tableau 2D (matrice) en C

```c
#define TAILLE 1000
int matrice[TAILLE][TAILLE];  // 1000x1000 = 1 million d'entiers
```

**En mÃ©moire**, C stocke les tableaux 2D en **row-major order** (par ligne) :

```
matrice[0][0], matrice[0][1], matrice[0][2], ..., matrice[0][999],
matrice[1][0], matrice[1][1], ...
```

#### Parcours cache-friendly (par ligne)

```c
// âœ… BON : Parcours sÃ©quentiel (row-major)
for (int i = 0; i < TAILLE; i++) {
    for (int j = 0; j < TAILLE; j++) {
        matrice[i][j] = 0;  // AccÃ¨s continu en mÃ©moire
    }
}
```

**Explication :** On accÃ¨de Ã  `matrice[i][0]`, `matrice[i][1]`, `matrice[i][2]`... qui sont contigus en mÃ©moire. Le cache prÃ©charge les donnÃ©es voisines, donc chaque accÃ¨s est un cache hit (aprÃ¨s le premier).

#### Parcours cache-hostile (par colonne)

```c
// âŒ MAUVAIS : Parcours par colonne (column-major)
for (int j = 0; j < TAILLE; j++) {
    for (int i = 0; i < TAILLE; i++) {
        matrice[i][j] = 0;  // AccÃ¨s non continu (stride de 1000)
    }
}
```

**Explication :** On accÃ¨de Ã  `matrice[0][j]`, `matrice[1][j]`, `matrice[2][j]`... Ces Ã©lÃ©ments sont espacÃ©s de 1000 entiers (4000 bytes) en mÃ©moire. Chaque accÃ¨s peut provoquer un cache miss.

#### Mesure de performance

```c
// test_cache.c
#include <stdio.h>
#include <time.h>

#define TAILLE 1000

int matrice[TAILLE][TAILLE];

void parcours_ligne() {
    for (int i = 0; i < TAILLE; i++) {
        for (int j = 0; j < TAILLE; j++) {
            matrice[i][j] = 0;
        }
    }
}

void parcours_colonne() {
    for (int j = 0; j < TAILLE; j++) {
        for (int i = 0; i < TAILLE; i++) {
            matrice[i][j] = 0;
        }
    }
}

int main() {
    clock_t debut, fin;

    // Test parcours par ligne
    debut = clock();
    parcours_ligne();
    fin = clock();
    printf("Parcours ligne:   %.3f ms\n",
           (double)(fin - debut) * 1000 / CLOCKS_PER_SEC);

    // Test parcours par colonne
    debut = clock();
    parcours_colonne();
    fin = clock();
    printf("Parcours colonne: %.3f ms\n",
           (double)(fin - debut) * 1000 / CLOCKS_PER_SEC);

    return 0;
}
```

**Compilation et exÃ©cution :**
```bash
gcc -O2 test_cache.c -o test_cache
./test_cache
```

**RÃ©sultats typiques :**
```
Parcours ligne:   1.2 ms
Parcours colonne: 8.5 ms  â† 7x plus lent !
```

**Conclusion :** Le parcours par colonne est **7 fois plus lent** Ã  cause des cache misses !

### Exemple 2 : Structure de donnÃ©es et alignement

#### Structure mal alignÃ©e (cache-hostile)

```c
struct Point {
    int x;          // 4 bytes
    char valide;    // 1 byte
    int y;          // 4 bytes
    char couleur;   // 1 byte
};  // Taille rÃ©elle : 16 bytes (Ã  cause du padding)

struct Point points[1000];

// Parcourir tous les x
for (int i = 0; i < 1000; i++) {
    int valeur = points[i].x;  // Les 'x' ne sont pas contigus
}
```

**ProblÃ¨me :** Les valeurs `x` sont espacÃ©es de 16 bytes (taille de la structure). Si vous voulez juste les `x`, vous chargez aussi les autres champs inutilement.

#### Structure rÃ©organisÃ©e (cache-friendly)

**Option 1 : RÃ©organiser les champs**
```c
struct Point {
    int x;          // 4 bytes
    int y;          // 4 bytes
    char valide;    // 1 byte
    char couleur;   // 1 byte
    // padding: 2 bytes
};  // Taille : 12 bytes (plus compact)
```

**Option 2 : Structure of Arrays (SoA)**
```c
// PlutÃ´t que Array of Structures (AoS)
struct Points {
    int x[1000];       // Tous les x ensemble
    int y[1000];       // Tous les y ensemble
    char valide[1000];
    char couleur[1000];
};

struct Points points;

// Parcourir tous les x : accÃ¨s sÃ©quentiel !
for (int i = 0; i < 1000; i++) {
    int valeur = points.x[i];  // âœ… Contigus en mÃ©moire
}
```

**Avantages SoA :**
- âœ… Meilleure utilisation du cache (donnÃ©es du mÃªme type ensemble)
- âœ… Vectorisation SIMD facilitÃ©e
- âœ… Moins de padding gaspillÃ©

**InconvÃ©nients SoA :**
- âŒ Moins intuitif
- âŒ Plus difficile Ã  passer en paramÃ¨tre
- âŒ AccÃ¨s Ã  un "point complet" plus complexe

---

## Techniques pour optimiser l'utilisation du cache

### 1. Parcourir les tableaux sÃ©quentiellement

**âœ… BON :**
```c
for (int i = 0; i < N; i++) {
    tableau[i] = valeur;
}
```

**âŒ MAUVAIS :**
```c
for (int i = 0; i < N; i += 8) {  // Stride = 8
    tableau[i] = valeur;
}
```

### 2. Utiliser des boucles avec un stride de 1

**Stride** = distance entre deux accÃ¨s successifs

```c
// Stride = 1 (optimal)
for (int i = 0; i < N; i++) {
    tableau[i] = 0;
}

// Stride = 2 (moins bon)
for (int i = 0; i < N; i += 2) {
    tableau[i] = 0;
}

// Stride = 1000 (trÃ¨s mauvais si TAILLE_CACHE < 4000 bytes)
for (int i = 0; i < 1000; i++) {
    matrice[i][0] = 0;
}
```

**RÃ¨gle d'or :** Plus le stride est grand, plus il y a de cache misses.

### 3. Regrouper les donnÃ©es frÃ©quemment utilisÃ©es ensemble

**âŒ MAUVAIS :**
```c
struct Personne {
    char nom[100];         // 100 bytes, rarement utilisÃ©
    int age;               // 4 bytes, souvent utilisÃ©
    char adresse[200];     // 200 bytes, rarement utilisÃ©
    int salaire;           // 4 bytes, souvent utilisÃ©
};
```

Si vous ne faites que lire `age` et `salaire`, vous chargez aussi `nom` et `adresse` dans le cache, gaspillant de l'espace.

**âœ… BON : SÃ©parer les donnÃ©es chaudes et froides**
```c
struct PersonneHot {     // "Hot data" = frÃ©quemment utilisÃ©
    int age;
    int salaire;
};

struct PersonneCold {    // "Cold data" = rarement utilisÃ©
    char nom[100];
    char adresse[200];
};
```

### 4. Bloquer les boucles (Loop Tiling / Blocking)

**Technique avancÃ©e** : Diviser une grosse boucle en petits blocs qui tiennent dans le cache.

**Exemple : Multiplication de matrices**

```c
// âŒ Algorithme naÃ¯f
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        for (int k = 0; k < N; k++) {
            C[i][j] += A[i][k] * B[k][j];
        }
    }
}
```

Pour de grandes matrices (N=1000), cet algorithme fait beaucoup de cache misses car il parcourt `B` par colonne.

**âœ… Avec blocking (tiling)**

```c
#define BLOCK_SIZE 32  // Taille de bloc qui tient dans le cache

for (int ii = 0; ii < N; ii += BLOCK_SIZE) {
    for (int jj = 0; jj < N; jj += BLOCK_SIZE) {
        for (int kk = 0; kk < N; kk += BLOCK_SIZE) {
            // Traiter un bloc de BLOCK_SIZE x BLOCK_SIZE
            for (int i = ii; i < ii + BLOCK_SIZE; i++) {
                for (int j = jj; j < jj + BLOCK_SIZE; j++) {
                    for (int k = kk; k < kk + BLOCK_SIZE; k++) {
                        C[i][j] += A[i][k] * B[k][j];
                    }
                }
            }
        }
    }
}
```

Les petits blocs tiennent dans le cache L1, ce qui amÃ©liore drastiquement les performances.

**AmÃ©lioration typique :** 2-5x plus rapide sur de grandes matrices !

### 5. PrÃ©chargement de donnÃ©es (Prefetching)

**PrÃ©chargement manuel (hint au CPU)**

```c
#include <xmmintrin.h>  // Pour _mm_prefetch

for (int i = 0; i < N - 8; i++) {
    _mm_prefetch(&tableau[i + 8], _MM_HINT_T0);  // PrÃ©charger 8 Ã©lÃ©ments Ã  l'avance
    // Traitement de tableau[i]
}
```

**Note :** Les CPUs modernes font dÃ©jÃ  du prefetching automatique pour les accÃ¨s sÃ©quentiels. Cette technique est rarement nÃ©cessaire en C classique.

### 6. Aligner les donnÃ©es en mÃ©moire

```c
// Forcer l'alignement sur 64 bytes (taille d'une ligne de cache)
struct __attribute__((aligned(64))) DonneesAlignees {
    int valeurs[16];
};
```

**Avantages :**
- âœ… Ã‰vite qu'une structure chevauche deux lignes de cache
- âœ… AmÃ©liore les performances de certaines instructions SIMD

---

## False Sharing (Faux partage)

### ProblÃ¨me spÃ©cifique au multi-threading

Le **false sharing** se produit quand deux threads modifient des variables diffÃ©rentes qui se trouvent sur la **mÃªme ligne de cache**.

**Exemple problÃ©matique :**

```c
// Structure partagÃ©e entre threads
struct Compteurs {
    int compteur_thread1;  // 4 bytes
    int compteur_thread2;  // 4 bytes (sur la mÃªme ligne de cache !)
} compteurs;

// Thread 1
void* thread1_func(void* arg) {
    for (int i = 0; i < 1000000; i++) {
        compteurs.compteur_thread1++;  // Modifie ligne de cache
    }
    return NULL;
}

// Thread 2
void* thread2_func(void* arg) {
    for (int i = 0; i < 1000000; i++) {
        compteurs.compteur_thread2++;  // Modifie MÃŠME ligne de cache
    }
    return NULL;
}
```

**Pourquoi c'est un problÃ¨me ?**

1. Thread 1 modifie `compteur_thread1`
2. La ligne de cache entiÃ¨re (contenant aussi `compteur_thread2`) est marquÃ©e "modifiÃ©e"
3. Thread 2 veut modifier `compteur_thread2`
4. Le cache de Thread 2 doit invalider sa copie et recharger la ligne depuis le cache de Thread 1
5. **Ping-pong de lignes de cache entre les threads** â†’ Performance catastrophique

### Solution : Padding pour sÃ©parer les donnÃ©es

```c
#define CACHE_LINE_SIZE 64

struct Compteurs {
    int compteur_thread1;
    char padding1[CACHE_LINE_SIZE - sizeof(int)];  // Remplissage

    int compteur_thread2;
    char padding2[CACHE_LINE_SIZE - sizeof(int)];
};
```

Maintenant `compteur_thread1` et `compteur_thread2` sont sur des **lignes de cache diffÃ©rentes**, donc pas de false sharing !

**AmÃ©lioration typique :** 10-100x plus rapide sur du code massivement parallÃ¨le.

**Alternative moderne (C11) :**

```c
#include <stdalign.h>

struct Compteurs {
    alignas(64) int compteur_thread1;
    alignas(64) int compteur_thread2;
};
```

---

## Mesurer l'efficacitÃ© du cache

### Avec perf (Linux)

```bash
# Compiler le programme
gcc -O2 -g test_cache.c -o test_cache

# Mesurer les cache misses
perf stat -e cache-references,cache-misses ./test_cache

# Exemple de sortie :
#    4,123,456      cache-references
#      234,567      cache-misses              #    5.69% of all cache refs
```

**InterprÃ©tation :**
- **Cache miss rate < 5%** : Excellent âœ…
- **Cache miss rate 5-10%** : Bon
- **Cache miss rate 10-20%** : Acceptable
- **Cache miss rate > 20%** : ProblÃ©matique âŒ

### Avec Valgrind Cachegrind

```bash
valgrind --tool=cachegrind ./test_cache

# Sortie exemple :
# I   refs:      10,000,000
# I1  misses:        12,345  (0.12%)
# LLi misses:         1,234  (0.01%)
#
# D   refs:       5,000,000  (3,000,000 rd + 2,000,000 wr)
# D1  misses:       456,789  (9.14%)
# LLd misses:        45,678  (0.91%)
```

**LÃ©gende :**
- **I** = Instructions (code)
- **D** = DonnÃ©es
- **I1/D1** = Cache L1
- **LL** = Last Level cache (L3)

**Analyse :**
- **D1 misses = 9.14%** â†’ Taux de dÃ©faut du cache L1 pour les donnÃ©es
- Si ce taux est Ã©levÃ©, optimiser l'accÃ¨s aux donnÃ©es

### Comparer deux versions du code

```bash
# Version cache-hostile
valgrind --tool=cachegrind ./test_cache_bad > rapport_bad.txt

# Version cache-friendly
valgrind --tool=cachegrind ./test_cache_good > rapport_good.txt

# Comparer
cg_diff rapport_bad.txt rapport_good.txt
```

---

## Cas d'Ã©tude : Somme de tableau

### Version 1 : NaÃ¯ve

```c
// sum_naive.c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#define TAILLE 10000000  // 10 millions

int main() {
    int *tableau = malloc(TAILLE * sizeof(int));

    for (int i = 0; i < TAILLE; i++) {
        tableau[i] = i;
    }

    clock_t debut = clock();

    long long somme = 0;
    for (int i = 0; i < TAILLE; i++) {
        somme += tableau[i];
    }

    clock_t fin = clock();

    printf("Somme: %lld\n", somme);
    printf("Temps: %.3f ms\n", (double)(fin - debut) * 1000 / CLOCKS_PER_SEC);

    free(tableau);
    return 0;
}
```

**Performance :**
```bash
gcc -O2 sum_naive.c -o sum_naive
./sum_naive
# Temps: 15.2 ms
```

### Version 2 : Avec dÃ©roulage de boucle (loop unrolling)

```c
// sum_unroll.c
long long somme = 0;
int i;

// Traiter 4 Ã©lÃ©ments Ã  la fois
for (i = 0; i < TAILLE - 3; i += 4) {
    somme += tableau[i];
    somme += tableau[i + 1];
    somme += tableau[i + 2];
    somme += tableau[i + 3];
}

// Reste
for (; i < TAILLE; i++) {
    somme += tableau[i];
}
```

**Performance :**
```bash
gcc -O2 sum_unroll.c -o sum_unroll
./sum_unroll
# Temps: 12.8 ms  â† 15% plus rapide
```

**Pourquoi ?** Moins de sauts conditionnels, meilleure utilisation des registres, parallÃ©lisme au niveau instruction.

### Version 3 : Avec plusieurs accumulateurs

```c
// sum_multi_acc.c
long long somme1 = 0, somme2 = 0, somme3 = 0, somme4 = 0;
int i;

// 4 accumulateurs indÃ©pendants
for (i = 0; i < TAILLE - 3; i += 4) {
    somme1 += tableau[i];
    somme2 += tableau[i + 1];
    somme3 += tableau[i + 2];
    somme4 += tableau[i + 3];
}

long long somme = somme1 + somme2 + somme3 + somme4;

// Reste
for (; i < TAILLE; i++) {
    somme += tableau[i];
}
```

**Performance :**
```bash
gcc -O2 sum_multi_acc.c -o sum_multi_acc
./sum_multi_acc
# Temps: 8.5 ms  â† 44% plus rapide que la version naÃ¯ve !
```

**Pourquoi ?** Les 4 accumulateurs peuvent Ãªtre calculÃ©s **en parallÃ¨le** par le CPU (instruction-level parallelism). Pas de dÃ©pendance entre les calculs.

**Mesure des cache misses :**
```bash
perf stat -e cache-references,cache-misses ./sum_naive
perf stat -e cache-references,cache-misses ./sum_multi_acc

# Les deux versions ont le mÃªme taux de cache misses (accÃ¨s sÃ©quentiel)
# Mais la version multi-accumulateurs est plus rapide grÃ¢ce au parallÃ©lisme
```

---

## Structures de donnÃ©es cache-friendly

### Listes chaÃ®nÃ©es vs Tableaux

**Liste chaÃ®nÃ©e :**
```c
struct Node {
    int valeur;
    struct Node *suivant;  // Pointeur vers le nÅ“ud suivant
};
```

**ProblÃ¨me :** Les nÅ“uds peuvent Ãªtre **Ã©parpillÃ©s en mÃ©moire** (Ã  cause de malloc). Parcourir la liste provoque beaucoup de cache misses.

**Tableau dynamique :**
```c
int *tableau = malloc(1000 * sizeof(int));
```

**Avantage :** Ã‰lÃ©ments **contigus en mÃ©moire**. Parcours sÃ©quentiel trÃ¨s cache-friendly.

**RÃ¨gle gÃ©nÃ©rale :** PrÃ©fÃ©rez les tableaux aux listes chaÃ®nÃ©es pour les performances (sauf si insertions/suppressions frÃ©quentes).

### Pool d'allocation

Si vous devez absolument utiliser des listes chaÃ®nÃ©es, allouez les nÅ“uds dans un **pool** :

```c
#define POOL_SIZE 10000

struct Node pool[POOL_SIZE];  // NÅ“uds contigus !
int pool_index = 0;

struct Node* allouer_node() {
    if (pool_index < POOL_SIZE) {
        return &pool[pool_index++];
    }
    return NULL;
}
```

Les nÅ“uds Ã©tant contigus, le parcours est plus cache-friendly.

---

## Recommandations pratiques

### âœ… Faire

1. **Parcourir les tableaux sÃ©quentiellement** (stride = 1)
2. **Utiliser des tableaux plutÃ´t que des listes chaÃ®nÃ©es** quand possible
3. **Regrouper les donnÃ©es chaudes ensemble** (hot/cold data separation)
4. **Aligner les structures critiques sur des lignes de cache**
5. **Ã‰viter le false sharing dans le code multi-threadÃ©**
6. **Utiliser Structure of Arrays (SoA)** pour les gros datasets
7. **Blocker les boucles** (loop tiling) pour de grosses matrices
8. **Mesurer avec perf ou Valgrind** avant d'optimiser

### âŒ Ã‰viter

1. **AccÃ¨s alÃ©atoires en mÃ©moire**
2. **Parcours par colonne de matrices en C**
3. **Structures avec beaucoup de padding inutile**
4. **Variables partagÃ©es entre threads sur la mÃªme ligne de cache**
5. **Listes chaÃ®nÃ©es pour de grands datasets**
6. **Sauts de pointeurs indirects**
7. **Stride trop grand dans les boucles**

---

## Checklist : Code cache-friendly

Avant d'Ã©crire du code critique en performance :

- âœ… Les donnÃ©es frÃ©quemment accÃ©dÃ©es sont-elles contiguÃ«s ?
- âœ… Les boucles parcourent-elles les tableaux sÃ©quentiellement ?
- âœ… Les structures sont-elles compactes (peu de padding) ?
- âœ… Les donnÃ©es chaudes sont-elles sÃ©parÃ©es des donnÃ©es froides ?
- âœ… Y a-t-il du false sharing dans le code multi-threadÃ© ?
- âœ… Puis-je utiliser un tableau au lieu d'une liste chaÃ®nÃ©e ?

AprÃ¨s avoir Ã©crit le code :

- âœ… Ai-je mesurÃ© le taux de cache misses avec perf ?
- âœ… Ai-je comparÃ© avec une version alternative ?
- âœ… L'optimisation a-t-elle vraiment amÃ©liorÃ© les performances ?

---

## Outils pour analyser le cache

### perf (Linux)

```bash
# Cache miss rate global
perf stat -e cache-references,cache-misses ./programme

# Cache L1, L2, L3 dÃ©taillÃ©
perf stat -e L1-dcache-loads,L1-dcache-load-misses \
          -e LLC-loads,LLC-load-misses ./programme

# Profiler les cache misses par fonction
perf record -e cache-misses ./programme
perf report
```

### Valgrind Cachegrind

```bash
# Profiling dÃ©taillÃ© du cache
valgrind --tool=cachegrind --cache-sim=yes ./programme

# Voir le rapport
cg_annotate cachegrind.out.<pid>

# Visualisation avec KCachegrind
kcachegrind cachegrind.out.<pid>
```

### Intel VTune (propriÃ©taire mais puissant)

Pour les processeurs Intel, VTune offre une analyse trÃ¨s dÃ©taillÃ©e du cache, des pipelines CPU, etc.

---

## Mythes et rÃ©alitÃ©s

### âŒ Mythe : "Les compilateurs optimisent tout automatiquement"

**RÃ©alitÃ© :** Les compilateurs font un excellent travail, mais ils ne peuvent pas :
- Changer l'ordre de parcours d'un tableau multidimensionnel
- Transformer Array of Structures en Structure of Arrays
- Deviner que vous voulez du loop tiling

**Votre responsabilitÃ© :** Structurer le code de maniÃ¨re cache-friendly.

### âŒ Mythe : "L'optimisation cache n'est utile que pour les supercalculateurs"

**RÃ©alitÃ© :** MÃªme sur un smartphone ou un Raspberry Pi, le cache a un impact Ã©norme. Les principes restent les mÃªmes.

### âœ… RÃ©alitÃ© : "L'optimisation prÃ©maturÃ©e est mauvaise, mais la conception cache-aware dÃ¨s le dÃ©part est bonne"

Ã‰crire du code sÃ©quentiel et compact dÃ¨s le dÃ©but ne coÃ»te rien et Ã©vite des refactorisations coÃ»teuses plus tard.

---

## Pour aller plus loin

### Lectures recommandÃ©es

- **"What Every Programmer Should Know About Memory"** par Ulrich Drepper (PDF gratuit, rÃ©fÃ©rence absolue)
- **"Computer Architecture: A Quantitative Approach"** par Hennessy & Patterson
- **"Systems Performance"** par Brendan Gregg (chapitre sur la mÃ©moire et le cache)

### Ressources en ligne

- **Gallery of Processor Cache Effects** : http://igoro.com/archive/gallery-of-processor-cache-effects/
- **Cache-Oblivious Algorithms** : https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
- **Intel Optimization Manual** : Documentation dÃ©taillÃ©e sur l'architecture x86

### Commandes utiles

```bash
# Afficher la hiÃ©rarchie mÃ©moire
lstopo        # NÃ©cessite hwloc : sudo apt install hwloc

# Infos dÃ©taillÃ©es CPU
cat /sys/devices/system/cpu/cpu0/cache/index*/size
cat /sys/devices/system/cpu/cpu0/cache/index*/level
cat /sys/devices/system/cpu/cpu0/cache/index*/type

# Benchmark mÃ©moire
sysbench memory run
```

---

## RÃ©sumÃ©

Le cache est la clÃ© des performances modernes :

1. âœ… **Principe fondamental** : AccÃ©der au cache est 100-300x plus rapide que la RAM
2. âœ… **LocalitÃ© spatiale** : DonnÃ©es voisines en mÃ©moire â†’ chargÃ©es ensemble
3. âœ… **LocalitÃ© temporelle** : DonnÃ©es rÃ©cemment utilisÃ©es â†’ restent dans le cache
4. âœ… **Lignes de cache** : 64 bytes chargÃ©s Ã  la fois
5. âœ… **Parcours sÃ©quentiel** : Toujours prÃ©fÃ©rer stride = 1
6. âœ… **False sharing** : Attention au multi-threading !
7. âœ… **Mesurer** : Utiliser perf et Valgrind pour identifier les problÃ¨mes

**Citation :**

> "Memory is the new disk, disk is the new tape" â€” Jim Gray

Les accÃ¨s mÃ©moire sont devenus le goulot d'Ã©tranglement principal. Ã‰crire du code cache-aware n'est plus optionnel pour les applications critiques en performance !

---

*Prochaine section : 27.5 Branch prediction*

â­ï¸ [Branch prediction](/27-optimisation-performance/05-branch-prediction.md)
