üîù Retour au [Sommaire](/SOMMAIRE.md)

# 21.3.4 Performance Analysis avec eBPF

## Introduction

L'**analyse de performance** (performance analysis) est l'art de comprendre **pourquoi** un syst√®me est lent et **o√π** se trouvent les goulots d'√©tranglement. C'est un d√©fi permanent pour tout ing√©nieur DevOps ou d√©veloppeur :

- Pourquoi mon API r√©pond en 2 secondes au lieu de 200 ms ?
- Quelle fonction consomme 80% du CPU ?
- Pourquoi mes requ√™tes SQL sont-elles si lentes ?
- Mon application fait-elle trop d'allocations m√©moire ?
- Combien de temps passe-t-on √† attendre des I/O disque ?

Historiquement, r√©pondre √† ces questions en **production** √©tait extr√™mement difficile car les outils de profiling traditionnels avaient un **overhead prohibitif** (50-100% de ralentissement), rendant leur utilisation impossible sur des syst√®mes en charge.

eBPF change la donne en permettant un **profiling continu en production** avec un overhead n√©gligeable (< 1%), ouvrant la porte √† une nouvelle √®re d'observabilit√© des performances.

---

## Les limites du profiling traditionnel

### 1. **Les outils avec overhead √©lev√©**

#### gprof : Le dinosaure du profiling C

`gprof` est l'outil historique pour profiler du code C, mais il n√©cessite de **recompiler** le code avec des flags sp√©ciaux :

```bash
gcc -pg program.c -o program  # Compilation avec instrumentation
./program                      # Ex√©cution (g√©n√®re gmon.out)
gprof program gmon.out        # Analyse
```

**Limitations :**
- ‚ùå **Overhead massif** : 50-200% de ralentissement
- ‚ùå N√©cessite **recompilation** avec `-pg`
- ‚ùå **Inutilisable en production**
- ‚ùå Pas de visibilit√© kernel
- ‚ùå Mesures impr√©cises (sampling basique)

#### Valgrind --tool=callgrind

Valgrind peut profiler le CPU, mais au prix d'un ralentissement extr√™me :

```bash
valgrind --tool=callgrind ./program
```

**Limitations :**
- ‚ùå **Overhead catastrophique** : 10-100x plus lent
- ‚ùå Absolument **inutilisable en production**
- ‚ùå R√©sultats fauss√©s par l'instrumentation lourde

### 2. **perf : Mieux mais incomplet**

`perf` (Linux Performance Events) est l'outil standard pour le profiling sous Linux.

```bash
# Profiler pendant 30 secondes
perf record -F 99 -ag -- sleep 30  
perf report  
```

**Avantages :**
- ‚úÖ Overhead faible (2-5%)
- ‚úÖ Sampling au niveau kernel
- ‚úÖ Support hardware counters (caches CPU, branch prediction)

**Limitations :**
- ‚ö†Ô∏è **Sampling statistique** : peut manquer des hotspots courts
- ‚ö†Ô∏è Pas de visibilit√© sur les **latences** (seulement CPU time)
- ‚ö†Ô∏è Difficile d'analyser les **attentes I/O**
- ‚ö†Ô∏è Pas de support natif pour les **flamegraphs**
- ‚ö†Ô∏è Pas d'agr√©gation par application/conteneur

### 3. **Logs et m√©triques applicatives**

Ajouter des logs pour mesurer la performance :

```c
clock_t start = clock();  
expensive_function();  
clock_t end = clock();  
printf("Time: %f ms\n", (end - start) * 1000.0 / CLOCKS_PER_SEC);  
```

**Limitations :**
- ‚ùå N√©cessite de **modifier le code source**
- ‚ùå Overhead non n√©gligeable (I/O pour √©crire les logs)
- ‚ùå Tr√®s verbeux en production
- ‚ùå Pas de vision syst√®me globale

### 4. **Le probl√®me fondamental : Observer sans perturber**

En physique quantique, le **principe d'incertitude** dit qu'on ne peut pas observer un syst√®me sans le perturber. C'est exactement le probl√®me du profiling :

```
Plus un outil est pr√©cis ‚Üí Plus il ralentit le syst√®me  
Plus il a un faible overhead ‚Üí Moins il est pr√©cis  
```

**eBPF r√©sout ce dilemme** en offrant √† la fois **pr√©cision** et **overhead minimal**.

---

## Comment eBPF r√©volutionne le profiling

### 1. **Overhead n√©gligeable : Le profiling continu**

eBPF permet de profiler en continu, 24/7, en production, avec **< 1% d'overhead**.

**Comparaison d'overhead :**

| Outil | Overhead CPU | Utilisable en production ? |
|-------|-------------|---------------------------|
| **gprof** | 50-200% | ‚ùå Non |
| **Valgrind callgrind** | 1000-10000% | ‚ùå Absolument pas |
| **strace** | 100-1000% | ‚ùå Non |
| **perf** | 2-5% | ‚ö†Ô∏è Oui mais limit√© |
| **eBPF (BCC/bpftrace)** | **< 1%** | ‚úÖ **Oui, 24/7** |

### 2. **Pr√©cision √† la microseconde**

eBPF capture les √©v√©nements avec une pr√©cision **nanoseconde** :

```c
// Capturer le timestamp exact
u64 timestamp_ns = bpf_ktime_get_ns();
```

Cela permet d'analyser :
- La latence d'une fonction sp√©cifique (ex : 42.3 ¬µs)
- Le temps pass√© dans chaque √©tat (running, waiting, I/O)
- Les variations de performance au fil du temps

### 3. **Visibilit√© compl√®te : Du hardware au userspace**

eBPF peut observer **tous les niveaux** du syst√®me :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Application (userspace)        ‚îÇ
‚îÇ   - Fonctions C/C++/Go/Python       ‚îÇ ‚Üê eBPF uprobes
‚îÇ   - Allocations m√©moire             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Biblioth√®ques (libc, etc)      ‚îÇ
‚îÇ   - malloc/free                     ‚îÇ ‚Üê eBPF uprobes
‚îÇ   - pthread_create                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Kernel Linux                ‚îÇ
‚îÇ   - Syscalls                        ‚îÇ ‚Üê eBPF tracepoints
‚îÇ   - Scheduler                       ‚îÇ ‚Üê eBPF kprobes
‚îÇ   - File I/O                        ‚îÇ
‚îÇ   - Network stack                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Hardware                   ‚îÇ
‚îÇ   - CPU cycles                      ‚îÇ ‚Üê eBPF + PMU
‚îÇ   - Cache misses                    ‚îÇ
‚îÇ   - Branch mispredictions           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4. **Agr√©gation intelligente dans le kernel**

Au lieu d'envoyer chaque √©v√©nement vers l'userspace (co√ªteux), eBPF **agr√®ge les donn√©es directement dans le kernel** :

```c
// BPF map pour compter les appels par fonction
BPF_HASH(counts, u64);  // cl√© = adresse fonction, valeur = count

int trace_function(struct pt_regs *ctx) {
    u64 ip = PT_REGS_IP(ctx);  // Instruction pointer
    u64 *val = counts.lookup(&ip);
    if (val) {
        (*val)++;
    } else {
        u64 one = 1;
        counts.update(&ip, &one);
    }
    return 0;
}
```

**R√©sultat** : Au lieu de millions d'√©v√©nements, on obtient un r√©sum√© agr√©g√© (ex : "fonction X appel√©e 42 000 fois").

---

## Les types de profiling avec eBPF

### 1. **CPU Profiling : O√π passe le temps CPU ?**

Le CPU profiling identifie les **fonctions qui consomment le plus de CPU**.

#### Technique : Stack Sampling

eBPF √©chantillonne p√©riodiquement (ex : 99 Hz) la **pile d'appels** (call stack) de tous les processus :

```
√âchantillon 1 (t=0.00s):
  main() ‚Üí process_request() ‚Üí parse_json() ‚Üí malloc()

√âchantillon 2 (t=0.01s):
  main() ‚Üí process_request() ‚Üí parse_json() ‚Üí strcmp()

√âchantillon 3 (t=0.02s):
  main() ‚Üí process_request() ‚Üí database_query() ‚Üí socket_write()

...apr√®s 10 000 √©chantillons...

R√©sultat agr√©g√© :
  parse_json() : 45% du temps CPU
  database_query() : 30%
  malloc() : 15%
  autres : 10%
```

#### Outil BCC : profile

```bash
# Profiler l'ensemble du syst√®me pendant 30 secondes (99 Hz)
$ profile-bpf 30
Sampling at 99 Hertz of all threads by user + kernel stack... Hit Ctrl-C to end.

main                           42342 samples  (45%)
  process_request              42342
    parse_json                 42342
      strcmp                   18954
      malloc                   14121

database_query                 28228 samples  (30%)
  send_query                   28228
    socket_write               28228

malloc                         14121 samples  (15%)
  ...
```

#### Flamegraphs : La visualisation ultime

Les **flamegraphs** (graphiques de flamme) visualisent les call stacks de mani√®re intuitive :

```
                   [     main     ]
                   /               \
    [process_request]           [background_task]
         /        \                    |
  [parse_json] [db_query]        [cleanup]
      /    \         |
[strcmp] [malloc] [socket_write]
```

**Largeur** = % de temps CPU  
**Hauteur** = Profondeur de la pile d'appels  

**G√©n√©ration d'un flamegraph :**

```bash
# 1. Profiler avec eBPF
$ profile-bpf -F 99 -ag 30 > profile.txt

# 2. G√©n√©rer le flamegraph (SVG interactif)
$ flamegraph.pl profile.txt > flamegraph.svg

# 3. Ouvrir dans le navigateur
$ firefox flamegraph.svg
```

**Interpr√©tation :**
- Les **fonctions larges** = hotspots (beaucoup de temps CPU)
- Cliquer sur une fonction pour zoomer
- Chercher les "plateaux" (fonctions leaf qui consomment)

### 2. **Off-CPU Profiling : O√π passe le temps d'attente ?**

Le **off-CPU profiling** mesure le temps o√π un thread **n'utilise PAS le CPU** :
- Attente I/O (disque, r√©seau)
- Attente locks (mutex, semaphores)
- Sleep volontaire
- Attente scheduler

#### Pourquoi c'est crucial

Exemple : Une API qui r√©pond en 2 secondes

```
CPU profiling :
  - parse_request() : 10 ms
  - database_query() : 50 ms
  - format_response() : 5 ms
  Total : 65 ms sur CPU

Mais l'API prend 2000 ms ! O√π sont les 1935 ms restants ?
```

**R√©ponse avec off-CPU profiling :**

```
Off-CPU profiling :
  - Attente query DB : 1800 ms (connexion satur√©e)
  - Attente lock mutex : 100 ms (contention)
  - Attente r√©seau : 35 ms
  Total : 1935 ms
```

#### Outil BCC : offcputime

```bash
# Profiler les temps d'attente (off-CPU) pendant 30s
$ offcputime-bpf 30

main                           1800000000 ns  (1.8s)
  process_request              1800000000
    database_query             1800000000
      [wait on socket]         1800000000

main                           100000000 ns   (100ms)
  process_request              100000000
    update_cache               100000000
      [wait on pthread_mutex]  100000000
```

#### Combiner CPU + Off-CPU

Pour une analyse compl√®te, on combine les deux :

```
Total response time = CPU time + Off-CPU time
    2000 ms        =   65 ms   +   1935 ms
```

**Flamegraph combin√©** : Montre les deux types de temps dans un seul graphe.

### 3. **Memory Profiling : Allocations et fuites**

eBPF peut tracer **toutes les allocations m√©moire** pour d√©tecter :
- Les fonctions qui allouent le plus
- Les fuites m√©moire (allocations non lib√©r√©es)
- La fragmentation m√©moire

#### Outil BCC : memleak

```bash
# D√©tecter les fuites m√©moire dans un processus
$ memleak-bpf -p 1234
Tracing outstanding memory allocations... Ctrl-C to end.

Top 10 stacks with outstanding allocations:
    addr = 7f8a4c0012a0 size = 4096
    [<ffffffff811c5c2b>] kmalloc
    [<ffffffff815a3d12>] tcp_sendmsg
    [<ffffffff815e2a41>] inet_sendmsg
    6428 bytes in 1 allocations from:
        tcp_sendmsg+0x1a
        ...
```

#### Outil bpftrace : Profiler malloc()

```bash
# Compter les allocations par taille
$ bpftrace -e '
  uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc {
    @alloc_sizes = hist(arg0);
  }
  uprobe:/lib/x86_64-linux-gnu/libc.so.6:free {
    @frees = count();
  }
'

Histogramme des tailles d'allocation :
@alloc_sizes:
[16, 32)              1234 |@@@@@@@@                        |
[32, 64)              4567 |@@@@@@@@@@@@@@@@@@@@@@@@        |
[64, 128)             8901 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
[128, 256)            2345 |@@@@@@@@@@@                     |
```

### 4. **I/O Profiling : Latence disque et r√©seau**

Les I/O sont souvent le goulot d'√©tranglement #1 des applications.

#### Outil BCC : biolatency (Block I/O)

```bash
# Histogramme de la latence des I/O disque
$ biolatency-bpf

Tracing block device I/O... Ctrl-C to end.

     usecs               : count     distribution
         0 -> 1          : 0        |                                        |
         2 -> 3          : 0        |                                        |
         4 -> 7          : 0        |                                        |
         8 -> 15         : 0        |                                        |
        16 -> 31         : 0        |                                        |
        32 -> 63         : 0        |                                        |
        64 -> 127        : 145      |@@@                                     |
       128 -> 255        : 1823     |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    |
       256 -> 511        : 1456     |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@          |
       512 -> 1023       : 234      |@@@@                                    |
      1024 -> 2047       : 67       |@                                       |
```

**Interpr√©tation** : La plupart des I/O prennent 128-511 ¬µs, mais certains vont jusqu'√† 2 ms.

#### Outil BCC : tcplife (TCP connections)

```bash
# Voir toutes les connexions TCP avec leur dur√©e et data transfer
$ tcplife-bpf

PID   COMM       LADDR           LPORT RADDR           RPORT TX_KB RX_KB MS
1234  postgres   127.0.0.1       5432  127.0.0.1       45231 0     12    234.5
5678  nginx      192.168.1.10    80    203.0.113.5     52341 156   8     1823.2
```

**Insights** :
- La connexion postgres a dur√© 234 ms
- La connexion nginx a dur√© 1,8 secondes (probl√®me ?)

### 5. **Scheduler Profiling : Temps d'attente CPU**

Le scheduler d√©cide quel processus s'ex√©cute sur le CPU. Si un processus attend longtemps avant d'√™tre schedul√©, c'est un probl√®me de contention CPU.

#### Outil BCC : runqlat

```bash
# Mesurer le temps d'attente dans la run queue du scheduler
$ runqlat-bpf

Tracing run queue latency... Ctrl-C to end.

     usecs               : count     distribution
         0 -> 1          : 12456    |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    |
         2 -> 3          : 8234     |@@@@@@@@@@@@@@@@@@@@@@@@                |
         4 -> 7          : 1234     |@@@                                     |
         8 -> 15         : 567      |@                                       |
        16 -> 31         : 89       |                                        |
        32 -> 63         : 23       |                                        |
        64 -> 127        : 5        |                                        |
```

**Interpr√©tation** : La plupart des threads attendent < 4 ¬µs, mais certains attendent jusqu'√† 127 ¬µs. Si les latences sont √©lev√©es (> 100 ¬µs), le syst√®me est surcharg√© (CPU saturation).

---

## Outils de profiling eBPF populaires

### 1. **BCC (BPF Compiler Collection) : La bo√Æte √† outils**

BCC fournit **70+ outils** pr√™ts √† l'emploi pour tous les types de profiling.

#### Classification des outils BCC

| Cat√©gorie | Outils | Description |
|-----------|--------|-------------|
| **CPU** | `profile`, `offcputime`, `cpudist` | Profiling CPU on/off |
| **M√©moire** | `memleak`, `slabratetop`, `mmapsnoop` | Allocations, fuites |
| **I/O Disque** | `biolatency`, `biotop`, `bitesize` | Latence, throughput |
| **I/O R√©seau** | `tcplife`, `tcpretrans`, `tcptop` | Connexions, latence TCP |
| **Filesystem** | `vfsstat`, `filelife`, `fileslower` | Op√©rations VFS |
| **Scheduler** | `runqlat`, `runqlen`, `cpuwalk` | Contention CPU |
| **Synchronisation** | `syncsnoop`, `llcstat` | Locks, caches |

#### Installation BCC

```bash
# Ubuntu 20.04+
sudo apt install bpfcc-tools linux-headers-$(uname -r)

# Les outils sont dans /usr/sbin/ avec le suffixe -bpfcc
ls /usr/sbin/*-bpfcc
# Exemples : profile-bpfcc, biolatency-bpfcc, tcplife-bpfcc
```

### 2. **bpftrace : Le langage de scripting**

`bpftrace` est un langage de haut niveau pour √©crire des **one-liners** de profiling custom.

#### Exemples de one-liners puissants

**Compter les syscalls par type :**

```bash
$ bpftrace -e 'tracepoint:syscalls:sys_enter_* { @[probe] = count(); }'

@[tracepoint:syscalls:sys_enter_read]: 12456
@[tracepoint:syscalls:sys_enter_write]: 8234
@[tracepoint:syscalls:sys_enter_poll]: 5678
```

**Mesurer la latence de read() :**

```bash
$ bpftrace -e '
  tracepoint:syscalls:sys_enter_read { @start[tid] = nsecs; }
  tracepoint:syscalls:sys_exit_read /@start[tid]/ {
    @latency_us = hist((nsecs - @start[tid]) / 1000);
    delete(@start[tid]);
  }
'
```

**Profiler une fonction sp√©cifique :**

```bash
# Tracer les appels √† malloc() avec la taille
$ bpftrace -e '
  uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc {
    printf("malloc(%d) called by %s\n", arg0, comm);
  }
'
```

### 3. **perf + eBPF : Le combo ultime**

Depuis Linux 4.x, `perf` peut utiliser eBPF pour √©tendre ses capacit√©s.

```bash
# Profiler avec perf et eBPF pour des flamegraphs
perf record -e cpu-clock -g -a -- sleep 30  
perf script | stackcollapse-perf.pl | flamegraph.pl > flame.svg  
```

### 4. **parca : Profiling continu et moderne**

**Parca** est une plateforme de profiling continu qui utilise eBPF.

**Fonctionnalit√©s :**
- Profiling 24/7 avec < 1% overhead
- Flamegraphs interactifs dans une UI web
- Comparaison de profils (avant/apr√®s d√©ploiement)
- Profiling dans Kubernetes (par pod/namespace)

**Installation :**

```bash
# D√©ployer dans Kubernetes
kubectl apply -f https://github.com/parca-dev/parca/releases/latest/download/kubernetes-manifest.yaml
```

### 5. **Pyroscope : Continuous Profiling Platform**

**Pyroscope** est une autre plateforme de profiling continu, compatible avec eBPF.

**Avantages :**
- Support multi-langages (C, Go, Python, Java, Node.js)
- Stockage long terme des profiles
- Analyse de tendances (√©volution performance dans le temps)
- Int√©gration avec Grafana

---

## Cas d'usage r√©els de performance analysis

### Sc√©nario 1 : "Mon API REST est lente"

**Contexte :** Une API REST r√©pond en 1,5s au lieu de 100ms attendus.

**Investigation avec eBPF :**

```bash
# √âtape 1 : CPU profiling
$ profile-bpf -p $(pgrep myapp) 30
Result : parse_json() consomme seulement 5% du CPU

# √âtape 2 : Off-CPU profiling
$ offcputime-bpf -p $(pgrep myapp) 30
Result :
  - wait_on_mutex (database_connection_pool) : 1200 ms
  - wait_on_socket (database_query) : 280 ms

# Diagnostic trouv√© !
Pool de connexions DB satur√© ‚Üí Threads attendent un mutex
+ Requ√™tes DB lentes (280ms chacune)
```

**Solution :**
- Augmenter la taille du pool de connexions DB
- Optimiser les requ√™tes SQL (indexes manquants)

**R√©sultat :** API passe de 1,5s √† 85ms üéâ

### Sc√©nario 2 : "Mon serveur consomme 100% CPU sans raison"

**Contexte :** Un serveur web consomme 100% CPU m√™me avec peu de charge.

**Investigation :**

```bash
# CPU profiling pour trouver le hotspot
$ profile-bpf 30

Result :  
main                           98456 samples  (98%)  
  event_loop                   98456
    check_timeout              98456
      gettimeofday             98456
```

**Diagnostic :** La fonction `check_timeout()` appelle `gettimeofday()` en boucle serr√©e !

**Code probl√©matique :**

```c
// BAD : Polling actif
while (1) {
    gettimeofday(&now, NULL);
    if (timeout_expired(&now)) {
        handle_timeout();
    }
    // Pas de sleep() ‚Üí Consomme 100% CPU !
}
```

**Solution :** Utiliser `poll()` ou `epoll()` avec timeout au lieu de polling actif.

### Sc√©nario 3 : "Fuite m√©moire myst√©rieuse"

**Contexte :** Une application consomme progressivement toute la RAM disponible.

**Investigation :**

```bash
# D√©tecter les fuites m√©moire
$ memleak-bpf -p $(pgrep myapp)

Top allocations non lib√©r√©es :
    addr = 0x7f8a4c001000 size = 1048576 bytes
    alloc_buffer+0x42
    process_image+0x123
    main+0x567

    Leaking 1 MB per call √† process_image()
```

**Diagnostic :** La fonction `process_image()` alloue un buffer de 1 MB mais ne le lib√®re jamais.

**Code probl√©matique :**

```c
void process_image(char *filename) {
    char *buffer = malloc(1024 * 1024);  // 1 MB
    read_image(filename, buffer);
    transform_image(buffer);
    // Oups : pas de free(buffer) ! üí£
}
```

**Solution :** Ajouter `free(buffer)`.

### Sc√©nario 4 : "Latence I/O disque anormale"

**Contexte :** Une base de donn√©es a des temps de r√©ponse erratiques.

**Investigation :**

```bash
# Analyser la latence des I/O disque
$ biolatency-bpf

     msecs               : count     distribution
         0 -> 1          : 8234     |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    |
         2 -> 3          : 456      |@@                                      |
         4 -> 7          : 123      |                                        |
         8 -> 15         : 89       |                                        |
        16 -> 31         : 234      |@                                       |
        32 -> 63         : 567      |@@                                      |
        64 -> 127        : 1234     |@@@@@                                   |
```

**Diagnostic :** Certains I/O prennent 64-127 ms (tr√®s lent pour du SSD !).

**Investigation plus pouss√©e :**

```bash
$ biotop-bpf
PID    COMM             D MAJ MIN  DISK       I/O  Kbytes  AVGms
8234   postgres         W 8   0    sda        4567 245678  89.2
```

Postgres fait beaucoup d'√©critures avec 89 ms de latence moyenne.

**Cause trouv√©e :** Disque satur√© (IOPS limit√©s). Solution : Migrer vers des disques plus rapides (NVMe).

### Sc√©nario 5 : "R√©gression de performance apr√®s d√©ploiement"

**Contexte :** Apr√®s un d√©ploiement, l'application est 30% plus lente.

**Investigation avec Parca (profiling continu) :**

```
Comparaison de flamegraphs :
  Avant d√©ploiement : parse_request() = 15% CPU
  Apr√®s d√©ploiement : parse_request() = 45% CPU (+200%)
```

**Diagnostic :** Une modification dans `parse_request()` a introduit un algorithme O(n¬≤) au lieu de O(n).

**Code probl√©matique (apr√®s d√©ploiement) :**

```c
// Nouveau code : O(n¬≤)
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        if (items[i] == items[j]) {
            // ... complexit√© quadratique !
        }
    }
}
```

**Solution :** Rollback + fix avec un algorithme O(n) utilisant une hash table.

---

## M√©triques avanc√©es de performance avec eBPF

### 1. **Hardware Performance Counters**

eBPF peut acc√©der aux **compteurs mat√©riels du CPU** (PMU - Performance Monitoring Unit).

#### M√©triques disponibles

| M√©trique | Description | Impact |
|----------|-------------|--------|
| **CPU cycles** | Nombre de cycles CPU | Base pour calculer l'IPC |
| **Instructions retired** | Instructions ex√©cut√©es | Performance r√©elle |
| **Cache misses (L1/L2/L3)** | D√©fauts de cache | Latence m√©moire |
| **Branch mispredictions** | Erreurs de pr√©diction | Pipeline stalls |
| **TLB misses** | D√©fauts TLB | Acc√®s m√©moire lents |
| **Bus cycles** | Cycles pass√©s sur le bus | Contention m√©moire |

#### IPC (Instructions Per Cycle)

L'**IPC** est une m√©trique cl√© pour √©valuer l'efficacit√© du code :

```
IPC = Instructions retired / CPU cycles

IPC > 2.0 : Excellent (code bien optimis√©)  
IPC 1.0-2.0 : Bon  
IPC < 1.0 : Probl√®me de performance (stalls, cache misses)  
```

**Mesure avec perf + eBPF :**

```bash
$ perf stat -e cycles,instructions,cache-misses ./myapp

Performance counter stats for './myapp':

    2,345,678,901      cycles
    4,123,456,789      instructions              #    1.76  insn per cycle
       12,345,678      cache-misses              #    0.53% of all cache refs
```

**Interpr√©tation** : IPC = 1.76 ‚Üí Bon, mais 0.53% de cache misses ‚Üí Possibilit√© d'optimisation.

### 2. **Cache-aware profiling**

Les d√©fauts de cache sont une source majeure de latence.

```bash
# Profiler les cache misses par fonction
$ perf record -e cache-misses -g ./myapp
$ perf report

Overhead  Command  Shared Object     Symbol
   45.23%  myapp    myapp             [.] process_array
   23.45%  myapp    myapp             [.] hash_lookup
```

**Diagnostic :** `process_array()` g√©n√®re 45% des cache misses ‚Üí Probablement un acc√®s m√©moire non continu.

### 3. **Context Switch profiling**

Les changements de contexte (context switches) sont co√ªteux (10-100 ¬µs chacun).

```bash
# Compter les context switches par processus
$ bpftrace -e '
  tracepoint:sched:sched_switch {
    @ctx_switches[comm] = count();
  }
'

@ctx_switches[nginx]: 1234
@ctx_switches[postgres]: 8901  ‚Üê Beaucoup de context switches !
```

**Cause possible** : Trop de threads, contention sur les locks, ou I/O bloquants fr√©quents.

---

## Performance Analysis dans Kubernetes

### 1. **Profiling par pod/namespace**

Avec Parca ou Pyroscope d√©ploy√©s dans Kubernetes, on peut profiler par ressource K8s :

```
Namespace: production  
Pod: backend-abc123  
Container: api-server  

Flamegraph :
  main() ‚Üí 100%
    handle_request() ‚Üí 80%
      database_query() ‚Üí 60%
        wait_on_lock() ‚Üí 55%  ‚Üê Hotspot identifi√© !
```

### 2. **D√©tection de noisy neighbors**

Dans un cluster partag√©, certains pods peuvent monopoliser les ressources.

```bash
# Profiler tous les pods et identifier les "noisy neighbors"
$ runqlat-bpf --interval 1

Pod: ml-training-xyz
  Run queue latency: 5000 ¬µs (5ms) ‚Üê Tr√®s √©lev√© !

Pod: web-frontend-abc
  Run queue latency: 50 ¬µs
```

**Diagnostic :** Le pod `ml-training-xyz` sature le CPU, causant des latences pour les autres pods.

**Solution :** Appliquer des CPU limits ou d√©placer vers un node pool d√©di√©.

### 3. **Analyse de performance de service mesh**

Les service mesh (Istio, Linkerd) ajoutent de la latence. eBPF permet de mesurer exactement combien.

```bash
# Mesurer la latence ajout√©e par Envoy sidecar
$ tcplife-bpf | grep envoy

Pod A ‚Üí Envoy sidecar : 0.5 ms  
Envoy ‚Üí Pod B : 1.2 ms  
Total overhead : 1.7 ms par requ√™te  
```

---

## Avantages du profiling eBPF

### 1. **Profiling en production sans ralentir**

| Aspect | Profiling traditionnel | eBPF Profiling |
|--------|----------------------|---------------|
| **Overhead** | 50-100% | < 1% |
| **Utilisable 24/7** | Non | Oui |
| **Modif code** | Souvent n√©cessaire | Non |
| **Pr√©cision** | √âchantillonnage grossier | Microseconde |
| **Visibilit√© kernel** | Limit√©e | Compl√®te |

### 2. **D√©tection proactive de r√©gressions**

Avec du profiling continu (Parca/Pyroscope), on d√©tecte imm√©diatement les r√©gressions :

```
D√©ploiement v2.3.1 ‚Üí Latence P99 passe de 50ms √† 150ms  
Alerte automatique ‚Üí Rollback imm√©diat  
Analyse du flamegraph ‚Üí R√©gression identifi√©e en 5 minutes  
```

### 3. **R√©duction du MTTR (Mean Time To Resolution)**

**Avant eBPF :**
```
Probl√®me report√© ‚Üí Investigation (2h) ‚Üí Reproduction (1h)
‚Üí Instrumentation (30min) ‚Üí Analyse (1h)
Total MTTR : 4-5 heures
```

**Avec eBPF :**
```
Probl√®me report√© ‚Üí eBPF profiling (2min) ‚Üí Flamegraph (30s)
‚Üí Hotspot identifi√©
Total MTTR : 5-10 minutes
```

**Gain** : **30x plus rapide** pour identifier la cause racine.

---

## Limitations et consid√©rations

### 1. **Symboles de debug requis**

Pour des flamegraphs lisibles, les binaires doivent inclure les symboles de debug.

**Compilation avec symboles :**

```bash
gcc -g program.c -o program  # Inclut symboles
```

**Pour des binaires existants :**

```bash
# Installer les debug symbols (Ubuntu)
sudo apt install myapp-dbgsym
```

### 2. **Overhead (m√™me s'il est faible)**

Bien que < 1%, dans des environnements extr√™mes :
- Applications temps r√©el (< 1 ms deadline)
- High-frequency trading

L'overhead peut √™tre critique. Dans ces cas, profiler en **staging** plut√¥t qu'en production.

### 3. **Courbe d'apprentissage**

Interpr√©ter les flamegraphs et les m√©triques PMU demande de l'exp√©rience.

**Recommandation** : Commencer par des outils simples (profile, biolatency) avant d'aller vers des analyses avanc√©es.

### 4. **Compatibility kernel**

**Requirements :**
- **Minimum** : Linux 4.9+ (eBPF de base)
- **Recommand√©** : Linux 5.4+ (support complet BCC)
- **Id√©al** : Linux 5.15+ (toutes features de profiling)

---

## Le futur du profiling avec eBPF

### 1. **Profiling distribu√©**

Des plateformes comme **Parca** et **Pyroscope** permettent de profiler des **syst√®mes distribu√©s entiers** :
- Tracer une requ√™te √† travers 10 microservices
- Identifier le service qui ajoute de la latence
- Analyser les bottlenecks inter-services

### 2. **IA + Profiling automatique**

L'int√©gration d'IA avec eBPF profiling pour :
- D√©tecter automatiquement les anomalies de performance
- Sugg√©rer des optimisations (ex : "Utilisez un index sur cette colonne")
- Pr√©dire les futurs probl√®mes de performance

### 3. **Profiling de Wasm et containers**

eBPF s'√©tend au profiling de :
- **WebAssembly** (Wasm) dans le kernel
- **Containers** avec isolation compl√®te
- **Serverless** (AWS Lambda, Google Cloud Functions)

---

## Ressources pour approfondir

### Outils open-source

| Outil | Description | URL |
|-------|-------------|-----|
| **BCC** | Collection d'outils de profiling | https://github.com/iovisor/bcc |
| **bpftrace** | Langage de scripting | https://github.com/iovisor/bpftrace |
| **Parca** | Profiling continu | https://www.parca.dev |
| **Pyroscope** | Profiling platform | https://pyroscope.io |
| **FlameGraph** | Visualisation | https://github.com/brendangregg/FlameGraph |

### Livres

- *"BPF Performance Tools"* de Brendan Gregg (LA bible du profiling eBPF)
- *"Systems Performance"* de Brendan Gregg (m√©thodologie d'analyse)

### Documentation

- **BCC Tutorial** : https://github.com/iovisor/bcc/blob/master/docs/tutorial.md
- **bpftrace Guide** : https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md
- **Brendan Gregg's Blog** : http://www.brendangregg.com/blog/

### Conf√©rences

- **Performance Summit** : Talks d√©di√©s au profiling eBPF
- **Linux Plumbers Conference** : Track performance analysis
- **FOSDEM** : Pr√©sentations sur les outils eBPF

---

## Conclusion

Le **profiling avec eBPF** repr√©sente une r√©volution dans l'analyse de performance des syst√®mes Linux. En tant qu'ing√©nieur DevOps ou d√©veloppeur, ma√Ætriser eBPF profiling vous permet de :

- ‚úÖ **Profiler en production 24/7** avec < 1% d'overhead
- ‚úÖ **Identifier les hotspots en quelques minutes** au lieu de plusieurs heures
- ‚úÖ **D√©tecter les r√©gressions imm√©diatement** apr√®s un d√©ploiement
- ‚úÖ **Analyser des m√©triques impossibles** auparavant (off-CPU, cache misses, etc.)
- ‚úÖ **R√©duire le MTTR de 30x** pour les probl√®mes de performance
- ‚úÖ **Optimiser sans deviner** : donn√©es pr√©cises au lieu d'intuitions

eBPF profiling est d√©j√† utilis√© en production par :
- **Netflix** : Profiling continu de millions de serveurs
- **Facebook/Meta** : Optimisation de performance √† l'√©chelle
- **Google** : Profiling dans les data centers
- **Uber, Shopify, GitLab** : Analyse de performance Kubernetes

La combinaison d'eBPF et de flamegraphs est devenue l'**outil standard** pour l'analyse de performance moderne. Si vous ne ma√Ætrisez qu'un seul outil de profiling, ce doit √™tre eBPF.

Avec les quatre sections sur eBPF (21.3.1 √† 21.3.4), vous avez maintenant une vue compl√®te de comment eBPF r√©volutionne l'observabilit√© Linux dans quatre domaines critiques : tracing/monitoring, networking, s√©curit√©, et performance analysis.

---

**üí° √Ä retenir :**
- eBPF permet du profiling continu en production avec < 1% overhead
- Les flamegraphs visualisent intuitivement o√π passe le temps CPU
- Off-CPU profiling r√©v√®le les temps d'attente invisibles au CPU profiling
- Des outils comme BCC et bpftrace rendent le profiling accessible
- Le profiling eBPF r√©duit le MTTR de plusieurs heures √† quelques minutes
- C'est devenu l'√©tat de l'art pour l'analyse de performance moderne

---

‚è≠Ô∏è [Toolchain eBPF](/21-introduction-ebpf/04-toolchain-ebpf.md)
