üîù Retour au [Sommaire](/SOMMAIRE.md)

# 21.3.1 Tracing et Monitoring avec eBPF

## Introduction

Le **tracing** (tra√ßage) et le **monitoring** (surveillance) sont deux activit√©s essentielles pour comprendre ce qui se passe r√©ellement dans un syst√®me Linux en production. Imaginez que votre serveur ralentit myst√©rieusement, qu'une application consomme trop de m√©moire, ou qu'un processus g√©n√®re des erreurs intermittentes. Comment identifier la cause sans perturber le syst√®me ?

C'est pr√©cis√©ment l√† qu'eBPF r√©volutionne l'observabilit√© des syst√®mes Linux modernes.

---

## Pourquoi eBPF est r√©volutionnaire pour l'observabilit√©

### Les limitations des approches traditionnelles

Avant eBPF, pour observer le comportement d'un syst√®me Linux, les ing√©nieurs utilisaient principalement :

**1. Les outils classiques (top, ps, netstat, iostat)**
- ‚úÖ Simples et disponibles partout
- ‚ùå Visions statiques et instantan√©es
- ‚ùå Informations agr√©g√©es sans d√©tails fins
- ‚ùå Impossible de r√©pondre √† "Pourquoi ce processus a appel√© open() 10 000 fois ?"

**2. Les logs applicatifs**
- ‚úÖ Utiles pour le debugging applicatif
- ‚ùå N√©cessitent d'√™tre ajout√©s au code source
- ‚ùå Overhead de performance significatif
- ‚ùå Invisibles au niveau du kernel

**3. strace (System Call Tracer)**
- ‚úÖ Permet de voir tous les appels syst√®me
- ‚ùå **Extr√™mement lent** : peut ralentir un processus de 100x
- ‚ùå Inutilisable en production sur des charges r√©elles

**4. Les kernel modules personnalis√©s**
- ‚úÖ Acc√®s complet au kernel
- ‚ùå Risque de crasher tout le syst√®me
- ‚ùå N√©cessite de recompiler et red√©marrer le kernel
- ‚ùå Difficile √† maintenir entre versions

### Ce qu'apporte eBPF

eBPF √©limine ces limitations en offrant :

| Caract√©ristique | Impact |
|-----------------|--------|
| **Performance** | Overhead minimal (< 1% dans la plupart des cas) |
| **S√©curit√©** | Le verifier emp√™che les crashs kernel |
| **Dynamique** | Attachement/d√©tachement √† chaud, sans reboot |
| **Granularit√©** | Acc√®s aux √©v√©nements au niveau microseconde |
| **Portabilit√©** | Fonctionne sur toutes les distributions r√©centes |

> **En r√©sum√©** : eBPF vous permet d'observer en temps r√©el, avec une pr√©cision chirurgicale, ce qui se passe dans le kernel et les applications, **sans ralentir le syst√®me** et **sans risquer de le crasher**.

---

## Les cas d'usage du tracing avec eBPF

### 1. **Tracing des appels syst√®me (syscalls)**

Les appels syst√®me sont la porte d'entr√©e entre l'espace utilisateur et le kernel. Tracer ces appels permet de comprendre exactement ce qu'une application demande au syst√®me.

#### Exemple concret : D√©boguer un programme qui √©choue

Imaginons qu'une application plante avec un myst√©rieux "Permission denied". Avec eBPF, vous pouvez :

```
# Tracer tous les appels open() pour le processus PID 1234
$ bpftrace -e 'tracepoint:syscalls:sys_enter_openat /pid == 1234/ {
    printf("%s tried to open: %s\n", comm, str(args->filename));
}'
```

**Sortie typique :**
```
myapp tried to open: /etc/config.json
myapp tried to open: /var/run/myapp.pid
myapp tried to open: /proc/self/limits
```

Vous d√©couvrez instantan√©ment que l'application essaie d'acc√©der √† `/var/run/myapp.pid` sans avoir les permissions. Probl√®me r√©solu en 30 secondes.

#### Ce que vous pouvez tracer

- **Fichiers** : `open`, `read`, `write`, `close`, `stat`
- **R√©seau** : `socket`, `connect`, `send`, `recv`
- **Processus** : `fork`, `execve`, `exit`
- **M√©moire** : `mmap`, `munmap`, `brk`
- **Tous les 300+ syscalls Linux**

### 2. **Tracing des fonctions kernel**

Vous pouvez aller encore plus loin et tracer des **fonctions internes du kernel** qui ne sont pas expos√©es via des syscalls.

#### Exemple : Comprendre le comportement du scheduler

```
# Tracer combien de fois chaque processus est schedul√©
$ bpftrace -e 'kprobe:finish_task_switch {
    @switches[comm] = count();
}'
```

**R√©sultat :**
```
@switches[systemd]: 142
@switches[nginx]: 3421
@switches[postgres]: 8763
```

Cela vous montre que PostgreSQL est en train de mobiliser massivement le CPU.

### 3. **Tracing des √©v√©nements r√©seau**

eBPF excelle dans l'observation du trafic r√©seau, que ce soit pour :
- Analyser la latence des connexions TCP
- D√©tecter les retransmissions (signe de probl√®mes r√©seau)
- Observer les flux de paquets en temps r√©el

#### Exemple : Mesurer la latence TCP par connexion

```
# Mesurer le temps entre SYN et ACK (handshake TCP)
$ tcplife-bpf
PID   COMM       LADDR           LPORT RADDR           RPORT TX_KB RX_KB MS
1234  nginx      192.168.1.10    80    203.0.113.5     45231 12    156   2.3
5678  postgres   127.0.0.1       5432  127.0.0.1       52341 0     4     0.1
```

Vous voyez imm√©diatement que la connexion nginx a une latence de 2,3 ms, ce qui peut indiquer un probl√®me r√©seau.

### 4. **Profiling de performance en production**

L'un des usages les plus puissants d'eBPF est le **profiling continu** sans impact sur les performances.

#### Flame Graphs en temps r√©el

Avec des outils comme `profile-bpf`, vous pouvez g√©n√©rer des **flame graphs** (graphiques de flamme) pour identifier les fonctions qui consomment le plus de CPU :

```
# Profiler l'ensemble du syst√®me pendant 30 secondes
$ profile-bpf -F 99 -ag 30 > profile.txt
$ flamegraph.pl profile.txt > flamegraph.svg
```

Cela produit une visualisation qui montre instantan√©ment :
- Quelles fonctions consomment du CPU
- Les cha√Ænes d'appels (call stacks)
- Les hotspots de performance

**Cas r√©el** : Netflix utilise eBPF pour profiler en continu des millions de serveurs en production, ce qui leur a permis d'identifier des optimisations donnant **30% de gain de performance**.

---

## Les types de tracing disponibles avec eBPF

eBPF offre plusieurs "points d'attache" (hook points) pour intercepter des √©v√©nements :

### 1. **Tracepoints** (Points de tra√ßage statiques)

Les tracepoints sont des points d'instrumentation **d√©finis dans le code kernel** par les d√©veloppeurs Linux.

**Avantages :**
- ‚úÖ API stable : garantis de ne pas changer entre versions du kernel
- ‚úÖ Bien document√©s
- ‚úÖ Optimis√©s pour la performance

**Exemple d'utilisation :**
```
tracepoint:syscalls:sys_enter_write  // Tous les appels write()
tracepoint:sched:sched_switch        // Changements de contexte
tracepoint:net:net_dev_xmit          // Envoi de paquets r√©seau
```

**Liste disponible :**
```bash
$ ls /sys/kernel/debug/tracing/events/
```

### 2. **Kprobes** (Kernel Probes dynamiques)

Les kprobes permettent d'attacher des programmes eBPF √† **n'importe quelle fonction kernel**, m√™me si elle n'a pas de tracepoint.

**Avantages :**
- ‚úÖ Flexibilit√© totale
- ‚úÖ Acc√®s √† des fonctions internes

**Inconv√©nients :**
- ‚ö†Ô∏è API instable : les noms de fonctions peuvent changer
- ‚ö†Ô∏è N√©cessite de conna√Ætre les internals du kernel

**Exemple :**
```
kprobe:tcp_sendmsg    // Intercepter l'envoi TCP
kprobe:vfs_read       // Intercepter toutes les lectures VFS
```

### 3. **Uprobes** (User-space Probes)

Les uprobes permettent de tracer des **fonctions dans les applications utilisateur** sans modifier leur code source.

**Cas d'usage :**
- Tracer les fonctions d'une biblioth√®que (ex : OpenSSL)
- Observer le comportement d'un binaire propri√©taire
- Mesurer le temps pass√© dans des fonctions critiques

**Exemple : Tracer malloc() dans libc**
```
uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc {
    printf("malloc called with size: %d\n", arg0);
}
```

### 4. **USDT** (User-level Statically Defined Tracing)

Certaines applications modernes embarquent des points de tra√ßage USDT (aussi appel√©s "probes DTrace") pour faciliter l'observabilit√©.

**Exemples d'applications avec USDT :**
- **PostgreSQL** : requ√™tes SQL, transactions
- **MySQL** : connexions, requ√™tes
- **Python** : garbage collector, fonctions
- **Node.js** : √©v√©nements asynchrones

---

## Les outils populaires bas√©s sur eBPF

### 1. **bpftrace** : Le couteau suisse du tracing

`bpftrace` est un langage de script de haut niveau pour eBPF, inspir√© de DTrace et awk.

**Philosophie :** One-liners puissants pour du tracing ad-hoc.

**Exemple d'utilisation :**
```bash
# Compter les appels syst√®me par processus
$ bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }'

# Histogramme de la latence d'√©criture sur disque
$ bpftrace -e 'kprobe:vfs_write { @start[tid] = nsecs; }
               kretprobe:vfs_write /@start[tid]/ {
                   @us = hist((nsecs - @start[tid]) / 1000);
                   delete(@start[tid]);
               }'
```

### 2. **BCC (BPF Compiler Collection)**

BCC est une collection d'**outils pr√™ts √† l'emploi** pour des t√¢ches courantes.

**Quelques outils populaires :**

| Outil | Description |
|-------|-------------|
| `execsnoop` | Tracer tous les nouveaux processus cr√©√©s |
| `opensnoop` | Voir tous les fichiers ouverts |
| `tcpconnect` | Observer les nouvelles connexions TCP |
| `biolatency` | Latence des I/O disque |
| `runqlat` | Temps d'attente dans la file du scheduler |
| `profile` | Profiler le CPU (flamegraphs) |

**Exemple d'utilisation :**
```bash
# Voir toutes les nouvelles connexions TCP sortantes
$ tcpconnect
PID    COMM         IP SADDR            DADDR            DPORT
1234   curl         4  192.168.1.10     93.184.216.34    80
5678   ssh          4  192.168.1.10     203.0.113.50     22
```

### 3. **Cilium** : Networking et s√©curit√© Kubernetes

Cilium utilise eBPF pour remplacer iptables dans Kubernetes, offrant :
- Routage r√©seau ultra-rapide
- Policies de s√©curit√© L3-L7
- Observabilit√© r√©seau native

### 4. **Pixie** : Observabilit√© automatique

Pixie s'installe dans un cluster Kubernetes et capture **automatiquement** :
- Toutes les requ√™tes HTTP/gRPC
- Les requ√™tes SQL vers les bases de donn√©es
- Les m√©triques syst√®me (CPU, m√©moire, r√©seau)

Le tout sans modifier une seule ligne de code applicatif.

---

## Cas d'usage DevOps r√©els

### Sc√©nario 1 : "Mon API est lente, mais pourquoi ?"

**Probl√®me :** Une API REST r√©pond en 2 secondes au lieu de 200 ms.

**Solution avec eBPF :**
```bash
# 1. V√©rifier si c'est un probl√®me de base de donn√©es
$ tcplife-bpf | grep postgres
# R√©sultat : Connexions PostgreSQL prenant 1,8s

# 2. Tracer les requ√™tes SQL lentes (avec USDT)
$ bpftrace -e 'usdt:/usr/lib/postgresql/bin/postgres:query__start {
    @start[arg0] = nsecs;
}
usdt:/usr/lib/postgresql/bin/postgres:query__done {
    $dur = (nsecs - @start[arg0]) / 1000000;
    if ($dur > 100) {
        printf("Slow query: %d ms\n", $dur);
    }
}'
```

**Diagnostic :** Les requ√™tes SQL sont effectivement lentes. Investigation c√¥t√© base de donn√©es n√©cessaire.

### Sc√©nario 2 : "Qui √©crit massivement sur le disque ?"

**Probl√®me :** Les I/O disque saturent √† 100%, mais impossible de savoir quel processus est responsable.

**Solution avec eBPF :**
```bash
$ biotop-bpf
PID    COMM             DISK    READS  WRITES  R_Kb   W_Kb
8234   postgres         sda     142    8421    1456   521847
1234   docker           sda     23     12      234    128
```

**Diagnostic :** PostgreSQL √©crit 500 Mo/s sur le disque. V√©rifier les logs ou les requ√™tes d'√©criture massive.

### Sc√©nario 3 : "Mon conteneur Docker crash al√©atoirement"

**Probl√®me :** Un conteneur se termine avec le code de sortie 137 (SIGKILL).

**Solution avec eBPF :**
```bash
# Tracer tous les signaux envoy√©s
$ killsnoop-bpf
TIME     PID    COMM             SIG  TPID   RESULT
12:34:56 1      systemd          9    8234   0
```

**Diagnostic :** Le processus PID 1 (systemd ou init du conteneur) envoie SIGKILL √† votre application. Probablement un OOM (Out Of Memory) Kill. V√©rifier les limites m√©moire du conteneur.

---

## Avantages d'eBPF pour le monitoring

### 1. **Observabilit√© sans modification de code**

Vous n'avez **pas besoin de recompiler** vos applications ou d'ajouter des biblioth√®ques d'instrumentation. eBPF observe depuis le kernel.

### 2. **Performance en production**

Contrairement √† `strace` ou aux logs verbeux, eBPF a un overhead **n√©gligeable** :
- Typiquement < 1% de CPU
- Peut √™tre laiss√© actif en continu

### 3. **Granularit√© extr√™me**

eBPF capture des √©v√©nements au niveau **nanoseconde**, ce qui permet d'identifier des probl√®mes de performance subtils.

### 4. **Vue unifi√©e syst√®me + application**

eBPF peut corr√©ler :
- Les √©v√©nements kernel (syscalls, r√©seau, disque)
- Les √©v√©nements applicatifs (USDT)
- Les m√©triques hardware (caches CPU, branch mispredictions)

Tout cela dans un **seul outil**.

---

## Comparaison : Avant et apr√®s eBPF

| T√¢che | Avant eBPF | Avec eBPF |
|-------|------------|-----------|
| **Tracer les syscalls** | `strace` (100x plus lent) | `bpftrace` (< 1% overhead) |
| **Profiler le CPU** | `perf` (√©chantillonnage) | `profile-bpf` (continu) |
| **Latence r√©seau** | `tcpdump` + analyse manuelle | `tcplife`, `tcptracer` (temps r√©el) |
| **Debugging kernel** | Kernel module (risqu√©) | eBPF (s√ªr, dynamique) |
| **Observabilit√© K8s** | Metrics + Logs (limit√©s) | Pixie, Cilium (complet) |

---

## Limites et consid√©rations

### 1. **Version du kernel**

eBPF n√©cessite un kernel r√©cent :
- **Minimum recommand√©** : Linux 4.9+
- **Id√©al** : Linux 5.x+ (support complet des features)
- **Ubuntu 20.04+** ou **Debian 11+** sont parfaits

### 2. **Courbe d'apprentissage**

Bien que des outils comme BCC simplifient l'usage, comprendre :
- Les hook points (tracepoints, kprobes)
- Les structures de donn√©es kernel
- Le langage bpftrace

demande un investissement initial.

### 3. **Permissions requises**

eBPF n√©cessite des **privil√®ges √©lev√©s** :
- Soit `CAP_BPF` + `CAP_PERFMON` (kernel 5.8+)
- Soit `root` (kernels plus anciens)

---

## Ressources pour aller plus loin

### Documentation officielle
- **Documentation eBPF** : https://ebpf.io/
- **BCC Tools** : https://github.com/iovisor/bcc
- **bpftrace** : https://github.com/iovisor/bpftrace

### Livres recommand√©s
- *"BPF Performance Tools"* de Brendan Gregg (bible de l'observabilit√© eBPF)
- *"Learning eBPF"* de Liz Rice (introduction claire et p√©dagogique)

### Talks et conf√©rences
- Linux Plumbers Conference (track eBPF annuel)
- eBPF Summit (conf√©rence d√©di√©e)

### Outils d'apprentissage
- **eBPF Playground** : Tester eBPF dans le navigateur
- **Katacoda eBPF tutorials** : Tutoriels interactifs

---

## Conclusion

Le **tracing et monitoring avec eBPF** repr√©sentent une r√©volution dans l'observabilit√© des syst√®mes Linux. En tant qu'ing√©nieur DevOps ou SysAdmin, ma√Ætriser eBPF vous donne un **superpouvoir** :

- ‚úÖ D√©boguer des probl√®mes de performance en production sans ralentir le syst√®me
- ‚úÖ Comprendre exactement ce qui se passe au niveau kernel
- ‚úÖ Anticiper les incidents gr√¢ce au monitoring continu
- ‚úÖ Optimiser les applications en identifiant les bottlenecks r√©els

eBPF n'est plus une technologie exp√©rimentale : elle est utilis√©e en production par **Netflix, Google, Facebook, et des milliers d'entreprises** pour observer des millions de serveurs chaque jour.

Dans la section suivante (21.3.2), nous explorerons comment eBPF transforme √©galement le **networking** et remplace des technologies comme iptables dans les environnements cloud-native.

---

**üí° √Ä retenir :**
- eBPF permet d'observer le syst√®me avec une pr√©cision microseconde
- L'overhead est n√©gligeable (< 1% CPU)
- Les outils comme `bpftrace` et BCC rendent eBPF accessible
- Le tracing eBPF fonctionne sans modifier le code source des applications
- C'est devenu l'outil standard pour l'observabilit√© moderne sous Linux

---

‚è≠Ô∏è [Networking](/21-introduction-ebpf/03.2-networking.md)
