üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.8 Serveur Concurrent

## Introduction

Jusqu'√† pr√©sent, nous avons cr√©√© des **serveurs it√©ratifs** : ils ne peuvent traiter qu'**un seul client √† la fois**. Si un client est lent ou reste connect√© longtemps, tous les autres clients doivent attendre leur tour.

Pour les applications r√©elles, ce n'est pas acceptable. Un vrai serveur doit pouvoir g√©rer **plusieurs clients simultan√©ment**. C'est ce qu'on appelle un **serveur concurrent**.

Dans cette section, nous allons explorer les trois principales approches pour cr√©er des serveurs concurrents en C :
1. **Multi-processus** (fork)
2. **Multi-threads** (pthreads)
3. **I/O Multiplexing** (select, poll, epoll)

**Objectif :** Comprendre les avantages, inconv√©nients et cas d'usage de chaque approche.

---

## Le Probl√®me du Serveur It√©ratif

### Rappel : Serveur it√©ratif simple

```c
while (1) {
    client_fd = accept(server_fd, ...);        // Attendre un client
    handle_client(client_fd);                  // Traiter le client
    close(client_fd);                          // Fermer
    // Seulement maintenant, accept() le client suivant
}
```

### Limitations critiques

‚ùå **Un seul client √† la fois** : Les autres clients sont bloqu√©s dans la file d'attente

‚ùå **Client lent = serveur bloqu√©** : Si un client prend 10 secondes, tous les autres attendent 10 secondes

‚ùå **Mauvaise utilisation des ressources** : Le CPU reste inactif pendant les I/O

‚ùå **Exp√©rience utilisateur d√©grad√©e** : Timeouts fr√©quents, latence √©lev√©e

### Exemple du probl√®me

```
Client 1 : Se connecte √† 12:00:00, trait√© pendant 5 secondes
Client 2 : Se connecte √† 12:00:01, ATTEND jusqu'√† 12:00:05 !
Client 3 : Se connecte √† 12:00:02, ATTEND jusqu'√† 12:00:10 !
```

**Inacceptable pour un serveur en production !**

---

## Les Solutions : Vue d'Ensemble

| Approche | Principe | Avantages | Inconv√©nients |
|----------|----------|-----------|---------------|
| **Multi-processus** | Un processus fils par client | Isolation forte, simple | Co√ªt m√©moire √©lev√©, communication complexe |
| **Multi-threads** | Un thread par client | L√©ger, partage m√©moire | Synchronisation complexe, risque de bugs |
| **I/O Multiplexing** | Un seul processus/thread | Tr√®s efficace, scalable | Complexit√© du code, architecture diff√©rente |

**Il n'y a pas de solution universelle** : le choix d√©pend de votre application.

---

## Approche 1 : Multi-Processus avec `fork()`

### Concept

Pour chaque client qui se connecte, cr√©er un **processus fils** d√©di√© qui g√®re ce client.

```
Processus Parent (serveur)
    ‚îú‚îÄ accept() client 1 ‚Üí fork() ‚Üí Processus Fils 1 (g√®re client 1)
    ‚îú‚îÄ accept() client 2 ‚Üí fork() ‚Üí Processus Fils 2 (g√®re client 2)
    ‚îî‚îÄ accept() client 3 ‚Üí fork() ‚Üí Processus Fils 3 (g√®re client 3)
```

### La fonction `fork()`

```c
#include <unistd.h>

pid_t fork(void);
```

**Fonctionnement :**
- Cr√©e une **copie** du processus actuel
- **Parent** : `fork()` retourne le PID du fils
- **Fils** : `fork()` retourne 0
- **Erreur** : `fork()` retourne -1

**Apr√®s `fork()` :**
- Les deux processus (parent et fils) s'ex√©cutent **en parall√®le**
- Le fils h√©rite des descripteurs de fichiers ouverts (dont les sockets)
- Chaque processus a sa propre m√©moire (copie)

### Sch√©ma de fonctionnement

```c
pid_t pid = fork();

if (pid < 0) {
    // Erreur
    perror("fork");
} else if (pid == 0) {
    // Code du FILS
    printf("Je suis le fils, PID = %d\n", getpid());
} else {
    // Code du PARENT
    printf("Je suis le parent, mon fils a le PID = %d\n", pid);
}
```

### Exemple Complet : Serveur Multi-Processus

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <signal.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define PORT 8080
#define BUFFER_SIZE 1024

// Gestionnaire pour nettoyer les processus zombies
void sigchld_handler(int sig) {
    (void)sig;
    // Attendre tous les fils termin√©s (sans bloquer)
    while (waitpid(-1, NULL, WNOHANG) > 0);
}

void handle_client(int client_fd) {
    char buffer[BUFFER_SIZE];
    ssize_t bytes_received;

    printf("[PID %d] Client connect√©\n", getpid());

    while ((bytes_received = recv(client_fd, buffer, BUFFER_SIZE - 1, 0)) > 0) {
        buffer[bytes_received] = '\0';
        printf("[PID %d] Re√ßu : %s", getpid(), buffer);

        // √âcho
        if (send(client_fd, buffer, bytes_received, 0) < 0) {
            perror("send");
            break;
        }
    }

    if (bytes_received == 0) {
        printf("[PID %d] Client d√©connect√©\n", getpid());
    } else {
        perror("recv");
    }

    close(client_fd);
}

int main() {
    int server_fd, client_fd;
    struct sockaddr_in server_addr, client_addr;
    socklen_t client_len;

    // Installer le gestionnaire de signal pour SIGCHLD
    struct sigaction sa;
    sa.sa_handler = sigchld_handler;
    sigemptyset(&sa.sa_mask);
    sa.sa_flags = SA_RESTART;  // Red√©marrer accept() si interrompu
    if (sigaction(SIGCHLD, &sa, NULL) == -1) {
        perror("sigaction");
        exit(EXIT_FAILURE);
    }

    // Cr√©er le socket serveur
    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd < 0) {
        perror("socket");
        exit(EXIT_FAILURE);
    }

    // SO_REUSEADDR
    int opt = 1;
    setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // Bind
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = INADDR_ANY;
    server_addr.sin_port = htons(PORT);

    if (bind(server_fd, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0) {
        perror("bind");
        close(server_fd);
        exit(EXIT_FAILURE);
    }

    // Listen
    if (listen(server_fd, 10) < 0) {
        perror("listen");
        close(server_fd);
        exit(EXIT_FAILURE);
    }

    printf("Serveur multi-processus en √©coute sur le port %d\n", PORT);

    // Boucle principale
    while (1) {
        client_len = sizeof(client_addr);
        client_fd = accept(server_fd, (struct sockaddr*)&client_addr, &client_len);

        if (client_fd < 0) {
            perror("accept");
            continue;
        }

        // Afficher l'IP du client
        char client_ip[INET_ADDRSTRLEN];
        inet_ntop(AF_INET, &client_addr.sin_addr, client_ip, INET_ADDRSTRLEN);
        printf("Nouvelle connexion de %s:%d\n", client_ip, ntohs(client_addr.sin_port));

        // Cr√©er un processus fils pour ce client
        pid_t pid = fork();

        if (pid < 0) {
            perror("fork");
            close(client_fd);
            continue;
        }

        if (pid == 0) {
            // PROCESSUS FILS
            // Le fils n'a pas besoin du socket serveur
            close(server_fd);

            // G√©rer le client
            handle_client(client_fd);

            // Terminer le processus fils
            exit(EXIT_SUCCESS);
        } else {
            // PROCESSUS PARENT
            // Le parent n'a pas besoin du socket client (le fils s'en occupe)
            close(client_fd);

            // Le parent continue imm√©diatement √† accept() d'autres clients
        }
    }

    close(server_fd);
    return 0;
}
```

**Compilation et test :**
```bash
gcc -o server_fork server_fork.c -Wall -Wextra
./server_fork
```

**Tester avec plusieurs clients :**
```bash
# Terminal 1
telnet localhost 8080

# Terminal 2
telnet localhost 8080

# Terminal 3
telnet localhost 8080
```

Tous les clients sont servis **simultan√©ment** !

---

### Gestion des Processus Zombies

#### Probl√®me

Quand un processus fils termine, il devient un **zombie** jusqu'√† ce que le parent appelle `wait()` ou `waitpid()`.

**Cons√©quence :** Accumulation de zombies, √©puisement de la table des processus.

#### Solution 1 : Signal SIGCHLD

```c
void sigchld_handler(int sig) {
    (void)sig;
    // Nettoyer tous les fils termin√©s
    while (waitpid(-1, NULL, WNOHANG) > 0);
}

// Dans main()
signal(SIGCHLD, sigchld_handler);
// ou mieux :
struct sigaction sa;
sa.sa_handler = sigchld_handler;
sigemptyset(&sa.sa_mask);
sa.sa_flags = SA_RESTART;
sigaction(SIGCHLD, &sa, NULL);
```

#### Solution 2 : Ignorer SIGCHLD (Linux)

```c
signal(SIGCHLD, SIG_IGN);
```

Sur Linux, cela demande au kernel de nettoyer automatiquement les zombies.

---

### Avantages du Multi-Processus

‚úÖ **Isolation forte** : Chaque client dans son propre espace m√©moire

‚úÖ **S√©curit√©** : Un crash d'un processus fils n'affecte pas les autres

‚úÖ **Simplicit√©** : Pas de synchronisation √† g√©rer

‚úÖ **Robustesse** : √âchec d'un client = seulement ce processus meurt

‚úÖ **Debugging facile** : Chaque processus peut √™tre debugg√© ind√©pendamment

### Inconv√©nients du Multi-Processus

‚ùå **Co√ªt m√©moire √©lev√©** : Chaque processus duplique la m√©moire

‚ùå **Co√ªt de cr√©ation** : `fork()` est relativement lent

‚ùå **Scalabilit√© limit√©e** : Difficile de g√©rer 10 000+ clients

‚ùå **Communication complexe** : IPC n√©cessaire pour communiquer entre processus

‚ùå **Contexte switching** : Changements de contexte co√ªteux avec beaucoup de processus

### Quand utiliser Multi-Processus ?

‚úÖ Applications avec peu de clients simultan√©s (< 100)

‚úÖ N√©cessit√© d'isolation forte (s√©curit√©)

‚úÖ Clients avec des traitements longs et ind√©pendants

‚úÖ Simplification du code (pas de threads)

---

## Approche 2 : Multi-Threads avec `pthreads`

### Concept

Au lieu de cr√©er des processus, cr√©er des **threads** l√©gers qui partagent le m√™me espace m√©moire.

```
Processus Serveur
    ‚îú‚îÄ Thread Principal (accept des clients)
    ‚îú‚îÄ Thread 1 (g√®re client 1)
    ‚îú‚îÄ Thread 2 (g√®re client 2)
    ‚îî‚îÄ Thread 3 (g√®re client 3)
```

### Avantages des Threads vs Processus

| Caract√©ristique | Processus | Thread |
|-----------------|-----------|--------|
| **M√©moire** | S√©par√©e (copie) | Partag√©e |
| **Co√ªt cr√©ation** | √âlev√© (~1-2 ms) | Faible (~10-100 ¬µs) |
| **Communication** | IPC n√©cessaire | Variables partag√©es |
| **Isolation** | Forte | Aucune |
| **Scalabilit√©** | Limit√©e | Meilleure |

### API pthread Essentielle

```c
#include <pthread.h>

// Cr√©er un thread
int pthread_create(pthread_t *thread, const pthread_attr_t *attr,
                   void *(*start_routine)(void*), void *arg);

// Attendre qu'un thread se termine
int pthread_join(pthread_t thread, void **retval);

// Se d√©tacher (pas besoin de join)
int pthread_detach(pthread_t thread);

// Terminer le thread courant
void pthread_exit(void *retval);
```

---

### Exemple Complet : Serveur Multi-Threads

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <pthread.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define PORT 8080
#define BUFFER_SIZE 1024

// Structure pour passer des arguments au thread
typedef struct {
    int client_fd;
    struct sockaddr_in client_addr;
} client_info_t;

// Fonction ex√©cut√©e par chaque thread
void* handle_client_thread(void *arg) {
    client_info_t *info = (client_info_t*)arg;
    int client_fd = info->client_fd;
    char buffer[BUFFER_SIZE];
    ssize_t bytes_received;

    // Afficher l'IP du client
    char client_ip[INET_ADDRSTRLEN];
    inet_ntop(AF_INET, &info->client_addr.sin_addr, client_ip, INET_ADDRSTRLEN);
    printf("[Thread %lu] Client connect√© : %s:%d\n",
           pthread_self(), client_ip, ntohs(info->client_addr.sin_port));

    // Lib√©rer la m√©moire allou√©e pour info
    free(info);

    // G√©rer le client (√©cho)
    while ((bytes_received = recv(client_fd, buffer, BUFFER_SIZE - 1, 0)) > 0) {
        buffer[bytes_received] = '\0';
        printf("[Thread %lu] Re√ßu : %s", pthread_self(), buffer);

        if (send(client_fd, buffer, bytes_received, 0) < 0) {
            perror("send");
            break;
        }
    }

    if (bytes_received == 0) {
        printf("[Thread %lu] Client d√©connect√©\n", pthread_self());
    } else {
        perror("recv");
    }

    close(client_fd);
    return NULL;
}

int main() {
    int server_fd, client_fd;
    struct sockaddr_in server_addr, client_addr;
    socklen_t client_len;
    pthread_t thread;

    // Cr√©er le socket serveur
    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd < 0) {
        perror("socket");
        exit(EXIT_FAILURE);
    }

    // SO_REUSEADDR
    int opt = 1;
    setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // Bind
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = INADDR_ANY;
    server_addr.sin_port = htons(PORT);

    if (bind(server_fd, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0) {
        perror("bind");
        close(server_fd);
        exit(EXIT_FAILURE);
    }

    // Listen
    if (listen(server_fd, 10) < 0) {
        perror("listen");
        close(server_fd);
        exit(EXIT_FAILURE);
    }

    printf("Serveur multi-threads en √©coute sur le port %d\n", PORT);

    // Boucle principale
    while (1) {
        client_len = sizeof(client_addr);
        client_fd = accept(server_fd, (struct sockaddr*)&client_addr, &client_len);

        if (client_fd < 0) {
            perror("accept");
            continue;
        }

        // Allouer une structure pour passer les infos au thread
        client_info_t *info = malloc(sizeof(client_info_t));
        if (info == NULL) {
            perror("malloc");
            close(client_fd);
            continue;
        }

        info->client_fd = client_fd;
        info->client_addr = client_addr;

        // Cr√©er un thread pour ce client
        if (pthread_create(&thread, NULL, handle_client_thread, info) != 0) {
            perror("pthread_create");
            free(info);
            close(client_fd);
            continue;
        }

        // D√©tacher le thread (pas besoin de join)
        pthread_detach(thread);
    }

    close(server_fd);
    return 0;
}
```

**Compilation :**
```bash
gcc -o server_pthread server_pthread.c -Wall -Wextra -pthread
#                                                       ^^^^^^^^ Important !
```

---

### Thread D√©tach√© vs Joinable

#### Thread Joinable (d√©faut)

```c
pthread_t thread;
pthread_create(&thread, NULL, func, arg);

// Le thread principal doit attendre
pthread_join(thread, NULL);
```

**Probl√®me :** Si on ne fait pas `join()`, fuite de ressources.

#### Thread D√©tach√© (recommand√© pour serveurs)

```c
pthread_t thread;
pthread_create(&thread, NULL, func, arg);

// Le thread se nettoie automatiquement √† la fin
pthread_detach(thread);
```

**Avantage :** Pas besoin de tracker les threads, nettoyage automatique.

---

### Synchronisation : Le Probl√®me du Partage de M√©moire

#### Exemple de Race Condition

```c
int client_count = 0;  // Variable globale partag√©e

void* handle_client_thread(void *arg) {
    client_count++;  // ‚ö†Ô∏è RACE CONDITION !
    printf("Clients : %d\n", client_count);

    // ... g√©rer le client ...

    client_count--;  // ‚ö†Ô∏è RACE CONDITION !
    return NULL;
}
```

**Probl√®me :** Deux threads peuvent modifier `client_count` en m√™me temps.

```
Thread 1 lit client_count = 5
Thread 2 lit client_count = 5    (avant que Thread 1 √©crive)
Thread 1 √©crit client_count = 6
Thread 2 √©crit client_count = 6  (‚ùå devrait √™tre 7 !)
```

#### Solution : Mutex

```c
#include <pthread.h>

pthread_mutex_t count_mutex = PTHREAD_MUTEX_INITIALIZER;
int client_count = 0;

void* handle_client_thread(void *arg) {
    // Verrouiller
    pthread_mutex_lock(&count_mutex);
    client_count++;
    int count = client_count;
    pthread_mutex_unlock(&count_mutex);
    // D√©verrouiller

    printf("Clients : %d\n", count);

    // ... g√©rer le client ...

    pthread_mutex_lock(&count_mutex);
    client_count--;
    pthread_mutex_unlock(&count_mutex);

    return NULL;
}
```

**R√®gle d'or :** Toute variable partag√©e entre threads doit √™tre prot√©g√©e par un mutex.

---

### Thread Pool (Pool de Threads)

Au lieu de cr√©er un thread par client, cr√©er un **pool fixe de threads** qui se partagent les clients.

**Avantages :**
- Limiter le nombre de threads (contr√¥le des ressources)
- √âviter le co√ªt de cr√©ation/destruction r√©p√©t√©e
- Meilleure scalabilit√©

**Principe :**
```
File de t√¢ches (clients en attente)
    ‚Üì
Thread Pool (5-10 threads workers)
    ‚Üì
Chaque thread prend un client, le traite, puis en prend un autre
```

**Impl√©mentation simplifi√©e :**
```c
#define THREAD_POOL_SIZE 10

typedef struct {
    int client_fd;
    // Autres infos...
} task_t;

// File de t√¢ches (impl√©mentation omise)
task_queue_t task_queue;
pthread_mutex_t queue_mutex;
pthread_cond_t queue_cond;

void* worker_thread(void *arg) {
    while (1) {
        // Attendre une t√¢che
        pthread_mutex_lock(&queue_mutex);
        while (queue_empty(&task_queue)) {
            pthread_cond_wait(&queue_cond, &queue_mutex);
        }

        task_t task = queue_pop(&task_queue);
        pthread_mutex_unlock(&queue_mutex);

        // Traiter le client
        handle_client(task.client_fd);
    }
    return NULL;
}

int main() {
    // Cr√©er le pool de threads
    pthread_t threads[THREAD_POOL_SIZE];
    for (int i = 0; i < THREAD_POOL_SIZE; i++) {
        pthread_create(&threads[i], NULL, worker_thread, NULL);
    }

    // Boucle accept
    while (1) {
        int client_fd = accept(server_fd, ...);

        // Ajouter √† la file
        pthread_mutex_lock(&queue_mutex);
        queue_push(&task_queue, (task_t){.client_fd = client_fd});
        pthread_cond_signal(&queue_cond);  // R√©veiller un worker
        pthread_mutex_unlock(&queue_mutex);
    }
}
```

---

### Avantages du Multi-Threading

‚úÖ **L√©ger** : Faible co√ªt m√©moire et CPU

‚úÖ **Rapide** : Cr√©ation/destruction rapide

‚úÖ **Partage de m√©moire** : Communication facile entre threads

‚úÖ **Scalabilit√©** : Peut g√©rer des milliers de clients

‚úÖ **Efficacit√©** : Meilleure utilisation des CPU multi-c≈ìurs

### Inconv√©nients du Multi-Threading

‚ùå **Complexit√©** : Synchronisation, race conditions, deadlocks

‚ùå **Debugging difficile** : Bugs non-d√©terministes, difficiles √† reproduire

‚ùå **Pas d'isolation** : Un bug peut corrompre tout le processus

‚ùå **Thread-safety** : Toutes les fonctions doivent √™tre thread-safe

‚ùå **Risque de fuite m√©moire** : Variables partag√©es mal g√©r√©es

### Quand utiliser Multi-Threading ?

‚úÖ Beaucoup de clients simultan√©s (100-10 000)

‚úÖ Communication entre clients n√©cessaire

‚úÖ Serveur performant avec peu de m√©moire

‚úÖ CPU multi-c≈ìurs √† exploiter

---

## Approche 3 : I/O Multiplexing

### Concept

Au lieu de cr√©er un processus ou thread par client, utiliser **un seul thread** qui surveille **plusieurs sockets** simultan√©ment.

**Principe :** "Dites-moi quand un socket est pr√™t pour lire/√©crire"

**Technologies :**
- `select()` : Standard POSIX, portable
- `poll()` : Am√©lioration de select
- `epoll()` : Sp√©cifique √† Linux, haute performance

### Avantages de l'I/O Multiplexing

‚úÖ **Ultra-l√©ger** : Un seul processus/thread

‚úÖ **Scalabilit√© extr√™me** : Peut g√©rer 100 000+ clients (C10K problem)

‚úÖ **Pas de synchronisation** : Pas de threads = pas de mutex

‚úÖ **D√©terministe** : Comportement pr√©visible

‚úÖ **Efficace** : Pas de contexte switching

### Inconv√©nients de l'I/O Multiplexing

‚ùå **Architecture diff√©rente** : Event-driven, plus complexe

‚ùå **Code plus long** : Logique distribu√©e en callbacks/√©tats

‚ùå **Un seul CPU** : N'exploite pas les multi-c≈ìurs (sauf avec processus multiples)

‚ùå **Op√©rations bloquantes interdites** : Tout doit √™tre non-bloquant

---

## `select()` : I/O Multiplexing de Base

### Principe

`select()` surveille plusieurs descripteurs de fichiers et indique lesquels sont pr√™ts.

```c
#include <sys/select.h>

int select(int nfds, fd_set *readfds, fd_set *writefds,
           fd_set *exceptfds, struct timeval *timeout);
```

**Param√®tres :**
- `nfds` : Valeur maximale des descripteurs + 1
- `readfds` : Ensemble des descripteurs √† surveiller en lecture
- `writefds` : Ensemble des descripteurs √† surveiller en √©criture
- `exceptfds` : Conditions exceptionnelles (rarement utilis√©)
- `timeout` : Timeout (NULL = blocage infini)

**Retour :**
- Nombre de descripteurs pr√™ts
- `0` si timeout
- `-1` si erreur

### Macros pour manipuler `fd_set`

```c
FD_ZERO(&set);           // Vider l'ensemble
FD_SET(fd, &set);        // Ajouter fd √† l'ensemble
FD_CLR(fd, &set);        // Retirer fd de l'ensemble
FD_ISSET(fd, &set);      // Tester si fd est dans l'ensemble
```

---

### Exemple Complet : Serveur avec `select()`

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/select.h>

#define PORT 8080
#define BUFFER_SIZE 1024
#define MAX_CLIENTS 30

int main() {
    int server_fd, client_fds[MAX_CLIENTS];
    fd_set readfds;
    int max_fd, activity, new_socket;
    struct sockaddr_in server_addr, client_addr;
    socklen_t client_len;
    char buffer[BUFFER_SIZE];

    // Initialiser tous les clients √† 0 (non utilis√©s)
    for (int i = 0; i < MAX_CLIENTS; i++) {
        client_fds[i] = 0;
    }

    // Cr√©er le socket serveur
    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd < 0) {
        perror("socket");
        exit(EXIT_FAILURE);
    }

    // SO_REUSEADDR
    int opt = 1;
    setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // Bind
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = INADDR_ANY;
    server_addr.sin_port = htons(PORT);

    if (bind(server_fd, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0) {
        perror("bind");
        exit(EXIT_FAILURE);
    }

    // Listen
    if (listen(server_fd, 10) < 0) {
        perror("listen");
        exit(EXIT_FAILURE);
    }

    printf("Serveur select() en √©coute sur le port %d\n", PORT);
    printf("Clients max : %d\n", MAX_CLIENTS);

    // Boucle principale
    while (1) {
        // Vider et pr√©parer l'ensemble des descripteurs
        FD_ZERO(&readfds);

        // Ajouter le socket serveur
        FD_SET(server_fd, &readfds);
        max_fd = server_fd;

        // Ajouter tous les sockets clients actifs
        for (int i = 0; i < MAX_CLIENTS; i++) {
            int fd = client_fds[i];

            if (fd > 0) {
                FD_SET(fd, &readfds);
            }

            if (fd > max_fd) {
                max_fd = fd;
            }
        }

        // Attendre une activit√© sur un des sockets
        activity = select(max_fd + 1, &readfds, NULL, NULL, NULL);

        if (activity < 0) {
            perror("select");
            break;
        }

        // Si activit√© sur le socket serveur = nouvelle connexion
        if (FD_ISSET(server_fd, &readfds)) {
            client_len = sizeof(client_addr);
            new_socket = accept(server_fd, (struct sockaddr*)&client_addr, &client_len);

            if (new_socket < 0) {
                perror("accept");
                continue;
            }

            // Afficher infos client
            char client_ip[INET_ADDRSTRLEN];
            inet_ntop(AF_INET, &client_addr.sin_addr, client_ip, INET_ADDRSTRLEN);
            printf("Nouvelle connexion : %s:%d (socket %d)\n",
                   client_ip, ntohs(client_addr.sin_port), new_socket);

            // Ajouter √† la liste des clients
            int added = 0;
            for (int i = 0; i < MAX_CLIENTS; i++) {
                if (client_fds[i] == 0) {
                    client_fds[i] = new_socket;
                    printf("Client ajout√© √† la position %d\n", i);
                    added = 1;
                    break;
                }
            }

            if (!added) {
                printf("Trop de clients, connexion refus√©e\n");
                close(new_socket);
            }
        }

        // V√©rifier tous les clients pour des donn√©es entrantes
        for (int i = 0; i < MAX_CLIENTS; i++) {
            int fd = client_fds[i];

            if (fd > 0 && FD_ISSET(fd, &readfds)) {
                // Lire les donn√©es
                ssize_t bytes_read = recv(fd, buffer, BUFFER_SIZE - 1, 0);

                if (bytes_read == 0) {
                    // Client d√©connect√©
                    getpeername(fd, (struct sockaddr*)&client_addr, &client_len);
                    char client_ip[INET_ADDRSTRLEN];
                    inet_ntop(AF_INET, &client_addr.sin_addr, client_ip, INET_ADDRSTRLEN);
                    printf("Client d√©connect√© : %s:%d (socket %d)\n",
                           client_ip, ntohs(client_addr.sin_port), fd);

                    close(fd);
                    client_fds[i] = 0;
                } else if (bytes_read < 0) {
                    perror("recv");
                    close(fd);
                    client_fds[i] = 0;
                } else {
                    // Donn√©es re√ßues
                    buffer[bytes_read] = '\0';
                    printf("Socket %d : %s", fd, buffer);

                    // √âcho
                    send(fd, buffer, bytes_read, 0);
                }
            }
        }
    }

    close(server_fd);
    return 0;
}
```

**Compilation :**
```bash
gcc -o server_select server_select.c -Wall -Wextra
./server_select
```

---

### Limitations de `select()`

‚ùå **Limite de descripteurs** : FD_SETSIZE (g√©n√©ralement 1024)

‚ùå **Performance** : O(n) pour chaque appel (parcourt tous les descripteurs)

‚ùå **Modification des sets** : `select()` modifie les `fd_set`, il faut les reconstruire √† chaque it√©ration

---

## `poll()` : Am√©lioration de `select()`

### Avantages sur `select()`

‚úÖ Pas de limite FD_SETSIZE

‚úÖ Ne modifie pas la structure d'entr√©e

‚úÖ API plus claire

```c
#include <poll.h>

int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

**Structure `pollfd` :**
```c
struct pollfd {
    int   fd;         // Descripteur de fichier
    short events;     // √âv√©nements √† surveiller
    short revents;    // √âv√©nements qui se sont produits
};
```

**√âv√©nements :**
- `POLLIN` : Donn√©es disponibles en lecture
- `POLLOUT` : Pr√™t pour √©criture
- `POLLERR` : Erreur
- `POLLHUP` : D√©connexion

### Exemple avec `poll()`

```c
#define MAX_CLIENTS 1000

struct pollfd fds[MAX_CLIENTS + 1];  // +1 pour le serveur
int nfds = 1;  // Nombre actuel de descripteurs

// Initialiser
fds[0].fd = server_fd;
fds[0].events = POLLIN;

for (int i = 1; i <= MAX_CLIENTS; i++) {
    fds[i].fd = -1;  // Non utilis√©
}

while (1) {
    int ready = poll(fds, nfds, -1);  // -1 = pas de timeout

    if (ready < 0) {
        perror("poll");
        break;
    }

    // V√©rifier le serveur
    if (fds[0].revents & POLLIN) {
        int client_fd = accept(server_fd, ...);

        // Ajouter √† la liste
        for (int i = 1; i <= MAX_CLIENTS; i++) {
            if (fds[i].fd == -1) {
                fds[i].fd = client_fd;
                fds[i].events = POLLIN;
                if (i >= nfds) {
                    nfds = i + 1;
                }
                break;
            }
        }
    }

    // V√©rifier tous les clients
    for (int i = 1; i < nfds; i++) {
        if (fds[i].fd == -1) continue;

        if (fds[i].revents & POLLIN) {
            // Lire donn√©es
            ssize_t n = recv(fds[i].fd, buffer, sizeof(buffer), 0);

            if (n <= 0) {
                // D√©connexion
                close(fds[i].fd);
                fds[i].fd = -1;
            } else {
                // Traiter donn√©es
                send(fds[i].fd, buffer, n, 0);
            }
        }
    }
}
```

---

## `epoll()` : Haute Performance (Linux)

### Pourquoi `epoll()` ?

`select()` et `poll()` ont une complexit√© **O(n)** : ils doivent parcourir tous les descripteurs √† chaque appel.

`epoll()` utilise une structure de donn√©es optimis√©e (arbre rouge-noir) avec une complexit√© **O(1)** pour la majorit√© des op√©rations.

**Performances :**
- 100 clients : `select()` ‚âà `epoll()`
- 10 000 clients : `epoll()` **100x plus rapide**

### API `epoll()`

```c
#include <sys/epoll.h>

// 1. Cr√©er une instance epoll
int epoll_create1(int flags);

// 2. Ajouter/Modifier/Supprimer un descripteur
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

// 3. Attendre des √©v√©nements
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

**Op√©rations `epoll_ctl()` :**
- `EPOLL_CTL_ADD` : Ajouter un descripteur
- `EPOLL_CTL_MOD` : Modifier les √©v√©nements surveill√©s
- `EPOLL_CTL_DEL` : Supprimer un descripteur

**√âv√©nements :**
- `EPOLLIN` : Donn√©es disponibles en lecture
- `EPOLLOUT` : Pr√™t pour √©criture
- `EPOLLET` : Mode Edge-Triggered (avanc√©)

---

### Exemple avec `epoll()`

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/epoll.h>

#define PORT 8080
#define MAX_EVENTS 100
#define BUFFER_SIZE 1024

int main() {
    int server_fd, epoll_fd;
    struct sockaddr_in server_addr;
    struct epoll_event event, events[MAX_EVENTS];

    // Cr√©er socket serveur
    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd < 0) {
        perror("socket");
        exit(EXIT_FAILURE);
    }

    int opt = 1;
    setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = INADDR_ANY;
    server_addr.sin_port = htons(PORT);

    if (bind(server_fd, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0) {
        perror("bind");
        exit(EXIT_FAILURE);
    }

    if (listen(server_fd, 10) < 0) {
        perror("listen");
        exit(EXIT_FAILURE);
    }

    // Cr√©er instance epoll
    epoll_fd = epoll_create1(0);
    if (epoll_fd < 0) {
        perror("epoll_create1");
        exit(EXIT_FAILURE);
    }

    // Ajouter le socket serveur √† epoll
    event.events = EPOLLIN;
    event.data.fd = server_fd;
    if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, server_fd, &event) < 0) {
        perror("epoll_ctl");
        exit(EXIT_FAILURE);
    }

    printf("Serveur epoll en √©coute sur le port %d\n", PORT);

    char buffer[BUFFER_SIZE];

    // Boucle √©v√©nementielle
    while (1) {
        int n_events = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);

        if (n_events < 0) {
            perror("epoll_wait");
            break;
        }

        // Traiter tous les √©v√©nements
        for (int i = 0; i < n_events; i++) {
            int fd = events[i].data.fd;

            if (fd == server_fd) {
                // Nouvelle connexion
                struct sockaddr_in client_addr;
                socklen_t client_len = sizeof(client_addr);
                int client_fd = accept(server_fd, (struct sockaddr*)&client_addr, &client_len);

                if (client_fd < 0) {
                    perror("accept");
                    continue;
                }

                char client_ip[INET_ADDRSTRLEN];
                inet_ntop(AF_INET, &client_addr.sin_addr, client_ip, INET_ADDRSTRLEN);
                printf("Nouvelle connexion : %s:%d (socket %d)\n",
                       client_ip, ntohs(client_addr.sin_port), client_fd);

                // Ajouter le client √† epoll
                event.events = EPOLLIN;
                event.data.fd = client_fd;
                if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &event) < 0) {
                    perror("epoll_ctl ADD client");
                    close(client_fd);
                }
            } else {
                // Donn√©es d'un client
                ssize_t bytes_read = recv(fd, buffer, BUFFER_SIZE - 1, 0);

                if (bytes_read <= 0) {
                    // D√©connexion
                    if (bytes_read == 0) {
                        printf("Client d√©connect√© (socket %d)\n", fd);
                    } else {
                        perror("recv");
                    }

                    epoll_ctl(epoll_fd, EPOLL_CTL_DEL, fd, NULL);
                    close(fd);
                } else {
                    // √âcho
                    buffer[bytes_read] = '\0';
                    printf("Socket %d : %s", fd, buffer);
                    send(fd, buffer, bytes_read, 0);
                }
            }
        }
    }

    close(server_fd);
    close(epoll_fd);
    return 0;
}
```

**Compilation :**
```bash
gcc -o server_epoll server_epoll.c -Wall -Wextra
./server_epoll
```

---

## Comparaison des Approches

### Tableau R√©capitulatif

| Crit√®re | Fork | Threads | select() | poll() | epoll() |
|---------|------|---------|----------|--------|---------|
| **Scalabilit√©** | Faible (< 100) | Moyenne (< 1000) | Faible (< 1024) | Moyenne | Excellente (> 10k) |
| **M√©moire** | √âlev√©e | Moyenne | Faible | Faible | Faible |
| **CPU** | √âlev√© | Moyen | Moyen | Moyen | Faible |
| **Complexit√©** | Simple | Moyenne | Moyenne | Moyenne | √âlev√©e |
| **Isolation** | Forte | Aucune | Aucune | Aucune | Aucune |
| **Portabilit√©** | POSIX | POSIX | POSIX | POSIX | Linux |
| **Debugging** | Facile | Difficile | Moyen | Moyen | Moyen |
| **Multi-core** | Oui | Oui | Non | Non | Non* |

*epoll peut √™tre combin√© avec plusieurs processus

### Performance selon le nombre de clients

```
Clients    | Fork  | Threads | select | poll  | epoll
-----------|-------|---------|--------|-------|-------
10         | ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ‚úÖ   | ‚úÖ‚úÖ‚úÖ  | ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ‚úÖ
100        | ‚úÖ‚úÖ   | ‚úÖ‚úÖ‚úÖ   | ‚úÖ‚úÖ    | ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ‚úÖ
1 000      | ‚ùå     | ‚úÖ‚úÖ     | ‚ùå      | ‚úÖ‚úÖ   | ‚úÖ‚úÖ‚úÖ
10 000     | ‚ùå     | ‚ùå       | ‚ùå      | ‚ùå     | ‚úÖ‚úÖ‚úÖ
100 000    | ‚ùå     | ‚ùå       | ‚ùå      | ‚ùå     | ‚úÖ‚úÖ
```

---

## Choix de l'Architecture

### Arbre de D√©cision

```
Nombre de clients simultan√©s ?
‚îÇ
‚îú‚îÄ < 50 clients
‚îÇ   ‚îî‚îÄ Utiliser fork() (simplicit√©)
‚îÇ
‚îú‚îÄ 50-500 clients
‚îÇ   ‚îî‚îÄ Threads ou poll()
‚îÇ
‚îú‚îÄ 500-10 000 clients
‚îÇ   ‚îî‚îÄ epoll() ou threads avec pool
‚îÇ
‚îî‚îÄ > 10 000 clients
    ‚îî‚îÄ epoll() + architecture √©v√©nementielle
```

### Selon le type d'application

| Application | Recommandation |
|-------------|----------------|
| **Serveur web** | epoll() + worker processes |
| **Serveur de chat** | Threads ou epoll() |
| **Serveur de jeu** | epoll() + UDP |
| **Proxy** | epoll() |
| **Serveur de fichiers** | Threads |
| **Daemon syst√®me** | fork() pour isolation |

---

## Architecture Hybride : Le Meilleur des Deux Mondes

### Combinaison : Plusieurs processus + epoll()

**Principe :** Utiliser `SO_REUSEPORT` pour cr√©er plusieurs processus, chacun avec sa propre boucle `epoll()`.

**Avantages :**
- Utilisation de tous les c≈ìurs CPU
- Scalabilit√© extr√™me
- Isolation entre processus

**Exemple (Nginx, HAProxy) :**
```
Processus 1 (CPU core 1) : epoll() ‚Üí g√®re 10 000 clients
Processus 2 (CPU core 2) : epoll() ‚Üí g√®re 10 000 clients
Processus 3 (CPU core 3) : epoll() ‚Üí g√®re 10 000 clients
Processus 4 (CPU core 4) : epoll() ‚Üí g√®re 10 000 clients

Total : 40 000 clients simultan√©s
```

---

## En R√©sum√©

Pour cr√©er des serveurs capables de g√©rer plusieurs clients, trois approches principales existent.

**Fork (Multi-Processus) :**
- ‚úÖ Simple, isolation forte
- ‚ùå Co√ªteux en m√©moire
- üìä < 100 clients

**Threads (Multi-Threading) :**
- ‚úÖ L√©ger, partage m√©moire
- ‚ùå Synchronisation complexe
- üìä 100-1000 clients

**epoll (I/O Multiplexing) :**
- ‚úÖ Ultra-performant, scalable
- ‚ùå Architecture diff√©rente
- üìä 1 000-100 000+ clients

**R√®gles de choix :**

1. **Petit serveur** (< 50 clients) ‚Üí **fork()** (simplicit√©)
2. **Serveur moyen** (50-500) ‚Üí **threads** ou **poll()**
3. **Gros serveur** (500+) ‚Üí **epoll()** (Linux) ou √©quivalent
4. **Production critique** ‚Üí **epoll() + multi-processus**

**Le C10K problem :**
Comment g√©rer 10 000 connexions simultan√©es ? R√©ponse : `epoll()` et architecture √©v√©nementielle.

Dans la prochaine section, nous verrons comment cr√©er des serveurs non-bloquants et des architectures I/O asynchrones avanc√©es.

---

**‚Üí Prochaine section : 20.9 Non-blocking I/O et epoll**

‚è≠Ô∏è [Non-blocking I/O et epoll](/20-reseau-sockets/09-non-blocking-io-epoll.md)
